\section{Marine \& Weather Engineering}
	\lettrine[lines=4]{\color{BrickRed}M}eteorology is the study of atmospheric phenomena such as clouds, weather depression and precipitation to understand how they form and evolve. It is a discipline that deals mainly with fluid mechanics applied to air but which makes use of various other branches of physics and chemistry. It therefore allows for weather forecasts based on mathematical models on (actually) relatively short and long term. It is also applied for the prediction of air quality, climate change and the study in several areas of human activity (construction, air traffic, etc.).
	
	Meteorology is linked to a large amount of variables that would be very difficult to make a list even not exhaustive... However on our planet Earth, an important factor not to be neglected is the one formed by ocean surfaces and their intrinsic dynamics which we will try to present through a summary mathematical study some of their properties. What is nice is that some results can also be applied to other Planets and sometimes very exotic one like Jupiter.
	
	Before beginning to the content of this section we also strongly recommend the reader to have a look of the section of Astronomy as the distance of the Earth (in general any planet...) to its main star (place on the foci of the orbit) has an influence on the power of the radiation (\SeeChapter{see section Optical Geometry}) and is therefore directly related to the eccentricity $e$ of the orbit. Especially the inclination of the Earth rotation relatively to the ecliptic plan has a strong influence on the weather conditions as it related to the existence of the seasons!
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/earth_inclination.jpg}
		\caption{Inclination of Earth (source: Wikipedia)}
	\end{figure}
	And the reader must be careful about the statement that at the periapsis people say it's winter because in fact... it depends where you live!!! Indeed, if you live in the northern hemisphere ti will be winter, but at the southern hemisphere it will be summer (people that have the money to travel during holidays know this fact very well) as illustrated in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/earth_seasons.jpg}
		\caption{Season space point of view (source: NASA)}
	\end{figure}
	The reader must also consider that the main factors of surface climate are:
	\begin{itemize}
		\item Latitude
		\item Topography
		\item Temperature
		\item Cloud cover
		\item Water cover
		\item Snow/Ice cover
		\item Wind circulation
	\end{itemize}
	The combination of all these factors is well represented in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/weather_factors.jpg}
		\caption{Annual mean temperatures in $2$ [m] height in the atmosphere from the NCEP data set over the period 1950-2000. Values in $^\circ$C.}
	\end{figure}
	This section is a general introduction to the basic to the field of technical applications of thermodynamics and fluid mechanics. It will allow the reader to become familiar with the language and some fundamental methods of calculations used by engineers in this branch. Of course, this study must be completed by practical laboratory and simulation work on computer.
	
	\subsection{Visual horizon}
	Let us consider first a little funny subject making often friendly debate during holidays or more seriously ... In some meteorological softwares, it is required during temperature and pressure measurement to manually enter the visual horizon... but this is difficult to determine when the weather is very good and we are in a height position (on a hill or small mountain).

	For this, consider the Earth as a perfect sphere with radius $R$ and a point of view of height $h$ relative to sea level which we will denote by $A$ (and without atmosphere to avoid optical effects...). The question is to know how far can be by maximum the point $C$ if it is by definition given by the tangent $\overline{AC}$ which is simply the horizon line:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/visual_horizon_01.jpg}
			\caption{Visual Horizon configuration experience}
		\end{center}	
	\end{figure}	
	The reader can probably already observed that the study will mainly appeal to trigonometry concepts (see corresponding section of this book) and elementary geometry (also see corresponding sections).
	
	The angle $\widehat{\text{O}CA}$ is a right angle. Indeed, a straight tangent at a point on a circle is perpendicular to the radius at that point. The triangle $\widehat{\text{O}CA}$  is therefore rectangle on $C$.
	
	Therefore we have:
	
	But, we have $\overline{OB}=\overline{OC}=R$. Hence we deduce:
	
	The $\overline{AC}$ distance is the distance in a straight line between our viewpoint and the (floor of the) boat that we see on the horizon. The distance, however, that interest us here is $\overline{BC}$: it is the distance that we should travel at the altitude $0$ to reach the ground of the boat we see on the horizon.
	
	For what will follows we will put for the curvilinear distance: $\overline{BC}=d$.
	
	When the angle $\alpha$ varies from $0^{\circ}$ to $360^{\circ}$ (full circle), we describe the entire circumference of the Earth, that is to say $2\pi R$, since the Earth is assumed to be round.
	
	Using the rule of three:
	If an angle of $360^{\circ}$ corresponds to a distance of $2\pi R$ then an angle $\alpha$ in degrees correspond to a distance:
	
	But we have seen previously that:
	
	Hence finally:
	
	With $R\cong 6,378\;[\text{km}]$, we find ($h$ must be expressed in kilometers):
	
	In Microsoft Excel this is given by:
	\begin{center}
	\texttt{=111.32*DEGREES(ACOS(6378/(6378+h)))}
	\end{center}
	We have then in vacuum, in a landscape without obstacles... the following table:
		
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If we do not take into account atmospheric refraction, we find in the table above that would need to go to an altitude of the order of several kilometers to see beyond $200$ [km] away. Yet, without going very far, at the heights of Nice (Alpes-Maritimes in France), it is possible to observe the Cap Corse (island) which is about $220$ [km] from the continent !!! Atmospheric refraction plays then a significant role in this phenomenon.
	\end{tcolorbox}
	The question that arise is: given that we are at a height $h_0$ and we look far at the horizon:
	\begin{enumerate}
		\item What is the distance of the horizon (same as above but with a different approach use by people that argue that the Earth is flat and that the NASA lie...)?

		\item At what level under the horizon is an object at a distance $d$ of the observer (eye)?
	\end{enumerate}
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/visual_horizon_02.jpg}
			\caption{Visual Horizon configuration experience}
		\end{center}	
	\end{figure}
	The application of Pythagoras gives us immediately:
	
	and:
	
	So let us compare the value of $d_1$ just obtained above (straight distance) for $d$ obtained previously that is the curvilinear distance:
	
	So the bias of the people that believe that the Earth (and all other planets) is flat ("flat Earthers") is that when they do the experiment to prove their arguments they forget the "perfectly spheric" assumption of that model. Indeed as we have seen in the section of Astronomy the Earth's shape is more like a irregular potatoes and is locally (at human scale, i.e. a few hundreds of kilometers sometimes) absolutely not spherical due to relief irregularities.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.9]{img/engineering/visual_horizon_03.jpg}
			\caption{Earth curvature visibility}
		\end{center}	
	\end{figure}
	Anyways an another argument is that we can prove with Isoperimetric Inequality that natural final shape for a cloud of dust is a sphere and otherwise that the acceleration of objects that fall all respect Newton's law in it's... spherical form! And also... if the Earth was flat... nobody so far as we know has seen it's border...
	
	Therefore the two relations to remember are:
	
	
	\pagebreak
	\subsection{Wind direction}
	We will now prove mathematically something quite intuitive: the winds are moving from high to low pressure (it's silly like that but we must still prove it!).

	We know (\SeeChapter{see section Continuum Mechanics}) that the pressure force exerted on a surface $S$ is normal to this surface and is equal in scalar form to $P\cdot S$.
	
	For an air parcel volume $\mathrm{d}V=\delta x\delta y\delta z$ the total pressure force in the $x$ direction is then:
	
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/wind_direction.jpg}
		\end{center}	
	\end{figure}
	In addition, we have (\SeeChapter{see section Differential and  Integral Calculus}):
	
	Therefore:
	
	The mass pressure force is then:
	We can do the same calculation following $y$. Finally, the massic horizontal pressure force is given by:
	
	Thus, the pressure force (massic or not) is opposite to the horizontal gradient. The transmission of information (of the force) is donce at the speed of sound for this equation (which explains the speed of air calls in your home or apartment and then force that can make slam doors or window).

It is therefore:

	\begin{itemize}
		\item  Directed from high to low pressure, perpendicular to the isobars

		\item Inversely proportional to the spacing of the isobars.
	\end{itemize}
	If we measure the values of the atmospheric pressure at different points of the globe and we connect between them the points of same pressure on a drawing, we get a series of curves, named "\NewTerm{isobars}\index{isobars}". The wind is directly determined by the atmospheric relief since it is a movement of air between the high to low pressure.

	The wind speed is then determined by the pressure gradient: that is, if the atmospheric pressure changes rapidly with distance, the wind blows strong, while it will be low in abarometric  "swamp" where this pressure remains almost unchanged on great distances. In summary, more the isobars are close together, more the wind will blow strong.

	The isobars are traditionally marked with a pitch of $5$ millibars on weather maps such as shown in the example below:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/depression.jpg}
		\end{center}	
		\caption{Typical representation of isobars (on a depression)}
	\end{figure}
	Charts showing isobars are useful because they identify features such as anticyclones (areas of high pressure) and depressions (areas of low pressure).

	Areas of high and low pressure are caused by ascending and descending air. As air warms, it ascends leading to low pressure at the surface. As air cools, it descends leading to high pressure at the surface. This is illustrated in the diagram below.
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/pressure_sytem.jpg}
		\end{center}	
		\caption{High and low pressure systems}
	\end{figure}
	In general, low pressure leads to unsettled weather conditions and high pressure leads to settled weather conditions.

	In an anticyclone (high pressure) the winds tend to be light and blow in a clockwise direction (in the northern hemisphere as we will prove later below). Also the air is descending, which reduces the formation of cloud and leads to light winds and settled weather conditions.

	In a depression (low pressure), air is rising and blows in an anticlockwise direction around the low (in the northern hemisphere). As it rises and cools, water vapor condenses to form clouds and perhaps precipitation. This is why the weather in a depression is often unsettled - there are usually frontal systems associated with depressions.
	
	To close this topic about winds, let us notice that meteorologists empirically defined (that is fun for the general culture) a unit of wind measurement that is just a match between the wind force and the distance between two isobars in steps of $5$ by $5$ [mb]:
	
	
	\subsection{Atmospheric Profile Models}
	An atmospheric profile model is a mathematical model constructed to predict the profile of temperature, pressure or other in the atmosphere, locally depending on the measured height supposing a steady state of the atmosphere (same as if we take out a cylinder piece of the atmosphere and freeze time to analyze some of the properties of the atmosphere inside depending on the height in the cylinder). 

	There a lot of models but let us see just two easy one:
	
	\subsubsection{Atmospheric Exponential Profile Model}
	Consider that the atmosphere is a perfect fluid in a uniform gravity field. Then from the following Bernoulli's theorem relation, proved in the section of Continuum Mechanics (static fluid):
	
	It comes then:
	
	Thus, to know the variation of pressure depending on height in the atmosphere or depth in the ocean, we have taken as hypothesis the "\NewTerm{hydrostatic equilibrium}\index{hydrostatic equilibrium}", that is the variation of pressure with height / depth is proportional to the gravity and density of the fluid.
	
	This is obviously not valid in the case of rapid movements of convection, like thunderstorms, but can easily be satisfied for slow movements (quasi-static) and at large scale: the "\NewTerm{synoptic scale}\index{synoptic scale}" ($2,000$-$20,000$ [km]).
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.7]{img/engineering/meteoric_scale.jpg}
		\end{center}	
		\caption{Various meteorological scales}
	\end{figure}
	We will then combine this last relation with a state equation, such as that of the ideal gas at temperature $T$ and of density $\rho$ whose constituent particles have a mass $m$. So we have the ideal gas equation (\SeeChapter{see section Continuum Mechanics}):
	
	with for recall $P$ that is the pressure expressed in Pascals, $V$ the volume in cubic meters, $R$ is the gas constant, $T$ is the temperature in Kelvin, $n$ the number of moles, $k$ is the Boltzmann constant, $\rho$ the particle density, $m$ the total mass of the particles.
	
	In the isothermal case (e.g. in the terrestrial stratosphere, above $10$ [km] altitude where the temperature is almost constant around $-55$ degrees Celsius), we found out easily by substitution that:
	
	Therefore, at a given pressure, the vertical pressure gradient is inversely proportional to temperature.

	Let us now consider the following relation:
	
	Using the exponential:
	
	The pressure therefore decreases exponentially with the altitude. $P_0$ being the pressure at ground level.

	Let us return to the relation:
	
	It can of course also be written as:
	
	which tells us that the distance $z$ between isobaric surfaces is directly proportional to temperature.

	We have also another common approach. Let us start again from the relation proved just earlier above, but for a mass $m$ of $1$ [kg]:
	
	and let us denote this relation as follow:
	
	Let us recall that (\SeeChapter{see section Differential and Integral Calculus}):
	
	Therefore:	
	
	and now let assume that the temperature variation is linear in the atmosphere (which is not far from the truth for the first $10$ to $20$ km of the atmosphere such that:
	
	with $\lambda<0$ which is the temperature gradient in $[\text{K}\cdot\text{m}^{-1}]$.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.4]{img/engineering/weather_temperature_pression_altitude_profile.jpg}
		\end{center}	
		\caption{Typical profile of temperature and pressure on Earth... late 20th century (source: ?)}
	\end{figure}
	Then we have:
	
	Therefore:
	
	Which give:
	
	After simplification:
	
	Hence:
	
	Which can written more aesthetically:
	
	A good practical example of application of this relation is the gliders and hang gliders. They expect the weather forecast that it communicates to them the height of the isotherm of zero degrees during its bulletins. They then deduce the temperature gradient per meter. For these athletes, good condition is to have a gradient of $1\;[^\circ \text{C}]$ by $100 $meters. It is therefore easy with the above relation to calculate the pressure at an altitude of $2,000$ meters and to derive the pressure gradient which allows them to use some the updrafts for their aerobatic practices.
	
	\subsubsection{Adiabatic Atmosphere Model}
	The adiabatic temperature gradient is, in the atmosphere, the air temperature change with altitude (ie the air temperature gradient), which only depends on the atmospheric pressure, that is to say:
	\begin{itemize}
		\item Without considerating heat exchange with the environment (other air masses, relief, ground, etc.)

		\item  Without considerating condensation (cloud formation) or precipitation.
	
		\item Without taking into account the Sun position and variations
	\end{itemize}
	This concept is of great importance in meteorology, as well as aviation and maritime navigation.

	We have prove in the section Thermodynamics the Laplace's thermodynamics law (satisfied under certain conditions!):
	
	with the Laplace coefficient:
	
	Therefore in massic form the Laplace's law becomes:
	
	We can take the logarithm:
	
	But by taking the logarithmic differential:
	
	We then have (relationship that we will reuse in the section of Music Mathematics)
	
	We could also have found this result directly from the relation proved in the section Thermodynamics:
	
	By taking also the logarithmic differential  of the ideal gas law where $n$ is the number of moles (\SeeChapter{see section Continuum Mechanics}:
	
	But in the massic form for one mole:
	
	where $M_m$ is the molar mass (\SeeChapter{see section Analytical Chemistry}, we have:
		
	Therefore:
	
	We then get:
	
	Therefore:
	
	Let us use the relation proved earlier shown above:
	
	It comes then:
	
	So we have an atmosphere with constant and negative thermal gradient (temperature decreases linearly with altitude):
	
	The last form using the molar mass is more convenient because it allows to characterize the studied medium. Note that this is a model that looks to work well for an altitude of $0$ to $90$ [km] on Venus but much less well for the planet Earth.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We then have for example for Earth (thus knowing that the model is not well adapted):
	\begin{gather*}
		g\cong 9.81\;[\text{m}\cdot{s}^{-2}]\qquad R\cong 8.314\;[\text{J}\cdot \text{K}^{-1}\cdot \text{mol}^{-1}]
	\end{gather*}
	and the adiabatic coefficient for air is equal to $\gamma=1.4$, and its molecular weight:
	\begin{gather*}
		M_{m,\text{air}}\cong 28.96\cdot 10^{-3}\;[\text{kg}\cdot\text{mol}^{-1}]
	\end{gather*}
	Therefore:
	
	which corresponds to the current idea: $1$ degree per $100$ meters.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Hypsometric equation}\mbox{}\\\\
	The hypsometric equation, also known as the thickness equation, relates an atmospheric pressure ratio to the equivalent thickness of an atmospheric layer under the assumptions of constant temperature and gravity. It is derived from the hydrostatic equation and the ideal gas law.
	
	So we have just prove for to hydrostatic equilibrium:
	
	
	We can integrate this relationship if we know $T$ as a function of $P$ or $z$. The direct measurement of $P$ in practice is easier (single altimeters are actually barometers if they don't use GPS).

	We can then separate the variables:
	
	By integrating between two levels $a$ and $b$:
	
	Since:
	
	Then to continue we use a trick. We will define the average temperature by the relation:
	
	Which then allows us to write:
	
	Therefore:
	
	or written differently:
	
	or after rearrangement and change the notation for the average temperature:
	
	Both boxed relations are each respectively named "\NewTerm{hypsometric equation}\index{hypsometric equation}" (from the Greek "hypso" for "height").
	
	\pagebreak
	\subsection{Planetary equilibrium temperature}
	The "\NewTerm{planetary equilibrium temperature}\index{planetary equilibrium temperature}" is a theoretical temperature that a planet would be at when considered simply as if it were a black body being heated only by its parent star. In this model, the presence or absence of an atmosphere (and therefore any greenhouse effect) is not considered, and one treats the theoretical black body temperature as if it came from an idealized surface of the planet.

	Other authors use different names for this concept, such as "\NewTerm{equivalent black-body temperature of a planet}\index{equivalent black-body temperature of a planet}", or the "\NewTerm{effective radiation emission temperature of the planet}\index{effective radiation emission temperature of the planet}".

	If the incident solar radiation ("\NewTerm{insolation}\index{insolation}") - or incident power radiatin -on the planet at its orbital distance from the Sun is $M_\otimes$, the amount of energy absorbed by the planet ($\mathrm{d}\Phi/\mathrm{d}S$) will depend on its reflection coefficient (albedo $\rho$) and and its cross-sectional area $S$ or average radius $R$ as:
	
	Note that the albedo would be zero for a blackbody as we have seen it in the section of Optical Geometry. However, in planetology, more useful results are obtained by accounting for a measured or assumed planetary albedo $\rho>0$.

	The infrared power radiated by the planet as thermal radiation will depend on its emissivity and its surface area, according to the Stefan–Boltzmann equation proved in the section of Thermodynamics:
	
	and as it is as power by unit surface we will multiply it by the above surface area of the planet such that:
	
	 For a spherical planet, the surface perfect spherical area is well know (\SeeChapter{see section Geometric Shapes}) therefore:
	
	But a planet that does not absorb all incident radiation (sometimes known as a grey body) emits less total energy than a black body and is characterized by an emissivity, $\varepsilon< 1$, therefore (by definition emissivity + albedo = 1):
	
	The equilibrium temperature is then calculated by setting $P_\text{in}=P_\text{out}$. Thus:
	
	Therefore:
	
	It is interesting to note that the equilibrium temperature does not depend on the size of the planet, because both the incoming radiation and outgoing radiation depend on the area of the planet!
	
	Using a more astrophysical point of view if we don't know $M_\odot$, based on the luminosity $L_\odot$ (power) of the star and its distance $d$ to the planet we get as proved in the using the relation proved in the section of astrophysics:
	
	Therefore:
	
	
	Earth's overall, average albedo is about $\rho\cong 0.31$ and the emissivity of earth is considered as being about $\varepsilon\cong 0.96$ and $M_\odot\cong 1376$ [W$\cdot$m$^{-2}$].

	A numerical application for earth gives $T_\text{eq}=253.7$ [K] (that is $-19.5$ [$^\circ$C]).

	Based on this calculation, Earth's expected average global temperature is well below the freezing point of water!

	Earth's actual average global temperature is around $14$ [$^\circ$C]. Our planet is warmer than predicted with  a pretty big difference!

	Why is Earth's temperature so much warmer than our calculations predicted? Certain gases in the atmosphere trap some extra heat, warming our planet like a blanket. This extra warming is called the greenhouse effect. Without it, our planet would be a frozen ball of ice. Thanks to the natural greenhouse effect, Earth is comfortable place for life as we know it. However, too much of a good thing can cause problems. In recent decades, a rise in the amount of greenhouse gases has begun to warm Earth a bit too much.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For extrasolar planets the temperature of the star can be calculated from the color of the star using Planck's law (\SeeChapter{see section Thermodynamics}). The calculated temperature of the star can be used with the Hertzsprung–Russell diagram (\SeeChapter{see section Astrophysics}) to determine the absolute magnitude of the star, which can then be used with observational data to determine the distance to the star and finally the size of the star. Orbital simulations are used to determine what orbital parameters (including orbital distance) produce the observations seen by astronomers.[10] Astronomers use a hypothesized albedo and can then estimate the equilibrium temperature.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Greenhouse effect}
	The greenhouse effect is the process by which radiation from a planet's atmosphere warms the planet's surface to a temperature above what it would be without its atmosphere.

	If a planet's atmosphere contains radiatively active gases (i.e., greenhouse gases) the atmosphere will radiate energy in all directions. Part of this radiation is directed towards the surface, warming it. The downward component of this radiation – that is, the strength of the greenhouse effect – will depend on the atmosphere's temperature and on the amount of greenhouse gases that the atmosphere contains.

	Earth's natural greenhouse effect is critical to supporting life. Human activities, primarily the burning of fossil fuels and clearing of forests, have intensified the natural greenhouse effect, causing global warming.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.9]{img/engineering/greenhouse_effect.jpg}
		\end{center}	
		\caption{Greenhouse effect concept}
	\end{figure}
	
	For a mathematical analysis of the greenhouse effect we must improve our above model. Consider for this that we denoted $T_S$ the temperature of surface of the Earth (or any other planet) and $T_A$ the temperature of its atmosphere.	The thermodynamic equilibrium of atmosphere gives us in the point of view of the radiation using again the Boltzmann law and considering that Earth's atmosphere radiate $50\%$ back in space and reflect back $50\%$ back to earth surface:
	
	Therefore it is immediate that:
	
	Remember now the equilibrium proved earlier:
	
	That simplify to:
	
	Using our new notation:
	
	Therefore after rearranging:
	
	Now we add to this the back-radiation due to atmosphere:
	
	That is:
	
	Therefore after rearrangement:
	
	A numerical application gives $T_S\cong 297$ [K] that is $14$ [$^\circ$ C]. This is quite more accurate than the previous model!

	\subsubsection{Milankovitch cycles}
	The Milankovitch cycles are periodic or quasiperiodic changes in the parameters of the Earth's orbit and tilt, which in turn affect the climate. The three major types of Milankovitch cycle are:
	\begin{itemize}
		\item changes in the eccentricity of the Earth's orbit
		\item changes in the obliquity, or tilt of the Earth's axis
		\item precession, meaning changes in the direction of the Earth's axis relative
to the fixed stars
	\end{itemize}
These changes do not affect the overall annual amount of solar radiation hitting the Earth, but they affect the strength of the seasons in different ways at different latitudes. It is widely believed that they are partially responsible for the glacial cycles. However, the details of how this occurs are complex and poorly understood.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.8]{img/engineering/milankovitch_cylces.jpg}
		\end{center}	
		\caption{Past and future Milankovitch cycles (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	By the mid-seventies the Earth’s orbital parameters were known over the last million years to a good accuracy thanks geological records. A decisive step was made by  André Berger (1978), who expressed in an analytical form the Fourier decomposition of the Earth's orbital parameters relevant for the astronomical theory of palaeoclimates. This work constitutes the first demonstration that the spectrum of climatic precession is dominated by periods of $19,000$, $22,000$ and $24,000$ years, that of obliquity, by a period $41,000$ years, and eccentricity has periods of $400,000$ years, $125,000$ years and $96,000$ years. 
	\end{tcolorbox}
	
	\subsection{Weather (sounding) balloon}
	A pretty interesting example of applied mathematics to weather engineering is the study of the famous weather balloons and especially the characteristic of their volume depending on the altitude which is often subject to debate in discussion groups when nobody formalizes the problem once and for all. You will therefore understand that this is what we will discuss here and especially we will try to determine the theoretical diameter thereof at a given altitude.
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.8]{img/engineering/weather_balloons.jpg}
		\end{center}	
		\caption[]{Small weather balloon}
	\end{figure}
	The statement of the often discussed problem is the following:

	Given a PVC (Polyvinyl Chloride) weather balloon of mass $m$ used to take at high-altitude an apparatus for performing measurements. The envelope of the balloon contains $n$ moles of the hydrogen ideal gas thus having a molar mass:
	
	The atmosphere will be treated as a perfect gas, of average molecular weight:
	
	at N.T.P. (Normal conditions of Temperature and Pressure).

	We first we want to find what is the ascension force experienced by the balloon?

	Then we want to determine the minimum amount of material $n_0$ providing the take off of the balloon for a given total mass (including the balloon istself!) of $2.6$ [kg]. then the volume $V_0$ corresponding to the starting altitude.

	Let us recall two things first to solve this first point:
	\begin{enumerate}
		\item Any body immersed in a liquid (or gas) undergoes an upward force equal to the weight of the volume it displaces (Archimedes force) according to the relation proved in the section Continuum Mechanics:
		

		\item An ideal gas with a mass in grams equal to the molar mass occupies according to the ideal gas law a volume of $22.4$ [L] at $273.15$ [K] and at a pressure of $1$ [atm] as we have proved it in the section of Thermochemisty. Which gives at N.T.P:
		
	\end{enumerate}
	So for the balloon to fleet at constant height (without mounting but without falling too ...) with just sufficient quantity $n_0$ of hydrogen, it is necessary according to the Archimedes principle that the volume of air that it moves has a weight equal to the total mass of the ballloon and the probe, thus $2.6$ [kg] in our case!

	So since $22$ [L] air weigh about $29$ grams, it is necessary that the volume that it moves to be equal to $2.6$ [kg] of air. Either by a simple rule of three:
	
	So if the balloon is spherical, it gives a radius of:
	
	Thus a diameter of about $1.56$ [m] at the ground. This is consistent with reality!

	We still need to determine the number of moles of hydrogen. It comes immediately:
	
	Now that we know the number of moles in the balloon , if we know the temperature and pressure at a height of $22,000$ [m] (typical altitude of a small weather balloon) it only remains to apply ideal gas law to determine the volume at this altitude then given by the relation provedin the section Continuum Mechanics:
	
	and at $22,000$ [m] above sea level, we have following the numerical tables available on the Internet:
	
	But because solar radiation is about $30\%$ higher in this altitude and the balloon is considered as adiabatic system (without heat exchange) and does not restore the power stored in the external environment. We consider that the temperature is at least $30\%$ higher which gives us such for values:
	
	
	We could also use (because of the adiabatic assumption) the Boyle's relation (\SeeChapter{see section Continuum Mechanics}) to achieve the same result:
	
	This gives a radius of about $2.33$ [m] (thus diameter  of about $4.6$ [m]) instead of $0.78$ [m] to the ground! An increase of the diameter of about $300\%$. However, it is more important to focus on the increasing surface to determine the elastic stress forces on the PVC. So we have before:
	
	and after:
	
	Thus an increase of the surface of about $1,000\%$ while a standard elastomer (including PVC as a part) is not resistant to a relative change more than $500\%$ !!! It is then much easier to understand from the point of view of the surface, why the balloon does not withstand an increase in the diameter of about $300\%$.

	Moreover, if we apply a little abusively Hook's law to the balloon (\SeeChapter{see section Continuum Mechanics}), with PVC Young's modulus which is between (source Wikipedia):
	
	We have:
	
	Which complies with the numeral data tables which give an elastic lower limit value of $50$ [MPa] and an upper limit of $80$ [MPa] for PVC (source Wikipedia). We can then calculate the minimum and maximum theoretical height that the balloon can reach.

	So for the minimum height, we will write first:
	
	Which then corresponds to a final radius of:
	
	Which corresponds to a volume of:
	
	Applying Boyle-Martiotte:
	
	which is a pressure corresponding to a height of about $18,000$ [m] according to the experimental measurements (\url{www.engineeringtoolbox.com}) and this is indeed a rare height at which PVC balloons blow up.

	Now let us do the same with the maximum height:
	

	Which then corresponds to a final radius of:
	
	Which corresponds to a volume of:
	
	Applying Boyle-Martiotte:
	
	which is a pressure corresponding to a height of about $24,000$ [m] according to the experimental measurements and this is indeed a height at which the highest PVC balloons burst.
	
	\subsection{Cyclogenesis and Anticyclogenesis}
	Cyclogenesis is the development or strengthening of cyclonic circulation in the atmosphere (a low-pressure area). 
	
	Most of the atmospheric mass is contained within the first $20$ kilometers in altitude, so that the large-scale meteorology runs on a thin spherical shell (and can be assimilated to the mechanics of a two-dimensional fluid).

	The circulation engine of the atmosphere in the tropics is solar heating. Because of the inclination of $23.5$ degrees to the axis of rotation of the Earth, the Sun is never more than a few tenths of a degree from the zenith at noon throughout the year in the tropics, which gives maximum warming around the geographic equator.

	We must therefore distinguish air circulation in the vicinity of the tropics, characterized by strong vertical movement due to thermal convection, and circulation at mid-latitudes, made almost of horizontal movements:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=1]{img/engineering/wind_circulation_earth.jpg}
		\end{center}	
		\caption[]{Simplified schematic and idealized caricature of wind circulation on Earth (source: ?)}
	\end{figure}
	Let us suppose that for a moment that we completely stopped the movement of air in the atmosphere relative to the surface of the planet, and that we should leave then start turning from West to East (left to right on the images) from the rest position. The pressure gradient force causes the air to move from high pressure areas to low pressure areas ("vacuum call"). These convective movements are named "\NewTerm{Hadley cells}\index{Hadley cells}".
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.7]{img/engineering/hadley_and_ferrel_cells.jpg}
		\end{center}	
		\caption[]{Hadley and Ferrel cells (source: \url{www.climatica.org.uk})}
	\end{figure}
	However, when the rotation movement began the Coriolis force (due to the rotation of the Earth) deflects the North-South winds towards the West and South-North winds eastward for an observer falling in the north Pole (\SeeChapter{see section Classical Mechanics}). We see therefore the formation of cyclones turning in in the opposite clockwise direction  in the Northern Hemisphere and vice versa in the Southern hemisphere (due to the direction of the vector $\vec{\omega}$ in this part of the hemisphere).
	
	The higher the air velocity increases, the more the Coriolis force increases accentuating the deviation. Eventually the Coriolis force reaches a value equal and opposite to that of the force of the pressure gradient, thus producing a flow of constant velocity (no acceleration), parallel to the isobars thus defining the geometric limit of the Hadley cell. This is what we name "\NewTerm{geostrophic balance}\index{geostrophic balance}". In practice, the flow outside the tropics is almost always in quasi-geostrophic equilibrium.

	In the absence of direct wind measurement, meteorologists can estimate the wind speed at a given point by measuring on a weather map the pressure gradient and the latitude. The geostrophic approximation is purely diagnostic. It has no predictive value because its equation contains no term of change depending on time.
	
	In the tropics, where the Coriolis force is becoming weaker until to be zero at the equator, there are other forces, such as centrifugal force, that balance the pressure gradient force.

	This is what we will prove here mathematically using Continuum Mechanics (fluids) and Classical Mechanics (see corresponding sections for the prerequisites).
	
	We know that in our system intervenes pressure forces (gradient), centrifugal forces, gravity forces. Forces to which we must not forget to add the Corlios force (implicitly its corresponding acceleration) of the system (meaning: the cyclone) the pulsation $\vec{\omega}$ (\SeeChapter{see section Classical Mechanics}):
	
	and the Coriolis force (implicitly: acceleration) of the fluid by unit mass (the reason for this unit choice will appear obvious few paragraphs further below) with respect to the pulsation $\Omega$ of the Earth:
	
	Thus, as we know (\SeeChapter{see section Classical Mechanics}), the Coriolis force will tend to deflect any descending movement to the right (East) in the Northern Hemisphere and any ascending movement to the left (West) in the Southern hemisphere (following we place ourselves and look in the direction of fluid movement in the previous figures).

	This is why the air at the base of the Hadley cell, traveling at low altitude from the Tropic to the Equator will be deflected to the West to give the east trade winds.

	We have also proved in the section of Continuum Mechanics a particular form of the Euler equation of the second form that was:
	
	Redesigned, this relation can also be written:
	
	But we had also proved that:
	
	It comes in the Earth reference frame:
	
	It is therefore of the equation defining the pressure within the fluid considered as isolated. In this relation, we must still subtract the Coriolis pressure  forces due to the geocentric reference frame to get the system the dynamic of the "cyclone"
	
	Which finally gives:
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=1]{img/engineering/north_south_earth_slice_for_cyclogenesis.jpg}
		\end{center}	
		\caption[]{North-South slice of Earth for cyclogenesis study}
	\end{figure}
	If we zoom on the reference frame related to the cyclone and translate to it the vector pulse of the Earth, we have:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=1]{img/engineering/north_south_earth_slice_for_cyclogenesis_zoom.jpg}
		\end{center}	
		\caption[]{Reference frame linked to the cyclone with the planar pulse}
	\end{figure}
	Therefore:
	
	Therefore we have:
	
	As we study the movements (almost) horizontal in the atmosphere at this latitude, we can consider that the fluid particles are liable to remain in the horizontal plane $(\vec{e}_x,\vec{e}_y)$. The components of the Coriolis force a plane motion are therefore ($v_z=0$):
	
	where $f$ is named the "\NewTerm{Coriolis parameter}\index{Coriolis parameter}". So the Coriolis force in oceanography and meteorology is traditionally denoted:
	
	The number $f$, positive in the northern hemisphere, negative in the southern hemisphere, ranging from $0$ to $1.458$ at the poles while the force is of the order of thousandths of Newton for fluid masses (ocean currents) and of the same magnitude (because the speed compensates for the low density) for gas (air currents).
	
	We now apply the approximation of the geostrophic equilibrium, that is to say that we consider that the air is animated by a uniform rectilinear motion (geostrophic wind), in other words, we neglect the action of centrifugal force due to the rotation of the vortex relatively that of the Coriolis force due to the rotation of the Earth, which means we assume that:
	
	with $R$ being the radius of the vortex and $\omega$ its pulsation. Since (\SeeChapter{see section Classical Mechanics}):
	
	the latter inequality becomes:
	
	where:
	
	is named the "\NewTerm{Rossby number}\index{Rossby number}" and has no dimensions. A small Rossby number signifies a system which is strongly affected by Coriolis forces, and a large Rossby number signifies a system in which inertial and centrifugal forces dominate. For example, in tornadoes, the Rossby number is large ($\cong 10^3$), in low-pressure systems it is low ($\cong 0.1-1$) and in oceanic systems it is of the order of unity, but depending on the phenomena can range over several orders of magnitude ($\cong 10^2-10^2$). As a result, in tornadoes the Coriolis force is negligible, and balance is between pressure and centrifugal forces (named "\NewTerm{cyclostrophic balance}\index{cyclostrophic balance}). 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For the middle latitudes ($\lambda=45^\circ$), the experiences and measures give $f=10^{-4}\;[\text{m}\cdot\text{s}^{-1}]$ and $v=10\;[\text{m}\cdot \text{s}^{-1}]$. The limit value for which $R_0=1$ is $R=100\;[\text{km}]$. For a larger scale, that is the case with hurricanes where $R\cong 100\;[\text{km}]$ we are close to the geostrophic balance. For an  a smaller scale, Coriolis is negligible and the wind is accelerated from high to low pressure.
	\end{tcolorbox}
	In other words the Rossby number therefore represents the ratio between the forces of inertia and the forces due to rotation that characterize the motion of a fluid in a rotating frame.

	So we can make the difference between a geophysical flow wight a large Rossby number or small Rossby number. If the Rossby number is greater than one, then the Coriolis forces due for example to the Earth's rotation are negligible relatively to the flow inertia. Otherwise for a Rossby very smaller than unity, the Coriolis forces dominate the movement of the fluid.

	Thus, if one approaches the equator, $f$ tends to $0$ the Rossby number becomes very large and at the poles it becomes very weak respectively.

	Under this approximation, our Euler equation can be written as:
	
	and since we are interested only to the horizontal plane of the atmosphere this simplifies even more to the form:
	
	Thus in fully developed and vector form by taking back the capital $P$ for pressure as it is customary in meteorology:
	
	It comes then:
	
	Therefore:
	Therefore in conventional form:
	
	The norm being given by:
	
	Therefore:
	
	which is named the "\NewTerm{equation of geostrophic winds}\index{equation of geostrophic winds}".
	
	Four scenarios are to be considered (with some reminders about some concepts already introduced at the beginnin of the section):
	\begin{enumerate}
		\item We are in the northern hemisphere and therefore $f$ is positive. Let us suppose that $\mathrm{d}P / \mathrm{d}R$ is positive, then the pressure increases away from the vortex center (which latter is therefore a minimum of low pressure). Therefore $v$ is positive and we have a vortex named "depression" in the northern hemisphere. Thus, the fluid (the wind) blows around the depression counterclockwise (West direction) in the Northern Hemisphere.

		\textbf{Definitions (\#\mydef):} 
		\begin{enumerate}
			\item[D1.] A "\NewTerm{depression}\index{depression}" (or "low pressure") is an area where the atmospheric pressure decreases horizontally toward a center of low pressure, that is to say a local minimum pressure.
	
			\item[D2.] The intense weather systems circulating around a closed center of low pressure (like a vacuum that attracts clouds from which the fact such systems are visible on satellite photos) consistently receive in  general the term "\NewTerm{cyclone}\index{cyclone}" or "\NewTerm{tropical cyclone}\index{tropical cyclone}".
		\end{enumerate}
		\begin{figure}[H]
			\begin{center}
				\includegraphics[scale=0.8]{img/engineering/cyclones.jpg}
			\end{center}	
			\caption[]{Some northern hemisphere cyclones}
		\end{figure}
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		On Earth we frequently associate depressions to bad weather e because the dynamics surrounding depression presupposes the existence of updrafts (can hardly get into the ground so the only way to escape is up!) That cause clouds and precipitation. In addition, the pressure gradient over a depression can cause strong winds.
		\end{tcolorbox}
	
		\item We are still in the Northern Hemisphere and therefore $f$ is positive. Let us suppose that $\mathrm{d}P / \mathrm{d}R$ is negative this time, the pressure then decreases away from the vortex center (which is thus reaching a maximum pressure). Therefore $v$ is negative and we have a vortex naed a "\NewTerm{high pressure}\index{high pressure}" in the northern hemisphere. Thus, the fluid (wind) blows around the high pressure clockwise (to the East) in the Northern Hemisphere.

		\textbf{Definitions (\#\mydef):} 
		\begin{enumerate}
			\item[D1.] A "\NewTerm{high pressure}\index{high pressure}" is an area where the atmospheric pressure increases horizontally toward a center of high pressure, that is to say a local maximum pressure.
	
			\item[D2.] The intense weather systems to flow around a closed center of high pressure (as a fan it rejects and disperses the clouds from which the fact that the high pressure are not simply visible on satellite photos) consistently receive the name of "\NewTerm{anticyclone}\index{anticyclone}".
		\end{enumerate}
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		Anticyclones usually bring good weather and clear skies. Atmospheric dynamics causes air at medium altitudes to be relatively warm and dry, and therefore without clouds.
		\end{tcolorbox}

		\item We are still in the southern hemisphere and therefore $f$ is negative. Let us suppose $\mathrm{d}P /\mathrm{d}R$ is positive, then the pressure increases away from the vortex center. Therefore $v$ is negative and we have a vortex still named a "high pressure" (or "anticyclone") in the Southern Hemisphere. Thus, the fluid (wind) blows around the high pressure but counterclockwise (to the West) in the Southern Hemisphere.

		\item We are still in the southern hemisphere and therefore $f$ is negative. Let us suppose that $\mathrm{d}P/\mathrm{d}R$ is negative, the pressure decreases away from the vortex center. Therefore $v$ is positive and we have a vortex still named "low pressure" (or "Hurricane") in the Southern Hemisphere. Thus, the fluid (the wind) blows around the low pressure clockwise (to the East) in the Southern Hemisphere.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is therefore possible to say generally on larger dimensions that the wind comes from high pressure (anticyclone) to move towards low pressure (depression).
	\end{tcolorbox}
	Here is an illustration example of a depression (anticyclone) and high pressure (cyclone) in the Northern Hemisphere such as represented by professional practitioners of meteorology:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/low_high_pressure.jpg}	
		\caption{Low pressure and high pressure illustration}
	\end{figure}
	We can indeed observe that depression (D) rotates counterclockwise and the high pressure (A) clockwise (and vice versa in the southern hemisphere).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is therefore possible to say generally on larger dimensions that the wind comes from high pressure (anticyclone) to move towards low pressure (depression).
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Tides}
	Tides are the rise and fall of sea levels caused by the combined effects of the gravitational forces exerted by the Moon and the Sun and the rotation of the Earth. The times and amplitude of tides at a locale are influenced by the alignment of the Sun and Moon, by the pattern of tides in the deep ocean, by the amphidromic systems of the oceans, and the shape of the coastline and near-shore bathymetry. Tides vary on timescales ranging from hours to years due to a number of factors.

	Tidal phenomena are not limited to the oceans, but can occur in other systems whenever a gravitational field that varies in time and space is present. For example, the solid part of the Earth is affected by tides, though this is not as easily seen as the water tidal movements.
	
	Among the phenomena of nature, the tide is one of the most majestic in its scope and power, one of the more surprising by its regularity and the discretion of its causes. We can then easily understand not only that it is imposed to the attention of browsers but it has, since the remotest antiquity, aroused the research of the most distinguished scholars.

	To approach the subject of the tides in a simply way, we can from a logical conclusion: If the lunar attraction was identical at each point of the Earth, there would be no tides. We must therefore approach the study of tides on the differences of forces. The influence of the moon on the tides is named the "\NewTerm{diurnal component}\index{diurnal component}".
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.74]{img/engineering/mont_saint_michel_tides.jpg}
		\caption{High and low tide at Mont Saint-Michel in France}
	\end{figure}
	Another nice phenomenon to study mathematically (and to see in real life or on YouTube) are the "tidal bore" which is the leading edge of the incoming tide forms a wave (or waves) of water that travels up a river or narrow bay against the direction of the river or bay's current:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/engineering/tidal_bore.jpg}
		\caption{Tidal bore}
	\end{figure}
	
	\subsubsection{First approach}
	Let us consider for the naive study of tilde a mass of water $m$ at the equator and at the poles. We will calculate the attractive force on the mass relatively to the center of the Earth and taking into account the influence of the Moon that has a mass $M_L$ (Lunar influence).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/tide_first_approach.jpg}
		\caption[]{Earth-Moon configuration for a first study of tides}
	\end{figure}
	Let us first compute the force $F_a$ at the equator to the nearest point to the Moon relatively to the figure above.

	Then we have:
	
	considering that $R \gg r$ and denoting the "\NewTerm{static tidal force}\index{static tidal force}":
	
	A numerical application for a mass $m= 1$ [kg] gives $f=1\cdot 10^{-6}\;[\text{m}\cdot \text{s}^{-1}]$.

	In vector form the previous relations is obviously written:
	
	As the Earth-Moon distance is approximately $60$ Earth radii, the intensity of acceleration varies approximately linearly (...) along the land portion of a line passing through the center of the Moon. This is particularly the case for the segment that connects the two antipodal points $A$ and $C$ in the figure above. We can write, O denoting the center of the Earth, that:
	
	We must now have to separate the two contributions of the Moon:
	\begin{enumerate}
		\item The force $\vec{F}_0$ applied on the center of mass $G$ is then uniform to the planet by construction. It is this force that is responsible for the revolution of our planet around the common center of mass of the two celestial objects.

		\item The esidual term $\pm \Delta \vec{F}_L$ is superimposed and takes opposite values at the antipodes. It is responsible for the tides (in the first approximation in this simplistic model).
	\end{enumerate}
	Thus, the force due to the Moon is of opposite sign to the horizontal. We then have two (lunar) tides per day at antipodal locations:
	\begin{itemize}
		\item That of the Moon who attracts (from this side of the Earth)

		\item That of the Moon that pushes (at the opposite side of the Earth)
	\end{itemize}
	Or schematically (without any respect for real proportions):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/tide_antipodal.jpg}
		\caption[]{Antipodal tide with antipodal bulge}
	\end{figure}
	If we considered the surface of the Earth as perfectly spherical and covered with water, it would then take the form of an ellipsoid whose axis would be directed to the celestial object generating the tide. We should then observe high tide and low tide that would take place twice a day and always at the same time. We name this the "\NewTerm{static tide}\index{static tide}" and the corresponding model "\NewTerm{static model of the tides}\index{static model of the tides}".

	It should be noted that the subtle play between the rotation of the Earth and the Moon produces enormous friction in water volume that have the effect of slowing the speed of rotation of the Earth about $1$ second every thousand years.
	
	\pagebreak
	\subsubsection{Second approach}
	For the second approach, which is worth seeing for general culture and also because it presents another interesting aspect of the explanation of the attraction between two celestial objects, let us consider the following scheme:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/tide_second_approach.jpg}
		\caption[]{Antipodal tide with antipodal bulge}
	\end{figure}
	We have proved in the section Trigonometry the cosine theorem (Al-Kashi formula) which gives us for recall:
	
	Hence the gravitational potential (\SeeChapter{see section Astronomy}):
	
	But:
	
	and the gravitational potential is of the form:
	
	Which we can develop in series of Maclaurin (\SeeChapter{see section Sequences and Series}) until order $2$:
	
	The gravitational potential then becomes:
	
	If we keep only the terms of power $1$ and $2$ on $r / a$, it remains:
	
	The first term of the potential is:
	The first term of the potential is:
	
	It is the potential for $r=0$, that is to say the potential created by the Moon at the center of the Earth.
This term contains no variable, it is constant and therefore its gradient is zero, it gives no force since:
	
	The second term:
	
	contains the two variables $r$ and $\theta$. Its gradient will not be zero. It will generate a gravitational force that we will calculate, but using a trick.

	Since:
	
	It comes:
	
	Consequently, the gradient in Cartesian coordinates is reduced for the mass $\mathrm{d}m$ situated at point $A$ by:
	
	Thus all the elements of the Earth undergo from the Moon parallel forces (on the $x$-axis only) directed towards the Moon. The total mass of the Earth is the sum of all these masses and the total force that the Earth undergoes on the part of the Moon is the sum of the elementar forces. Therefore:
	
	And therefore the total force is only along the $x$-axis and is given by:
	
	This force is the same as if all the mass of the Earth were concentrated at the center $T$ and if the whole mass of the Moon were concentrated at the center $L$. The Moon undergoes on the part of the Earth the same force of contrary sense, it is the principle of mutual actions. It is the latter which forces the Moon to revolve around the Earth.

	The third term is the one responsible for the tides:
	
	The force deriving from the gradient in polar coordinates is given by (\SeeChapter{see section Vector Calculus}):
	
	Therefore, the radial component of the force is:
	
	The orthoradial component:
	
	Therefore:
	
	For a mass $m$ of water, the tidal force is:
	
	For $\theta=\{0,\pi/2,\pi\}$, the orthoradial component $F_\theta$ vanishes. We then have for these three angles:
	
	For $\theta=0$ the force is only radial:
	
	The sign of this force is negative and it is directed towards the Moon. For $\theta=0$ the force is also only radial and we fall back on the same expression:
	
	Therefore, the amplitude is the same for the angle $\theta=0=\pi$.
	
	On the other hand, if the reader remember that to return to Cartesian components we have (\SeeChapter{see section Vector Calculus}):
	
	We see then that for $\theta=0$, the force is oriented in the positive direction of the $x$-axis since $x$ will be positive (but zero following $y$). There is therefore a tide in the direction of the Moon (almost intuitive).

	We also see that for $\theta=\pi$, the force is oriented in the negative direction of the $x$-axis since $x$ will be negative (but zero following $y$). There is therefore a tide in the opposite direction to the moon (counter-intuitive for most people).

	For $\theta=\{\pi/2,3\pi/2\}$, the component $x$ is zero and the radial component will be:
	
	And it is directed towards the center of the Earth since:
	
	We see then that for $\theta=\pi/2$ the force is oriented in the positive direction of the axis $y$ since $y$ will be positive (but zero following $x$). We also see that for $\theta=3\pi/2$, the force is oriented in the negative direction of the $y$ axis since $y$ will be negative (but zero following $x$).

	In reality, the tides are much more complex than the above model (eccentricity of the lunar orbit, superimposition of the diurnal tide, lunar orbit, Moon-Sun alignment, inclination of the plan of the Moon's orbit, equinoxes, etc.). Here is a superb series of images of the elevation of the surface of the oceans in meters, on one tide cycle , calculated from a more elaborate model:
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_01.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_02.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_03.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_04.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_05.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_06.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_07.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_08.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_09.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_10.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_11.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_12.jpg}
		\end{subfigure}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_13.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_14.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_15.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_16.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_17.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_18.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_19.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_20.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_21.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_22.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_23.jpg}
		\end{subfigure}
		\begin{subfigure}{.4\textwidth}
		  \centering
		  \includegraphics[width=1\linewidth]{img/engineering/tide_24.jpg}
		\end{subfigure}
		\caption{Real complexity of tides on Earth (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The phenomenon is therefore due to the deformation of the surface of the oceans as a result of the combined attractions of the celestial bodies. This movement can even destroy the celestial boject which undergoes it: if the tidal force prevails over the gravitational force of its constituents, the celestial objects disintegrates. This limit where the tidal forces outweigh the gravitational force is named the "Roche limit" (\SeeChapter{see section Astronomy}).
	\end{tcolorbox}
	Besides the diurnal tide due to the attraction of the Moon, it is necessary to count on a tide due to the centrifugal force of the movement of the Earth and the Moon around their center of mass (but it depends on the latitudes, the relief and a lot of other parameters objectively because in some places of the planet there is only one tide per day). Indeed, the Earth and the Moon revolve around the center of mass which defines the orbit of Earth-Moon couple (the scales are not respected):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/tides_superposition.jpg}
		\caption{Principle of tides superimposed on diurnal tides}
	\end{figure}
	And we will ignore the tides of equinoxes and others... until today...
	
	\pagebreak
	\subsection{Lorenz equation}
	The "\NewTerm{free convection}\index{free convection}" or "\NewTerm{natural convection}\index{natural convection}" is the flow regime obtained when we heat a fluid without imposing any external flow. This is the case for atmospheric convection movements (hot gases in cold gases), convection movements of the molten rock responsible for plate tectonics, movements of hot water under pressure in geysers or cakes... and many other phenomena...

	These flows are inexplicable if we do not couple the equations of dynamics and thermodynamics!

	We will in this context establish the famous system of Lorenz equations at the price, however, of numerous approximations and assumptions in order to simplify as much as possible the calculations and the mathematical tools used (because at the time of the development of the model the computers were not what they are today).

	We will therefore prove in the context of convection (one of the important dynamics of our atmosphere) that the equations which determine certain parameters of the motion are very sensitive to the initial conditions, the purpose of which is to show the difficulty of predicting to a more or less long term with deterministic theoretical models (reason why in meteorology we use nowadays the method of the finite elements).

	A priori, the density $\rho$ is a function of temperature and pressure by the state law of perfect gases (\SeeChapter{see section Continuum Mechanics}). It is therefore natural to think that if we heat a wall, the temperature of the surrounding fluid increases by diffusion. The pressure stratification is changed, the pressure gradient creates the movement.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/free_convection.jpg}
		\caption{One of the everyday life most known convection...}
	\end{figure}
	In all sections of this book, we have so far neglected any variation of $\rho$. But decoupling is no longer valid here since it is heating that causes movement. We will therefore allow a variation of the density with the heating assuming however that this perturbation is small. It is therefore necessary to reintroduce a variation of $\rho$ around an equilibrium position: rest. On the other hand, the viscosity will be assumed constant...

	Thus, given a far fluid at rest and at the temperature $T_{\infty}$, it is in the presence of a wall heated to the temperature $T_P$. To obtain the dependence of $\rho$, let us recall some classical thermo-elastic coefficients (\SeeChapter{see section Thermodynamics}):
	\begin{itemize}
		\item Coefficient of isobaric compressibility (or dilatation according to the writing in terms of density) :
		

		\item Isothermal compressibility coefficient:
		
	\end{itemize}
	By assuming now that density is mainly related to temperature (for simplicity) we can write (this hypothesis works well for fluids but not too much ... for gas !!):
	
	Using the general form of Taylor's development (\SeeChapter{see section Sequences and Series}):
	
	We then have an approach with the engineer way ...:
	
	Therefore:
	
	where $\bar{T}$ is therefore a coefficient without dimensions (as $\varepsilon$ ...) more easily measurable experimentally.

	The continuity equation (\SeeChapter{see section Thermodynamics}) or of mass balance that is for recall:
	
	Then becomes:
	
	to the first order in $\varepsilon$. Moreover, we have proved in the section of Continuum Mechanics  that if the fluid is incompressible:
	
	Let us remember that in a first approximation the fluid is incompressible. It then only remains:
	
	As we wish to study a flow in the presence of gravity, it would be judicious to put:
	
	and therefore to focus only on variations around the hydrostatic equilibrium position ($\delta p$ is dimensionless!). We have proved still in the same section of Continuum Mechanics that in the case of the incompressible fluid with viscosity, the first form of Euler's equation (equation of motion):
	
	Let us first consider the both terms:
	
	which are written along the $Z$ axis:
	
	When there is movement, the projection along $Z$ thus make appears:
	
	which we then rewrite:
	
	Therefore:
	
	since:
	
	It comes:
	
	It remains therefore a buoyancy force directed upward.

	The variation of the density as a function of temperature in the product $\rho\mathrm{d}_t\vec{v}$ of the relation:
	
	will be neglected ($\rho\mathrm{d}_t\vec{v}\cong \rho_\infty \mathrm{d}_t\vec{v}$) because we will restrict ourselves in case where the speed is small. We have then by reintroducing the viscosity...:
	
	and we have then material derivative (\SeeChapter{see section Continuum Mechanics}):
	
	there also a useful relation:
	
	We then have as expression of the force density:
	
	To continue, we will seek to determine the energy law of the equation of behavior proved in the section Continuum Mechanics that was for recall
	
	so that it also accounts for the relation between the stresses and thermodynamic characteristics of the fluid, such as heat flow and temperature. We will do this by characterizing the diffusion of energy in the medium due to the effects  (supposedly decoupled) of the viscosity of the fluid and the thermal conduction of the fluid.

	We rewrite this relation with new constants and another notation for the divergence:
	
	where $\mu_d$ and $\lambda$ are in this context named the "\NewTerm{Lamé coefficients}\index{Lamé coefficients}" (also named the "\NewTerm{Lamé parameters}\index{Lamé parameters}" or "\NewTerm{Lamé constants}\index{Lamé constants}"). The terms $\lambda$ and $\mu$ are individually referred to as "\NewTerm{Lamé's first parameter}" and "\NewTerm{Lamé's second parameter}".
	
	We have also proved in the sectoin of Continuum Mechanics the relation:
	
	Therefore:
	
	Which gives:
	
	Let us denote the total energy as:
	
	where $e$ is the internal mass energy of the fluid (so related to a unit of fluid mass). Now the instantaneous variation of the internal energy of the fluid is equal to the contribution of a mechanical power and of the supply of heat (according to what has been seen in the section of Thermodynamics):
	
	where $P$ gives the power of the external forces  to the system given necessarily by the force of the ambient potential field and the mechanical forces given by the stress tensor only (we are always in the situation of a perfect fluid). That is:
	
	and using the Ostrogradsky theorem (\SeeChapter{see section Vector Calculus}):
	
	which has indeed the units of a power we have indeed:	
	
	For the heat power $\dot{Q}$ it is very easy also thanks to the developments we had made in the section of Thermodynamics where we obtained the equation of heat:
	
	Therefore:
	
	We have finally:
	
	So all this gives us the equation of the energy of a fluid:
	
	Therefore:
	
	and as (\SeeChapter{see section Thermodynamics}) the heat flow follows the Fourier law:
	
	We have then:
	
	Either by using the Laplacian definition of a scalar field (\SeeChapter{see section Vector Calculus}):
	
	Or explicitly:
	
	Now, by making the scalar product of:
	
	With the velocity $\vec{v}$ we get the balance of the kinetic energy:
	
	That is to say explicitly:
	
	By subtracting (just proved earlier): 
	
	and (just proved also earlir above):
	
	we get a local relation of the specific internal energy $e$:
	
	That is to say explicitly:
	
	But we also have (derivation of a product):
	
	Therefore:
	
	Indeed:
	
	We have therefore:
	
	And as the stress tensor $\sigma$ is symmetric:
	
	we have therefore:
	
	which is sometimes written (yes I also don't believe what I see sometimes...):
	
	where $\overline{\overline{D}}$ is named the "\NewTerm{tensor of deformation rate}\index{tensor of deformation rate}" and $\overline{\overline{\sigma}} : \overline{\overline{D}}$ represents the double contracted product of the stress tensor and deformation rate tensor.

	We have proved, for recall, in the section of Continuum Mechanics that:
	
	where:
	
	Therefore, it is simple to make the difference between normal forces and tangential forces. Whatever, to come back on the energy equation:
	
	and replacing explicitly the stress tensor $\sigma$, we get:
	
	But in our case:
	
	therefore we can write:
	
	But we also have (using the trace as seen in the section of Linear Algebra):
	
	We have therefore:
	
	Thus into technical condensed form (just for information...):
	
	It is clear that from the point of view of the entropy (\SeeChapter{see section Thermodynamics}) we have:
	
	We also have:
	
	Thus reduced to the massic values:
	
	The time variation giving:
	
	But with have the continuity equation (\SeeChapter{see section Thermodynamics}):
	
	Which gives finally:
	
	or written slightly differently:
	
	Injected in:
	
	This gives:
	
	If we consider the velocity gradient to be very small (quasi-static) then we can write the approximation:
	
	Let us now give the expression of the entropy (exact total differential) as a function of the temperature and pressure parameters only:
	
	Either in massic form:
	
	But have also proved in the section Thermodynamics the following relation:
	
	Either in massic form:
	
	Which gives us:
	
	But, we have also proved in the section Thermodynamics, one of the following Maxwell's relations:
	
	Either in massic form:
	
	hence:
	
	Therefore:
	
	Then our relation:
	
	can be written:
	
	If we assume that the variation of the density with the temperature is small, we then have at the atmospheric scale:
	
	and remembering that (material derivative):
	
	It finally comes:
	
	We now have two important equations:
	
	Therefore:
	
	
	\subsubsection{Rayleigh-Bénard convection cells (Benard-Marangoni instability)}
	Let us now briefly examine the Rayleigh-Bardard\index{Rayleigh-Bénard convection cells} problem, which consists of two plates limiting one fluid being more heated than the other.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The "\NewTerm{Benard-Marangoni instability}\index{Benard-Marangoni instability}" is a free surface configuration of Rayleigh-Benard instability. In fact, a thin layer of fluid is placed on a horizontal plane but its upper face is not in contact with an other plane but it is a free surface in contact with the air. The temperature of the lower plane must be superior to the temperature of the air to develop instabilities.
	\end{tcolorbox}
	We can then observe parallel longitudinal rolls in a film of viscous fluid (silicone oil) held between two plates at a hot temperature at the bottom and cold at the top. Here is a picture of these rolls seen from sides (but normally we don't notice it, because they only get visible when you add mark-particles):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/rayleigh_benard_convection_cell_oil_slice.jpg}	
		\caption{Rayleigh-Bénard convection cells (instability)}
	\end{figure}
	view from the top:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/rayleigh_benard_convection_cell_oil_from_top.jpg}	
		\caption{Rayleigh-Bénard convection cells viewed from top (instability)}
	\end{figure}
	It is a problem of natural convection: the heated fluid at the bottom expands and ascends entrained by the Archimedes principle, arrived at the top it cools and falls down. It is this movement that must be explained that is similar to that of the Earth's atmosphere, sun convection cells and also Earth's mantel dynamics as illustrated below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/rayleigh_benard_convection_cell_sun.jpg}	
		\caption{Rayleigh-Bénard convection cells on our sun\index{solar granulation} (source: Swedish Vaccum Solar Telescope 1997-07-10)}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/engineering/rayleigh_benard_convection_cell_earth.jpg}	
		\caption{Rayleigh-Bénard convection cells on Earth (source: ?)}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/rayleigh_benard_convection_cell_earth_mantle.jpg}	
		\caption{Rayleigh-Bénard convection cells as supposed in Earth mantle (source: ?)}
	\end{figure}
	We also notice experimentally that the convection movements are made approximately according to a torus (see the photo seen from the side just above). We can take advantage of this symmetry to simplify the analysis.
	
	Let us consider then a vertical loop of fluid circulating at constant speed (thus without too much turbulence ...):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/rayleigh_benard_convection_cell_simplified_illustration.jpg}	
		\caption{Vertical fluid loop with gradient in homogeneous gravific field}
	\end{figure}
	The configuration will be set as follows:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/rayleigh_benard_convection_cell_configuration_study.jpg}	
		\caption{Required configuration for the theoretical model of convection cell}
	\end{figure}
	Where $T_0$ is the average temperature of the fluid (caution!: do not forget that it is not an extensive quantity!) and where we indicated respectively the temperatures inside the torus and outside it (ie the environment ) which may all vary with time.

	We see that the difference of temperature is of $2T_2$ between the top and the bottom and of $2T_3$ between the right and the left.

	We put that the temperature varies linearly with the height (which of course is false in an atmospheric model ...):
	
	We notice that it is possible to parametrise the temperature along the inside of the torus with the following relation (parametric equation of the circle):
	
	We have then according to the figure above:
	
	Having put this, let us return to:
	
	We will pass this system in polar coordinates best matching the geometry of our problem. Let us recall first that in term:
	
	the differential operator $\vec{\nabla}$ is the divergence. Now, we have proved in the section of Vector Calculus that this one was then written in polar coordinates:
	
	Now, if we rely on the hypothesis that in the volume of the torus the velocity varies neither as a function of the angle nor inside the torus (and therefore does not vary according to the radius $r$), then in polar coordinates :
	
	We have then:
	
	We will reduce the analysis to a single dimension which will be that the phenomenon depends only on the angle. We then have in polar coordinates and explicating all the terms:
	
	where we have at the same time made the projection along the $z$ axis such that:
	
	The differential coefficient of the last term will bother us. We replace it by a coefficient $\Gamma$ which we assume to be constant and which opposes the motion such as we have:
	
	or more explicitly:
	or more explicitly:
	
	We now integrate this on the whole loop as a function of $\phi$. We have then:
	
	We then have the pressure term which disappears, because there is no pressure gradient along the loop. So:
	
	We then after (\SeeChapter{see section Differential and Integral Calculus}):
	
	and:
	
	Then it remains:
	
	Therefore:
	
	We see in this equation that the motion is driven by the horizontal temperature difference $T_3$.

	Now, let us come back to:
	
	If we neglect the tangential forces inside fluid, then we have:
	
	where $D$ is the thermal diffusion coefficient (\SeeChapter{see section Thermodynamics}).

	In polar coordinates this is reduced to:
	
	and we will also make another approximation:
	
	And we have the two relations:
	
	By subtracting:
	
	Therefore:
	
	and also:
	
	Therefore:
	
	After derivation:
	
	We group together the terms:
	
	We then have the following three differential equations that govern the dynamics of the system:
	
	We finish the multiple simplifications by putting...:
	
	Which gives us:
	
	Putting it this in a more clean way, we get:
	
	Now, let introduce the following dimensionless variables:
	
	where we can assimilate:
	\begin{itemize}
		\item $X$ at the non-dimensional speed

		\item $Y$ to the non-dimensional temperature difference between rising and falling currents

		\item $Z$ to the dimensionless deviation of the convection equilibrium
	\end{itemize}
	We then have indeed:
	
	Therefore:
	
	In an even more condensed and traditional form:
	
	where we have:
	
	where we have:
	
	that correspond to the "\NewTerm{Prandtl number}\index{Prandlt number}" that a dimensionless number, named after the German physicist Ludwig Prandtl, defined as the ratio of "\NewTerm{momentum diffusivity}\index{momentum diffusivity}" to "\NewTerm{thermal diffusivity}\index{thermal diffusivity}" and where for recall:
	\begin{itemize}
		\item $\Gamma$ is the momentum diffusivity (kinematic viscoscity) in $[\text{m}^2\cdot \text{s}^{-1}]$
		\item $K$ is the thermal diffusivity in $[\text{m}^2\cdot \text{s}^{-1}]$
		\item $\mu_d$ is the dynamic viscoscity in $[\text{Ps}^2\cdot \text{s}]$
		\item $\kappa$ is the thermal conductivity in $[\text{W}\cdot \text{m}^{-1}\cdot\text{K}^{-1}]$
		\item $c_P$ is the specific heat in $[\text{J}\cdot\text{kg}^{-1}\cdot\text{K}^{-1}]$
	\end{itemize}
	with some typical values:
	
	And:
	
	which is assimilated to the "\NewTerm{Rayleigh number}\index{Rayleigh number}"  that is also a dimensionless number associated with "\NewTerm{buoyancy-driven flow}\index{buoyancy-driven flow}", also known as "\NewTerm{free convection}\index{free convection}" or "\NewTerm{natural convection}\index{natural convection}". When the Rayleigh number is below a critical value for that fluid, heat transfer is primarily in the form of conduction; when it exceeds the critical value, heat transfer is primarily in the form of convection.
	
	\subsubsection{Lorenz attractor and chaos}
	The previous system of three equations is essentially the same as that of the famous "\NewTerm{Lorenz system}\index{Lorenz system}". With one difference (!), the (real) Lorenz system contains a factor $b$ in the last equation (which does not change the result anyway since we still get a strange attractor in the end as we go on see):
	
	Pr, Re and $b$ are strictly positive, and we often put $\text{Pr}=10$, $\text{Re}=28$, $b=8/3$ where the Prandtl number corresponds to the value of that of water.

	The Lorenz equations describe the convection phenomena of an ideal two-dimensional fluid in a reservoir heated from below.

	We see by this by this proof that contrary to the statements not demonstrated on many Internet pages and forums that:
	\begin{enumerate}
		\item The system is by no far not simple mathematically and is very approximate

		\item That there are truly simpler and chaotic systems (\SeeChapter{see section Population Dynamics})
	\end{enumerate}
	The interest of the Lorenz equations lies, however, in the sensitivity to the initial conditions and the convergence of the dimensionless variables.

	Let's see an example with Maple 4.00b:

	\texttt{>with(DEtools):\\
		>lorenz:=diff(x(t),t) = 10*(y(t)-x(t)),diff(y(t),t) = 28*x(t)-y(t)-x(t)*z(t),\\
		diff(z(t),t) = x(t)*y(t)-8/3*z(t);\\
		>DEplot3d({lorenz}, [x(t),y(t),z(t)], t=0..100, stepsize=0.01, 
		[[x(0)=10, y(0)=10, z(0)=10]], orientation=[-35,75], linecolor = t, thickness = 1);\\
	}
	
	This gives for the first $100$ units of dimensionless time:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_maple.jpg}	
		\caption{Phase Space of the Lorenz Equations System with Maple 4.00b}
	\end{figure}
	or for the first $10$ units of dimensionless time:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_first_ten_time_units_maple.jpg}	
		\caption[]{Phase Space of the Lorenz Equations System with Maple 4.00b}
	\end{figure}
	Well so far we realize that the dimensionless parameters revolve around two points named the "\NewTerm{strange attractors}\index{strange attractors}".

	\textbf{Definition (\#\mydef):} In the study of dynamic systems, an "\NewTerm{attractor}\index{attractor}" (or "\NewTerm{set-limit}\index{set-limit}") is a set, a curve or a space towards which a system evolves irreversibly in the absence of perturbations.

	Now always for the same values of adimensional time, we take $x(0)=10.1$, that is to say a relatively small change of the initial conditions. We have then:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_small_perturbation_maple.jpg}	
		\caption[]{Small variation in initial conditions of the Lorenz system with Maple 4.00b}
	\end{figure}
	We thus notice that the phenomenon is no longer really similar.

	Let us consider for example the variable $x$ taking as initial conditions:
	\begin{gather*}
		x(0)=10 \qquad y(0)=10 \qquad z(0)=10
	\end{gather*}
	then:
	\begin{gather*}
		x(0)=10 \qquad y(0)=10.1 \qquad z(0)=10
	\end{gather*}
	thus a small variation of $0.01$ on the value of $y(0)$.

	Either in Maple 4.00b:
	\texttt{>DEplot({lorenz}, [x(t), y(t), z(t)], t=0..15, stepsize = 0.01, [[x(0)=10, y(0)=10, z(0)=10],[x(0)=10, y(0)=10.01, z(0)=10]], scene = [t,x], linecolor = [blue,green], thickness = 1);}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_under_small_variation_of_initial_conditions_maple.jpg}	
		\caption[]{Analysis of the dimensionless variable $x$ for a small variation of initial conditions with Maple 4.00b}
	\end{figure}
	We see that the system shifts rather quickly from the initial model whereas at the beginning it remains identical but the overall shape remains.

	Another thing ... depending on the parameters the system can converge. Indeed, by changing the factor $28$ by the value $22$ we have for example (convergence on the left):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_convergence_maple.jpg}	
		\caption[]{Convergence of the Lorenz system with Maple 4.00b}
	\end{figure}
	or with the value $19$ the result is even more trivial:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_direct_convergence_maple.jpg}	
		\caption[]{Convergence of the Lorenz system with Maple 4.00b}
	\end{figure}
	Or with a value close to $1$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_direct_fast_convergence_maple.jpg}	
		\caption[]{Fast convergence of the Lorenz system with Maple 4.00b}
	\end{figure}
	We notice a last interesting case is that if the Prandtl number is $1$ then the system is stable:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lorenz_phase_system_stability_maple.jpg}	
		\caption[]{System Stability with Maple 4.00b}
	\end{figure}
	This sensitivity to the initial conditions, as well as the shape of the strange attractor of Lorenz, led the meteorologists to make a metaphor with the following sentence: can the flutter of a butterfly in Brazil cause a tornado in Texas? (By stopping the dissipation of the error due to the scales considered ...).

	Hence the name afterwards of "\NewTerm{Butterfly effect}\index{Butterfly effect}" for the study of the attractor of Lorenz also named in the field the "\NewTerm{Lorenz's butterfly}\index{Lorenz's butterfly}".
	
	\pagebreak
	\subsection{Waves}
	When one finds oneself on a boat, the movement of the water makes many people sick. It is also the case when one is in the galley to calculate this type of movement... Indeed, the waves in the water are neither transverse waves nor longitudinal waves. They are a bit of both. The water particles describe circles or ellipses, in a vertical plane parallel to the displacement of the wave. The big problem is that one of the boundary conditions is on the surface of the water and we do not know where that surface is. It is precisely one of the unknowns.

	The problem is arduous enough for this calculation not to be found in books of general or specialized physics. We then tried to include it here by simplifying enormously and keeping only the simplest cases. The following calculation can be taken as an example of how physicists simplify life by making approximations and also as a masterful example of a common but complex phenomenon to be modeled.

	We will not compute the wave equation. We must calculate the motion of the particles from the surface to the bottom of the water, since the movement extends to the bottom. But since particles describe ellipses, we need at least two variables per point: we need a vector field! We will therefore calculate the vector field of the velocities of the particles, but we will not calculate it directly. What we will find is a scalar potential of velocities whose gradient will be the vector field of velocities.

	A final word on the waves that we will calculate. These waves are named "\NewTerm{gravity waves}\index{gravity names}" (not to be confused with "gravitational waves" in General Relativity), because it is the weight of the water that acts as a restoring force to bring down the summits and ascend the hollows.

	I would like to thank Mr. Louis Peralta for his mathematical mastery and his pedagogy without which these exciting developments could never have been presented in this book. I am indebted to him, with his agreement, to copy / paste $99\%$ of his course material.

	We will make calculations in the simplest case: waves in a rectilinear channel of rectangular section and of constant width and depth... This reduces the dimensions of the problem to two (since the third one has an identical wave profile independently of where we are positioned in the width of the channel): the depth and direction of advance of the waves. Of course, we will neglect the friction on the bottom or sides of the canal and ... many other things.

	The coordinate system chosen is then:
	\begin{itemize}
		\item $x$ horizontal, positive in the direction of wave advancement (direction of channel length)

		\item $y$ horizontal in the direction perpendicular to the advancement of the waves (direction of the width of the channel). Again, no variable will depend on $y$ for the reasons already mentioned earlier

		\item $z$ vertical and positive upwards. The zero is chosen on the surface of undisturbed water. Under water, $z$ will therefore be negative. The depth of the channel being $h$, the bottom of the water will then be $z=-h$
	\end{itemize}
	Now, let us recall that we proved after very long developments in the section of Continuum Mechanics, that if the fluid is incompressible (divergence of the vector field of the velocities is therefore zero such that $\vec{\nabla}\circ\vec{v}=0$) and the dynamic viscosity is also ($\mu_d=0$), we then we have the Euler equation of the first form:
	
	because $U$ is for recall of the scalar potential of the gravitational field.

	What we will explicitly write in the following form in the case of our study of gravity waves:
	
	Before continuing, we will change the form of the term $\mathrm{d}_t\vec{v}$ which is a total derivative and not partial. Let us consider one of the two components ($x$ or $z$) of this term. Let us take the component $x$ of the velocity vector and use what we saw in the section of Differential and Integral Calculus (we omitted the $y$ component since the system does not depend on it):
	
	We have in our situation:
	
	Then we get:
	
	The first term $\partial_t v_x$ is a linear term, whereas the other two are quadratic terms (as they include the product of a velocity by the derivative of a velocity) which are an infection (...) in all formal calculations. We shall therefore limit our calculations to cases where these quadratic terms are negligible relatively to the linear terms (implicitly implying that the speed must vary very little, otherwise this hypothesis is no longer valid). With this restriction, we then have:
	
	We can then write:
	
	We will add an additional restriction to our calculations. This restriction consists in imposing that the rotational velocity is zero:
	
	That is to say, as we have seen in the section of Contiuum Mechanics, that there are no vortices in the fluid. We had already imposed that the velocity was zero in the $y$-direction, which prevents horizontal vortices horizontal axis of rotation). Now we eliminate the possibility of vortices with vertical axis of rotation also. It is not, in fact, too restrictive, we know that an object abandoned on the waves does not turn on itself unless it is caught in rolls. In the cases that we are going to calculate, the amplitudes are limited, we are in the linear case and, above all, not in the case of the rollers!!

	We have proved in the section of Vector Calculus that when the curl of a vector variable is zero, the variable can be expressed as the gradient of a scalar potential:
	
	It comes then that we can write:
	
	Then we get:
	
	What can be written:
	
	This equation acts as a wave equation, even if it does not directly describe the position or velocity of the particles. Once the equation of the function $\phi(x,z,t)$ are solved, we can deduce the velocities:
	
	It should not be forgotten that since the divergence of the vector field of the velocities is zero (incompressible field assumption):
	
	Therefore the scalar Laplacian of $\phi$ is zero:
	
	Therefore:
	
	with $-h<z<0$.
	
	The solution must satisfy boundary conditions. The most obvious is that the vertical velocity must be zero at the bottom of the water:
	
	To find the boundary conditions at the surface, we will define the variable $\xi(x,t)$ which is equal to the vertical displacement of a particle of water located on the surface of the water whose equilibrium position (in the absence of waves) is $x$ (not to be confused with the other $x$...!). The height of the surface of the water, measured by reference to the level of the water without wave, will therefore be given by $\xi(x,t)$. Thus, the vertical velocity $v_z$ at the surface of the water will be:
	
	This is a very subtle tip and not obvious at all to anticipate!

	At the surface, we can consider that the pressure $P$ is always equal to the atmospheric pressure for all the positions in $x$ and in $\xi$. Hence, a priori nothing will change if the atmospheric pressure changes (...), and it is more convenient to consider it null. With this, the equation:
	
	Becomes:
	
	From which we deduce:
	
	If we place ourselves on the surface, which corresponds to $z=0$, the preceding relation becomes:
	
	From which we deduce:
	
	If we place ourselves on the surface, which corresponds to $z=0$, the preceding relation becomes:
		
	and deriving this relation relatively to the time again, we get:
	
	Then using the relation proved just earlier above:
	
	It comes:
	
	Let us sum up the conditions that the scalar potential $\phi$ must satisfy:
	
	We will only look for solutions of the separable type (\SeeChapter{see section Differential and Integral Calculus}). That is to say that $\phi$, which is a function of $x$, $z$ and $t$, will then be able to be written as the product of three functions, each of which depends on a single variable. Moreover, we will work only in sinusoidal regime (an intuition ... of the observation in real life) and using the formalism of the phasors (\SeeChapter{see section Wave Mechanics}). Therefore, we have:
	
	As usual, we will only keep the real part of the phasors of the solutions at the end. 

	Let us now calculate the second partial derivatives with respect to $x$ and $z$:
		
	The following equation:
	
	tells us that under the surface, these two expressions are equal to a given sign:
	
	from which we deduce:
	
	$\lambda$ can only be a constant, since both sides of the expression depend on independent variables (which is very tricky for an observation!). This constant can take, in principle, any real or complex value.

	This gives us the following system of two independent differential equations of the second order:
	
	It is therefore a system of differential equations of order 2 independent, almost identical to that solved in the section of Thermodynamics when we studied the equation of heat.

	That being said, we do not know however the sign of the constant $\lambda$. What we know from what we have studied in the section of Differential and Integral Calculus is that if $b$ is positive then the solution of the differential equation in $z$:
	
	will have harmonic solutions. This would be very surprising ... and counter-intuitive with what we observe in reality. So we will impose $\lambda$ as being negative and then we have seen in the section of Differential and Integral Calculus that we will have by adopting pretty much the same notations:
	
	where we have the discriminant of the second differential equation (\SeeChapter{see section Differential and Integral Calculus}) which holds:
	
	So in other words:
	
	He then comes:
	
	and the discriminant of the first differential equation will be purely complex:
	
	and therefore can be written:
	
	Our system is simplified even more:
	
	If we consider the advance as zero (null phase shift) we have:
	
	Since:	
	
	we have then:
	
	So in the second parenthesis, we just have two waves that go in opposite directions. Let's keep one of the two to simplify the analysis (no matter what!):
	
	To determine the constants, we will use the limit condition given above:
	
	Which in the present case gives us:
	
	So of course we are not going to simplify totally so that $k$ is zero otherwise it would not correspond with reality. On the other hand, we will simplify in the following way:
	
	Which imposes in the non-trivial case that:
	
	So we can choose what we want, but if we want to simplify life by anticipating what is coming ... it would be better to take the following choice:
	
	Therefore:
	
	Therefore we get:
	
	Using the hyperbolic functions (\SeeChapter{see section Trigonometry}) we have:
	
	We will continue to try to determine the constants using the second limit condition:
	
	Then it comes (for the derivative of the hyperbolic cosine see the section of Differential and Integral Calculus):
	
	and therefore by putting $z=0$:
	
	and simplifying:
	
	or written differently:
	
	Let us recall that (\SeeChapter{see section Wave mechanics}):
	
	The prior-previous relation does not, to my knowledge, have any solution for expressing the phase velocity directly from an analytic expression (it is therefore transcendent) as a function of the depth $h$ and of the frequency $f$ (implicitly the pulsation $\omega$) of the wave without passing through a Maclaurin development (\SeeChapter{see section Sequences and Series}) of the hyperbolic tangent for small values of $kh$ such that:
	
	from which we derive when the depth $h$ and $k$ are also small by making an elementary simplification of the preceding relation:
	
	Relation which is named "\NewTerm{Lagrange celerity formula}\index{Lagrange celerity formula}". For example, for a tsunami, the wavelengths $\lambda$ are immense (on the order of a hundred kilometers) so the product $hk$ is small (of the order of $0.25$ in the oceans).

	If, on the other hand, the depth $h$ is very large as well as $k$ then we have in Taylor development  series (\SeeChapter{see section Sequences and Series}):
	
	from which we derive also after an elementary simplification:
	
	Relation which is named "\NewTerm{Newton celerity formula}\index{Newton celerity formula}".
	However we can numerically solve the relation:
	
	and then we have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/gravity_wave_speed_period_plot.jpg}	
		\caption{Speed-Period gravity wave relation plot (source: LPFR)}
	\end{figure}
	We can also by observing this plot understand why waves break (supercritical flux). Indeed, since the phase velocity for a given period increases as a function of the depth $h$, the upper part of the wave goes faster than the lower part and thus does not collapse due to lack of support. The supercritical part of the wave is destructive in the case of tsunamis because it is accelerated in its fall by gravity.

	It also follows that the larger waves catch up with the small ones and that their amplitudes are superimposed over a certain distance. We will determine later the function linking the amplitude of the wave to the distance.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/gravity_waves_type.jpg}	
		\caption{Most common type of well-known gravity wave (source: ?)}
	\end{figure}
	Let us now return to:
	
	by conserving only the real part:
	
	Let us recall that we saw at the beginning:
	
	To get $Z$ we must therefore derive the prior-previous relation with respect to $z$ and then integrate the result with respect to time.

	The derivative thus gives:
	
	Therefore:
	
	The primitive gives:
	
	Therefore:
	
	And placing ourselves on $z=0$ (on the surface) we get:
	
	and by denoting by $R$ the amplitude of the wave:
	
	With this the vertical displacement $Z$ becomes:
	
	and remembering that the potential of speeds is given by:
	
	we can then rewrite it with this new constant $R$ as following:
	
	The horizontal velocity of a particle of water will then be:
	
	and the horizontal displacement will be the integral of the previous relation with respect to time:
	
	Therefore:
	
	and at the surface $z=0$:
	
	
	\subsubsection{Depth of a wave}
	It is now interesting to calculate the depth of penetration of the wave. Indeed, we know from experience that the waves are less and less visible as the water height is big (excepted for Rogue Wave that are special non-linear cases).

	Remember that we have just proved:
	
	Which corresponds to the vertical displacement of the wave. If:
	
	where we will consider that the wavelength of the waves is small and that the depth $h$ of the medium in which they propagate is large.

	We then have:
	
	and:
	
	and we see that for this approximation to be satisfied, it is necessary that:
	
	So that the remaining part of of calculations to be valid, we will impose small values of $z$ (ie close to the surface).

	Since then:
	
	Without forgetting that $-h<z<0$ (so $z$ is negative for recall!). The amplitude of the movements thus decreases exponentially under these conditions with the depth. The reader can verify that for some values of small $kh$ we have:
	
	which shows the limitations of the model with this approach...
	
	\subsubsection{Wave's amplitude}
	For the amplitude of the horizontal movement the situation is the same. That is to say:
	
	where we will consider that the wavelength of the waves is small and that the depth $h$ of the medium in which they propagate is large.

	As we have proved earlier above that:
	
	It comes then:
	
	where for recall, we always have $-h<z<0$ and that this model is valid only near the surface.

	So for these two approximations, we have then when $z$ is small in absolute value with respect to $h$ (therefore in shallow water):
	
	Thus we can observe that for the case of equation and in shallow water (for $z$ small), the water particles describe a circular motion (since the two components $Z$ and $X$ have the same amplitude).

	On the other hand, in the general case, we have:
	
	The amplitudes are therefore no longer the same and the movements are then elliptic.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/gravity_wave_summary.jpg}	
		\caption{Water gravity wave study summary (source: ?)}
	\end{figure}

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{70} & \pbox{20cm}{\score{2}{5} \\ {\tiny 30 votes,  48.00\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Mechanical Engineering}
	\lettrine[lines=4]{\color{BrickRed}M}echanical Engineering is the set of knowledge related to mechanics, at the physical sense (sciences of movements) and at the technical sense (the study of mechanisms). This field of knowledge goes from design of mechanical product to the recycling of this latter through the way, of course, of production, maintenance, etc. Its applications are very important in many areas of everyday life either for the manufacture of machinery, toys, home appliances, medical devices or buildings or a variety of means of transport... and the list goes on...
	
	Again, we will focus here only on the mathematical formalization of practical cases of current applications in the industry! So this section is only a general introduction to technical applications of mechanics and must be complemented by laboratory practices (or computer simulation with for the SimuLink™ MATLAB™ toolbox and the SimDriveline™ MATLAB™ toolbox) and by reading also the section of Thermodynamics where the solid state equation is treated and the section of Continuum Mechanics where the Navier-Stokes equation is proved. The mix of mechanics and magnetism systems (AC motors for example) is treated in the section Electrodynmics.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It would be pretentious to claim in this section that we want to do as good as the two free French PDF \textit{Éléments de machine} and \textit{Résistance des Matériaux} of Nicolet Gaston Raymond that are at our point of view unrivaled in content and quality at this date (compared to the same non-free books on the same subjects!). It is therefore strongly recommended to refer to them if you want to drive full information about mechanical engineering (see the download section of the site).
	\end{tcolorbox}
	
	\subsection{Gears}
	A "\NewTerm{gear}\index{gear}" is a mechanical system consisting of two intermeshing gears for the transmission of the rotational movement therebetween or for the propulsion of a fluid (when the speak about: gear pump).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/gear_train.jpg}	
		\caption{Small Gear train}
	\end{figure}
	
	Two or more gears working in a sequence (train) are named a "\NewTerm{gear train}\index{gear train}" or, in many cases, a "\NewTerm{transmission}"\index{transmission}; such gear arrangements can produce a mechanical advantage through a gear ratio and thus may be considered a simple machine. Geared devices can change the speed, torque, and direction of a power source. The most common situation is for a gear to mesh with another gear; however, a gear can also mesh with a non-rotating toothed part, called a rack, thereby producing translation instead of rotation.
	
	The inventor of the cogwheel is considered by many practitioners as being no less than the famous mathematician, engineer and physicist Archimedes.
	
	We find gears absolutely everywhere in our daily lives: cars, bicycles, watches, adjustable chairs, etc. We strongly recommend also the reader to think about the gears (trays and sprockets connected by a chain of transmission) of his bike to interpret the results that follow below.
	
	The common vocabulary about a simple standard gear is given in the figure below:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/gear_definitions.jpg}
		\end{center}	
		\caption{Single Gear simple definitions (source: Wikipedia)}
	\end{figure}
	It seemed important to us to first introduce briefly how to calculate the tooth pitch of a cylindrical wheel depending on another one for the most famous type of gears is the "\NewTerm{spur gear}\index{spur gear}" like the one shown below:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/spur_gear_definitions.jpg}
		\end{center}	
		\caption[]{Spur Gear definitions for our analysis}
	\end{figure}
	For the driving to operate so we must ensure that the teeth of a one of the cylindrical wheel of diameter $\varnothing=D_1$ well interspersed between the teeth of another wheel of diameter $\varnothing=D_2$. For this, we must introduce the concept of "\NewTerm{circular pitch}\index{circular pitch}" from the teeth of each wheel which we will denote respectively by $P_1$ and $P_2$.
	
	For this we will use the assumption that the teeth contact point can be assimilate to the diameters (represented above by black circles) that the cylindrical wheels would have if there were no slip (and therefore we would not need teeth ...) in the same way that the cog-wheel. These circles are named "\NewTerm{primitive circles}\index{primitive circles}", or "\NewTerm{primitive cylinders}\index{primitive cylinders}" or "\NewTerm{primitive diameters}\index{primitive diameters}".
	
	The tooth pitch will thus be expressed as a function of the circumference and the number of teeth on each wheel. If we denote by $Z_1$ the number of teeth a cylindrical gear $1$ and the $Z_2$ the number of teeth of a cylindrical gear $2$, then we have the tooth pitch values that are equal respectively to:
	
	and as for the gear train to work we must have both tooth pitch to be equal such that:
	
	Therefore we have:
	
	where $m$ is named the "\NewTerm{tooth module}\index{tooth module}". So we will retain that:
	
	We also observe by the above relation that the tooth pitch is not proportional to the tooth module (the choice of a large module provides a number of small teeth and the choice of a small module a large number of teeth).
	
	For people interested in more stuff:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/spur_gear_detailed_definitions.jpg}
		\end{center}	
		\caption{Spur Gear definitions}
	\end{figure}
	
	\subsubsection{Transmission ratios}
	The "\NewTerm{transmission ratio}\index{transmission ratio}" also named "\NewTerm{reduction ratio}\index{reduction ratio}" of a gear or a pulley system is very important in the field of mechanics and is defined by:
	
	and therefore if the speed (pulsation) is constant (see the section of Classical Mechanics for the detailed calculations of the kinematics of the circular motion) and no longer varies between starting and nominal operation, it comes:
	
	This is a widely used technology in everyday life but for people that like famous examples, we have such transmission rations in cars (and generally almost all engines):
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.8]{img/engineering/transmission_ratio_example_car.jpg}
		\end{center}	
		\caption{Motor with few pulleys and drive belts}
	\end{figure}
	as well as in almost all mechanical watches:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/transmission_ratio_example_watch.jpg}
		\end{center}	
		\caption{A gear system with particular transmission ratio of a watch}
	\end{figure}
	and bicycles:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.6]{img/engineering/bicycle_gear_ratio.jpg}
		\end{center}	
		\caption{Bicycle gears ratios}
	\end{figure}
	We also of have of course:
	
	Therefore, the ratio of the rotational speed (angular frequency) between the output gear and the input gear is equal to the ratio of the angles traveled between the output gear and the input gear or the inverse ratio of their respective time period.
	
	We even by our study of kinematics of the circular motion:
	
	
	Thus, to use the example of the bicycle ... if we want to apply the greatest moment of force possible to the rear wheel by minimizing the moment of force we provide on the pedal so the best strategy according to the relationship given above is to take the smallest possible plate associated to the largest sprocket (the ideal would be to have a plate smaller than the largest sprockets for a transmission ratio greater than one).
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.8]{img/engineering/bycicle_gears_ratio.jpg}
		\end{center}	
		\caption{Gear example with bicycle (source: Wikipedia)}
	\end{figure}
	We also better understand why it is recommended for countries having periods of snow to maximize the transmission factor when driving car (on snows). As the moment of force will be minimized avoiding too much slipping.
	
	Caution! The principle of pulley/gear with transmission ratio is a great force multiplicator but in no case it multiplies (increase or decrease) the work or the power as a simple calculation with the above figure can be done since for the work:
	
	is the same for the bot situations.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The norm ISO 1122-1  provides the inverse definition and denote the transmission ratio $i$ instead of $r$:
	
	\end{tcolorbox}

	\subsubsection{Gears association}
	For reasons of geometrical or mechanical constraints, it is sometimes necessary to build gear stages as shown for example with the gear train of 4 wheels below:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.8]{img/engineering/gear_train_multiple_wheels_3d.jpg}
		\end{center}	
		\caption{Gear train example in 3D with multiple wheels}
	\end{figure}
	Either into technical schematic form (not according to Swiss VSM Standards):
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/gear_train_multiple_wheels_2d.jpg}
		\end{center}	
		\caption{Equivalent gear train example in 2D with multiple wheels}
	\end{figure}
	The overall transmission ratio will then be given for the speed of rotation by:
	
	where the fourth equality is simplified because in the case above:
	
	We can also express the total transmission in terms of diameters. Since we have proven that:
	
	he then comes immediately:
	
	expression that we can not simplify!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Thus, in the context of a transmission of a watch with astronomical complication, the transmission ratio was obtained by determining the rational fraction $1802/217$. This being very difficult to implement with only two gears, we'll just build with three axes and four gears (with $7$, $31$, $34$ and $53$ teeth) the same ratio - which fortunately is not irreducible - as follows:
	
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We would like with a $4$ wheel gear train on $2$ axes, driven by the time axis (which spins through $12$ hours), make a very precise transmission ratio $r$, to make a watch complication that shows a moon phase with the classic disc having $2$ moons and turning behind a mask:
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.4\textwidth}
			\includegraphics[width=\textwidth]{img/engineering/moon_phase_disc.jpg}
		\end{subfigure}
		\begin{subfigure}{0.4\textwidth}
			\includegraphics[width=\textwidth]{img/engineering/moon_phase_mask.jpg}
		\end{subfigure}				
	\end{figure}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	As constraints the maximum number of teeth per wheel shall not exceed $300$ teeth and do not fall below $7$. The accuracy will be the best permitted within these numbers of teeth.\\
	
	We will take $29$ days $12$ hours and $44$ minutes for the lunar month.\\
		
	The calculation of the desired ratio is relatively simple. The two moons disc must make a full turn in $5,102,885.8$ seconds. The wheel of the timepiece makes one revolution in $12$ hours, or $43,200$ seconds.\\
		
	For a turn of the two moons disc (leading axis) the axis of the $12$h wheel (driven axis):
	
	and rounding as below, we have an error of about $3$ ten-thousandth of a second per $12$ hours. Approximately $0.2$ seconds per year of delay. What is acceptable for a mechanical watch.

	By rounding:
	
	we have an error of about $30$ seconds per year, which is still acceptable for a mechanical watch. If we take again away one decimal place, then we have a delay of $6$ minutes per year, even removing an additional decimal place, we would have a $49$ minutes delay (which is still acceptable for many mechanical watches). By cons, beyond, this is no longer acceptable!\\
	
	Now, to find the nearest decomposable rational fraction to this number there exist numerous empirical methods (by trial and error or by using a Brocot tree) and tables but the least worst ... for us ... is that using the continuous fractions (\SeeChapter{see section Number Theory}) when it is applicable...\\
	
	Remember that we have proved that:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	considering:
	
	If we denote by $x$ the ratio $a / b$ then the relations above give us that $q_1$ is the integer part of $x$, $q_2$ the integer part of $b/r_1$ thus of $b/((x-q_1)b)=1/(x-q_1)$ either and so on...\\
	
	We then have in our case:
	
	Thus after we put everything to a common denominator:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	If the difference (error) between this fraction and the exact value is acceptable (just have to calculate the error of time this causes after a year of operation of the watch for example), we stop ourselves here. But there is a second criterion ... the numerator and denominator must be decomposable in a satisfactory manner in relation to requirements of the number of teeth of the gears. Or in this case, the denominator ($5788$) can not be decomposed satisfactorily in relation to our constraints (do the decomposition with the command \texttt{ifactor} of Maple 4.00b and you will see!).\\
	
	If we continue to develop our continued fraction, we will not find an acceptable solution before the numerator or the denominator exceeds the maximum permissible value of $300^2$.

	Thus, we return for example to our fraction (well this is not an acceptable approximation because in reality the maladjustment of the watch will be too fast):
	
	and we cheat a bit by trial and error to find a good ratio:
	
	Then, as we have a four gears train, we must have:
	
	and then we take (always helping with to function \texttt{ifactor} of Maple 4.00b):
	
	\end{tcolorbox}
	
	\paragraph{Odd/Even Gear "problem"}\mbox{}\\\\
	The gear association below has a \underline{closed loop} of four different gears. No two of them have the same diameter, and no two have the same number of teeth. They turn smoothly. Any even number of gears, with parallel axles, or even smooth wheels, can be put into such a loop, and they will turn freely, no matter what their tooth count or size. There's a simple way to prove this, using elementary geometry.
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/even_loop_gear.jpg}
		\end{center}	
	\end{figure}
	The underlying principle isn't physics, but geometry, and it applies not only to gears, but to smooth friction wheels as well. The figure shows two friction wheels of different diameter. When they turn without slipping, the two circles must turn through the same arc, but in opposite sense of rotation. That is, if the left wheel turns clockwise through arc $A$, the right wheel turns counter-clockwise through arc $B$, and $B = - A$:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/friction_gear.jpg}
		\end{center}	
	\end{figure}
	If these were gears, then if one gear turns through $N$ teeth, the other gear turns through $N$ teeth in the other direction. It follows that if you had a string of an odd number of gears and tried to make a loop by meshing the gears at the end of the loop, their points of contact would be moving in opposite directions. If you closed a loop of an even number of gears, they would all turn quite smoothly through the same arc.
	
	There are even exotic gears that have non-circular perimeters, such as oval or elliptical. You could make a model with an even number of these in a loop, and the gears in the model would turn smoothly.
	
	A typical mistake is to think that the following will work:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.6]{img/engineering/serial_gears_loop.jpg}
		\end{center}	
	\end{figure}
	
	\subsubsection{Type of Gears}
	Here are some basic types of gears and how they are different from each other.
	
	\begin{itemize}
		\item The most common gears are "\NewTerm{spur gears}\index{spur gears}" and are used as we have seen previously in series for large gear reductions along a same plane! The teeth on spur gears are straight and are mounted in parallel on different shafts. These are particularly loud, due to the gear tooth engaging and colliding. Each impact makes loud noises and causes vibration, which is why spur gears are not used in machinery like cars. 

		\item "\NewTerm{Helical gears}\index{helical gears}" operate more smoothly and quietly compared to spur gears due to the way the teeth interact. The teeth on a helical gear cut at an angle to the face of the gear. When two of the teeth start to engage, the contact is gradual--starting at one end of the tooth and maintaining contact as the gear rotates into full engagement. Helical is the most commonly used gear in transmissions.

		\item "\NewTerm{Bevel gears}\index{bevel gear}" are used to change the direction of a shaft’s rotation (bevel gears are most often mounted on shafts that are 90 degrees apart). Bevel gears have teeth that are available in straight, spiral, or hypoid shape. Straight teeth have similar characteristics to spur gears and also have a large impact when engaged.

		\item "\NewTerm{Worm gears}\index{worm gears}" are used in large gear reductions. The setup is designed so that the worm can turn the gear, but the gear cannot turn the worm. The angle of the worm is shallow and as a result the gear is held in place due to the friction between the two. The gear is found in applications such as conveyor systems in which the locking feature can act as a brake or an emergency stop.
		
		\item "\NewTerm{Geneva gears}\index{Geneva gears}" or "\NewTerm{Maltese cross gears}\index{Maltese cross gears}" are gears mechanism that translates a continuous rotation into an intermittent rotary motion. The rotating drive wheel has a pin that reaches into a slot of the driven wheel advancing it by one step. The drive wheel also has a raised circular blocking disc that locks the driven wheel in position between steps. In the most common arrangement, the driven wheel has four slots and thus advances by one step of $90^\circ$ for each rotation of the drive wheel. If the driven wheel has n slots, it advances by $360^\circ/n$ per full rotation of the drive wheel.
	\end{itemize}
	While manual transmissions (manually shifting a gear selector mechanism that disengages one gear and selects another) have remained relatively unchanged over the years, electronically controlled automatic, semi-automatic, and continuously variable transmissions (CVTs) have become increasingly complex, but also easier to use than ever before. However, modern transmissions of all types have become more prone to failure, primarily because of this higher level of complexity.
	
	The Continuously Variable Transmissions (CVT) is a transmission mechanism that doesn’t use gears as its means of producing various vehicle speeds at different engine speeds. Instead of gears, the system relies on a rubber or metal belt running over pulleys that can vary their effective diameters:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/cvt.jpg}
		\caption{Continuously Variable Transmissions (CVT)}
	\end{figure}
	To keep the belt at its optimum tension, one pulley will increase its effective diameter, while the other decreases its effective diameter by exactly the same amount. This action is exactly analogous to the effect produced when gears of different diameters are engaged.
	
	Now just a word about epicyclic gears!
	
	\textbf{Definition (\#\mydef):} An "\NewTerm{epicyclic gear train}\index{epicyclic gear train}" or "\NewTerm{planetery gear}\index{planetery gear}" consists of two gears mounted so that the center of one gear revolves around the center of the other. A carrier connects the centers of the two gears and rotates to carry one gear, called the planet gear, around the other, called the sun gear. The planet and sun gears mesh so that their pitch circles roll without slip. A point on the pitch circle of the planet gear traces an epicycloid curve. In this simplified case, the sun gear is fixed and the planetary gear(s) roll around the sun gear.
	
	The epicyclic gear train below consists of a sun gear (yellow), planet gears (blue) supported by the carrier (green) and an annular gear (pink). The red marks show the relative displacement of the sun gear and carrier, when the carrier is rotated $45^\circ$ clockwise and the annular gear is held fixed:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/epicyclic_gear_train.jpg}
		\caption{Epicyclic gear (source: Wikipedia)}
	\end{figure}
	Yes ok... but why use a epicyclic train gear excepted in watches knowing that the latter or not one of the most important thing in life!
	
	There is one well know example: modern cars using CVT (and not only) have epicyclic gears and also almost all bicycles:
	\begin{figure}[H]
		\centering
		\includegraphics{img/mechanics/bycicle_gear.jpg}
		\caption{Bicycle hub gear (special example)}
	\end{figure}

	Why do CVT and bicycles use this type of gear? 
	
	First because this type of gear is very compact in surface and in volume and as forces are distributed across $4$ planets gears (sometimes more!) the effort on the sun gear is divided. This is for the technical point of view...
	
	Now for the utility point of view: Consider the figure above, where for example the sun gear is driven by the motor of the car and the annular gear  is connected in one way or another to the transmission of the car. Therefore if we make the sun gear and the carrier solidary (connected) they turn together in the same sens. Then we speak of "\NewTerm{forward gear}\index{forward gear}" as the whole mechanism move as a single unique gear. If we remove what makes the sun gear and annular solidary than as it is a double epicyclic gear, the sun gear will force the annular gear to turn in the reverse direction and the we get a "\NewTerm{reverse gear}\index{reverse gear}".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If the reader want to see such a gear in action he can take a look to YouTube where there is a ton of video on this subject.
	\end{tcolorbox}
	The gear ratio of an epicyclic gearing system is somewhat non-intuitive, particularly because there are several ways in which an input rotation can be converted into an output rotation. Indeed, in some hybrid vehicle transmissions, two of the components are used as inputs with the third providing output relative to the two inputs!!
	
	OK let us now introduce some mathematical (physical properties of this gear. For this let us consider the following figure where for simplification purposes our epicyclic has its planetary gears that are horizontal:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/planetary_gear_configuration_study.jpg}
		\caption{Epicyclic gear simplified schema}
	\end{figure}
	From the figure above we want to prove a canonical relation for the tooth counts. For this purpose let us denote the number of teeth of the ring gear by $N_r$, of the sun gear by $N_s$, and of the planet gears by $N_p$.
	
	We will assume that our planetary gear to work out is that all teeth have the same pitch, or tooth spacing. This ensures that the teeth mesh.
	
	The determination of a canonical between all teeth can  be made more clear by imagining "gears" that just roll (no teeth), and imagine an even number of planet gears. From the illustration above we can see that the diameters of the sun gear, plus two planet gears be must equal to the ring gear size. That is:
	
	And if we assume that all teeth have the same pitch that we will denote here by $P$, that is:
	
	Therefore injecting and simplifying in the prior-previous relation we get:
	
	We see that the special case $N_p=0$ brings us to the obvious results $N_r=N_s$.
	
	Let us denote first by $\omega_r,\omega_s,\omega_p,\omega_c$  the angular velocity of respectively the Ring (annulus) gear (pink color in the first figure), Sun gear (yellow color in the first figure), Planet gears (blue color in the first figure) and Carrier (green color in the first figure) and $N_r,N_s,N_p$ the corresponding number of teethes of each gear (the Carrier no have teeth of sure!).
	
	Now, usually in a planetary gear, one of the gears is held fixed. For example, if we hold the ring gear in a fixed position, $\omega_r$ will always be zero. So we can ignore. And consider also that in this example we drive the sun gear and we want to know $\omega_c$ in function of $\omega_s$.
	
	For this we use the previous teeth canonical relation. Indeed, if the carrier do one turn, this means that each of the planetary gears have traveled $N_r$ teeth, and therefore made $N_r/N_p$ turns. 
	
	
	\pagebreak
	\subsection{Strength of materials}
	Strength of materials (or SoM for the intimes...) is, like all the other sections of this book, an extremely vast area which level of detail and complexity of the calculations can explode rather quickly. We will in the following paragraphs focus on the essentials that the engineer (in business) needs to know. The developments are oversimplified for trivial special cases (straight bars and beams). In reality, we use the tensor calculus, design of experiments or computer modeling experience with FEM (finite element methods).
	
	Before we begin to study some simple concrete cases let us de some reminders of the proofs we have proceed during our study of this subject in the section of Continuum Mechanics:
	\begin{itemize}
		\item A solid considered as rigid does not exist, it is only a convenient approximation. Experience shows that a solid is in fact still slightly deformable under the effect of external forces.

		\item Relationships between deformations and tensions are usually complicated due to the anisotropy of the crystal lattices. However, the solids are generally not single crystals but polycrystalline substances consisting of assemblies of microcrystals associated randomly, they can be thus considered as isotropic.
	\end{itemize}
	Then, we should consider globally the following assumptions relatively to the developments that will follow:
	\begin{itemize}
		\item[H1.] Matter is homogeneous, that is to say for recall that it has the same physical constitution and the same structure throughout all its volume.

		\item[H2.] Matter is isotropic, that is to say for recall that its mechanical properties are the same in any point of the volume.

		\item[H3.] The material is perfectly elastic, that is to say, for recall that after removal of the external forces, the volume immediately takes back its original dimensions (unlike the plastic limit!).

		\item[H4.] The deformations (displacements of the points of the characteristics line ) are small compared with the dimensions of the objects studied.

		\item[H5.] Any straight section (cross-sections) before deformation remain straight after deformation (\NewTerm{Navier-Bernoulli hypothesis}\index{Navier-Bernoulli hypothesis}).

		\item[H6.] The results obtained in material strength theory can only be applies to a sufficient distance from the concentrated efforts applied on the volume (\NewTerm{Barré Saint Venant hypothesis}\index{Barré Saint Venant hypothesis}).

		\item[H7.] In the elastic range, matter obeys the law of proportionality and then the deformations are given by Hooke's law alread proved in the section of Continuum Mechanics. This linear law allows us to apply the principle of superposition to forces and to resistive deformations.
	\end{itemize}
	We proved in the section of Continuum Mechanics that Hooke's law that states, when the deformations are reversible, that there is proportionality between tension and deformation (it's a variation formulation of Hooke's law):
	
	or:
	
	where $E$ is the Young's modulus, $\varepsilon$ the normal deformation and $\sigma$ the normal stress. 

	Let us indicate that the ratio:
	
	is often named "\NewTerm{stiffness (rigidity) of the bar}\index{stiffness (rigidity) of the bar}" in the literature and is often denoted by $k$.

	We have also proved in the section of Continuum Mechanics that the shear stress was given by:
	
	where $G$ is the "shear modulus", $\gamma$ the angle of deformation, and $\eta$ the Poisson's ratio that is a dimensionless number. thus we have a relation between the modulus of elasticity and rigidity in the case of small deformations.

	We have proved also in the same chapter that for a solid or a liquid subjected to a uniform isotropic pressure we had:
	
	The compressibility coefficient $\kappa$ is therefore  a positive number, so using the above relation, we have:
	
	and then comes a known result:
	
	So the Poisson coefficient can not be bigger than $1/2$ and can be negative (in the latter case we speak then of "\NewTerm{auxetic materials}\index{auxetic materials}").

	Finally, let us remember that we saw in the section of Continuum Mechanics that the unit contraction along the $z$-axis was given during a traction along the $x$-axis by:
	
	That is to say when written differently (focusing on the $XZ$ plane)
	
	Therefore:
	
	and that is what shows the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/test_piece_traction.jpg}
		\caption{Traction on a test piece}
	\end{figure}
	We also proved in the section of Continuum Mechanics the following relation during in our study of flexural modulus:
	
	which expresses the bending moment for a beam under a moment of force $M$ (torque), then the span describes an arc of with a curvature of radius $R$ and where $I$ characterizes the "\NewTerm{shape stiffness (rigidity)}\index{shape stiffness (rigidity)}" of the material having a a given cross sectional area $S$. This is a very important relation in many areas of mechanical and civil engineering (shipbuilding, automobile, architecture, etc.).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	$I$ is named the "\NewTerm{static moment of inertia}\index{static moment of inertia}" or "\NewTerm{quadratic moment}\index{quadratic moment}" as we have already specified it in the section of Continuum Mechanics.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Quadratic moments}
	Let us see the three conventional static moments of inertia $I_x$ in the field of Material Strengths because they are often encountered in practice (construction).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The theory of moments of inertia is presented for recall in the section of Classical Mechanics. In the section on Geometric Shapes we proved in detail the moments of inertia of the most common volumes.
	\end{tcolorbox}
	Let us see first the quadratic moment of inertia $I_x$ of a  rectangular plate of side $b$ and height $h$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/quadratic_moment_rectangular_plate.jpg}
	\end{figure}
	The domain occupied by the plate is given by:
	
	Then we have:
	
	As we will see further below , it is this result that explains why the right one below configuration will have more strength:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/wood_strength.jpg}
	\end{figure}
	\pagebreak
	And now let us study the quadratic static moment of a disc of diameter:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/quadratic_moment_circular_plate.jpg}
	\end{figure}
	Here the domain of integration is:
	
	where $d$ is the diameter of the disc.
	
	To calculate this integral, we use polar coordinates that are for recall given by (\SeeChapter{see section Vector Calculus}):
	
	
	We always have:
	
	
	The Jacobian of the transformation is equal to $r$ (\SeeChapter{see section Differential and Integral Calculus}). We get:
	
	
	Now let us calculate the quadratic static inertia of a ring of external diameter $D$ and internal diameter $d$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/quadratic_moment_ring_plate.jpg}
	\end{figure}
	Here is the domain of integration is:
	
	where $D$ and $d$ are the diameters of the large and small discs.

	If we denote by $S_1$ the domain of the large disc and $S_2$ that of the small disk then:
	
	using the quadratic static moment of inertia of the disk.
	
	For summary, we have therefore:
	
	and finally there exist also the static polar quadratic moment of $S$ with respect to a point O:
	
	It is therefore easy in simple cases to know the static polar moment of inertia and it is very useful for the study of torsion.

	It follows from these tools as more elements of the section are located away from the axis, the more the static quadratic moment will be important and the more (we will prove it just further below) the "arrows" will be weak.
	
	\pagebreak
	\subsubsection{Equation of the elastic line}
	For this case study example but widely used in practice we will first have to obtain mathematically the geometric form that takes the neutral axis of a beam subjected to bending forces.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	 If all the reactions of a solicited system can be found from the static equilibrium equations, the problem is say to be "\NewTerm{statically determined}\index{statically determined}" or "\NewTerm{isostatic}\index{isostatic}". When the equations of static fail to find the equilibrium of a system, the system is say to be "\NewTerm{hyperstatic}\index{hyperstatic}" or "\NewTerm{statically indeterminate}\index{statically indeterminate}". We also say that while there is "freedom of movement" despite of $n$ connection points, we are in isostatic situation.
	\end{tcolorbox}
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/neutral_fiber.jpg}
		\caption{Beam (simplified ...) subjected to a force}
	\end{figure}
	By definition of the derivative and under the assumption of small deformations (however that still works well up to $45^\circ$...):
	
	Or by derivating once again:
	
	Moreover, the figure shows that (\SeeChapter{see section Trigonometry}):
	
	But the fact that the neutral fiber curve deviates little from the $y$ axis (small deformations), we can write:
	
	Therefore:
	
	Therefore we can write using the relationships obtained above:
	
	which is the differential equation giving $z=f(y)$, named "\NewTerm{equation of the elastic line}\index{equation of the elastic line}".
	
	Another common but less intuitive approach to get the same relation is to start from:
	
	and to recall that the radius of curvature $R$ is given by (\SeeChapter{see section Differential Geometry}):
	
	Either by adapting the notation to our context:
	
	and neglecting the first derivative for the small deformations we find indeed:
	
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	\textbf{E1.} Cantilevered beam (only at one side) that is classic case in construction and housing) with concentrated load (punctual) at the end:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/cantilevered_beam_punctual_force_extremity.jpg}
		\caption{Cantilevered beam with punctual force on extremity}
	\end{figure}
	In the section $S$ at any point, the moment of force (bending) is therefore:
	
	On the other hand:
	
	By eliminating $R$ between these two relations, it remains:
	
	The figure shows that the boundary conditions are:
	
	We get after integration:
	
	Therefore:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	If $y=L$ the deformation is maximum and $z$ therefore takes the maximum value $f$ named the "\NewTerm{arrow}\index{arrow (mechanical engineering)}". It follows:
	
	This give us finally the maximum vertical deflection of the end to a beam fixed on one side:
	
	All data from this relation are known to us (foce, length, Young's modulus, static momentum) and it is then possible to determine whether the bar will break or not because we simply need to apply the relation proved above:
	
	So thanks to the relation:
	
	and knowing experimentally from what experimental value of $\sigma$ the material breaks down we will know when the beam will also break down (at least approximately!) knowing the arrow $f$, the bending moment $M$ and the static quadratic moment $I$.

	So we have a result that will be useful to us later:
	
	and integrating from $0$ to $L$ we find the arrow of our previous beam!\\

	\textbf{E2.} The Sustained beam (also named "\NewTerm{beam two times supported}" or "\NewTerm{isostatic beam}\index{isostatic beam}") is the most classic example in construction and therefore architecture (and for those who played during childhood to put wooden slats to move over a small river). This is a homogeneous beam, of constant section, resting on two free bearings at its ends and subjected to a load $F$ at the center:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/sustained_beam.jpg}
		\caption{Sustained beam with punctual centered force}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We can therefore consider that it is as if we had $F / 2$ at both ends of two beams of length $L / 2$ (thus the sum is indeed equal to $F$, that is to say, the bending moment). Notice that we neglect the weight of the beam relatively to that of $F$, but $F$ may be simply the weight of the beam! Using the last relation of the previous example, we have:
	
	Therefore:
	
	Thus the maximum vertical deflection of a beam connected on both sides is finally (changing a bit the notation):
	
	Thus, for a same length of beam, at identical $F$ the arrow  is $16$ times smaller than for a fixed beam! It was intuitive that it was lower for the same force but hard to guess that it would be of a factor $16$...!\\

	It is this relationship that is also used for IPN beams (famous beams in building construction!).\\

	\textbf{E3}. Beam built only one side still named "cantilever beam" (also classic case in the construction and housing) but now with a constant line load denoted $w$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/cantilevered_beam_line_constant_load.jpg}
		\caption{Beam fixed on one side with constant line load}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The development is simple but some simplifications are smart to get at the end an elegant result. So we always start from the equation of the elastic line by adopting the notations relatively to the selected configuration (see figure above):
	
	Thus explicitly:
	
		As the moment of force $M$ is zero at $x = 0$, we have the constant that is zero. Since then:
	
	Integrating again, we get:
	
	As for $x=L$ by hypothesis the deformation is zero, we have then the constant that is given by:
	
	Therefore:
	
	By integrating a last time:
	
	And as on $x=L$ we have $y$ that is also zero, it comes for the constant:
	
	Therefore:
	
	And as the arrow is anyway on $x=0$, we have then:
	
	
	\end{tcolorbox}
	
	\paragraph{Euler-Bernoulli Beam equation}\mbox{}\\\\
	Let us now consider the case of a beam fixed on both sides (case even more common that the three previous examples!). The analysis will be a bit more difficult and we will have to introduce several new concepts.

	A beam in practice must withstand at least the following efforts:
	\begin{itemize}
		\item Tension or Compression:
		\begin{figure}[H]
			\centering
			\includegraphics{img/engineering/beam_tension_or_compression.jpg}
		\end{figure}
		
		\item Shearing (shear):
		\begin{figure}[H]
			\centering
			\includegraphics{img/engineering/beam_shearing.jpg}
		\end{figure}

		\item Flexion (bending stress):
		\begin{figure}[H]
			\centering
			\includegraphics{img/engineering/beam_flexion.jpg}
		\end{figure}
	\end{itemize}
	If a beam is in equilibrium, then the internal forces must satisfy at all points the following relations:
	
	Let us consider now a beam embedded at its two ends (dual-embedded beam) and let take in a slice of infinitesimal length $\mathrm{d}y$ such that locally its curvature is zero. The beam will be assumed to be subjected to a uniform force over its entire length (a force which can also be assimilated to its own weight, as already mentioned above). It is then customary to denote by $\vec{w}$ the force per unit length (total weight divided by the length) which is obviously a linear load:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/beam_euler_bernoulli_equation_01.jpg}
	\end{figure}
	If the beam is at equilibrium once deformed (weakly or stronly deformed, this does not matters !) then the sums of the forces of tension, compression, shear and bending must be zero at each point as we have already said! This does not however mean that at each point of the elastic line the values of each of the forces is equal! On the contrary! There are of course differences (if not, there would be no deformation...!).

	Let us first sum the local forces of tension and compression (horizontal) of the element of length $\mathrm{d}y$. We then have schematically:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/beam_euler_bernoulli_equation_02.jpg}
	\end{figure}
	That is to say algebraically (the variational can be negative or positive whatever!):
	
	If now we take care of the vertical forces at the source of the shear then we have schematically:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/beam_euler_bernoulli_equation_03.jpg}
	\end{figure}
	That is to say algebraically (the variational can be negative or positive whatever!):
	
	And finally for the bending moments the sum is also necessarily zero at equilibrium. However, in contrast to the two preceding algebraic sums where we could only use the differential, we have to choose a reference point $R$ for the bending moments since, for recall... the moment of force is by definition the product of a force by a distance. We will therefore naturally choose the center of gravity:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/beam_euler_bernoulli_equation_04.jpg}
	\end{figure}
	The uniform linear load $\vec{w}$ on the length $\mathrm{d}y$ generates a force at mid distance of:
	
	but as it is confounded with the choices of our reference point, then its moment of strength is zero!

	We then have algebraically:
	
	And if we neglect the differentials of order two it remains only:
	
	Finally, we have:
	
	To determine the bending moment ofrom the linear load (which is of primary interest to the practitioner), we derive the third relation twice and make a substitution:
	
	Hence:
	
	The problem with this last relation is the knowledge of the moments. We should get rid of this term absolutely. What we know easily is the deformation function and we have proved earlier above that:
	
	It then comes immediately by substituting the preceding relation in the prior previous one:
	
	where the product $EI$ is known as the "\NewTerm{flexural rigidity}".
	
	This is the most important elementary relation of the theory of beams, that covers the case for small deflections of a beam that is subjected to lateral loads only, because it allows by knowing the linear load to determine the function of deformation or conversely! It is so important to know that it is named the "\NewTerm{static beam equation}\index{static beam equation}" or in honor to those who have determined it: "\NewTerm{Euler-Bernoulli equation}\index{Euler-Bernoulli equation}".  It seemed it was first enunciated circa 1750,but was not applied on a large scale until the development of the Eiffel Tower and the Ferris wheel in the late 19th century

	Since it is a differential equation of order $4$ that will generate four constants at each integration, we will then need $4$ initial conditions to solve it completely.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The Euler-Bernoulli equation by its assumptions is only a special case of a more general theory named the "\NewTerm{Timoshenko beam theory}\index{Timoshenko beam theory}" published in 1921.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We would like to calculate the deformation of a beam fixed on both sides and loaded uniformly knowing its length $L$, its modulus of elasticity $E$, its moment of inertia $I$. \\

	We start from (we change the notations only to show that according to some text books the axes can be denoted differently):
	
	With the initial conditions:
	
	And integrating by repetition, we have:
	
	From the two initial conditions:
	From the two initial conditions:
	
	It follows that:
	
	With the other two remaining conditions, we obtain the following system which must allow us to determine the two remaining constants:
	
	Then it is simply a matter of solving a simple linear system (\SeeChapter{see section Linear Algebra}):
	
	Subtracting both relation in the simple adequate way we get immediately:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Therefore for an ideal beam embedded on both sides subjected to a uniform load and described by:
	
	To determine the arrow, we must therefore look for the point $x$ where this relation has an optimum. We then have:
	
	It follows that the arrow has a maximum at $x = L / 2$. Injecting this into $y (x)$ we get the famous relation often given in the literature but rarely proved:
	
	It follows that the arrow of a beam is proportional to the fourth power of the length of the beam! Such high dependence imposes significant limitations on Civil Engineering structures based on beams.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The constant linear load, either over the total length of the beam or in successive sections, is a frequent observed type of stress in horizontal axis of pieces. It may come from the weight of the pieces with a constant section or from a load caused by an external force (eg gravity).
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Potential elastic energy}\mbox{}\\\\
	After a quick overview of the various elastic deformations of the parts stressed by the fundamental forces, we will establish here the general expression of the elastic energy accumulated in a beam of any shape stressed by external forces.

	Let us recall for this study that we can write Hook's law (\SeeChapter{see section Continuum Mechanics}) in the following form:
	
	and the elastic potential energy of a spring proved in the section of Classical Mechanics is given by:
	
	Or in relative displacement:
	
	In the traction domain (or compression) of the beams, it is customary to consider the beam as a spring (...) and then to use the stiffness constant of Hook's law ... hoping that this is conforming to experimental observations...:
	
	Where, according to usage, we denote the longitudinal displacement $L$ instead of $x$. By injecting Hook's law (yes ... it turns a little bit a loop... but that's engineering...):
	
	It is then sufficient to divide by the length of the bar to have the elastic linear energy:
	
	The energy density, noted conventionally denoted as in thermodynamics by a minuscule letter, is then:
	
	
	\pagebreak
	\subsubsection{Torsion}
	Let us recall first to the reader a study we made in the section on Classical Mechanics on the torsional pendulum where some elements had deliberately been muted.... Let's study this in more detail as it is very useful for transmission shafts or springs in everyday life.

	Let us consider for this a cylindrical wire fixed at its base and subjected to a torsion moment $\vec{M}$. Under the effect of this torsion moment, the upper face of the wire is offset by an angle $\theta$ with respect to the lower face, the material undergoing a torsion stress (or shearing $\tau$):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/wire_into_torsion.jpg}
		\caption{Wire under torsion}
	\end{figure}
	Let us imagine an extraction of the inside the wire an elementary tube of radius $r$, of thickness $\mathrm{d}r$, and let us observe the effect of the torsion on this tube unrolled (this will allow us an approximate approach of the concerned phenomenon):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/torsion_piece_wire_extraction.jpg}
		\caption[]{Extraction an unroll of an element of the wire}
	\end{figure}
	Let us look for a relation between torsion moment $\vec{M}$ and torsion angle $\theta$ that would therefore also apply to a beam as as below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/rectangular_torsion_beam.jpg}
	\end{figure}
	For the unrolled tube, let us apply shear relations as proved and introduced in the section of Continuum Mechanics:
	
	but the figure shows that (the deformations being weak) to the first order in series of Taylor (\SeeChapter{see section Sequences and Series}):
	
	hence:
	
	The elementary moment due to this force is by definition of the moment of force (torque):
	
	Since $\vec{r}$ and $\vec{F}$ are perpendicular:
	
	The total moment (torque) is then:
	The total moment (torque) is then equal to:
	
	therefore:
	
	We thus fall back the relation of the torsional pendulum that we pout during our study of the torsional pendulum in the section of Classical Mechanics:
	
	 with the difference that this time the constant $k$, the "\NewTerm{torsion constant}\index{torsion constant}" is explicit!!!!

	The numerator of the constant $k$ is named in the domain of the strength of materials the "\NewTerm{torsional rigidity}\index{torsional rigidity}" or "\NewTerm{torsional stiffness}\index{torsional stiffness}" and the constant $k$ itself is often referred to as "\NewTerm{shaft rigidity}\index{shaft rigidity}" instead of "torsional constant". In practice, the main aim is to find the numerical value of the following expression:
	
	since this will give the angular amplitude of the torsion.

	Let us see therefore a very important application to the compression spring of helical type (the approach is approximate again...) working in torsion.

	First, it must be realized that when a force is applied to the spring, the ends will rotate by a small angle $\theta$ (torsion) corresponding to the movement by a distance $x$ which itself corresponds to the narrowing of the spring (yes indeed! this length must be taken somewhere....).

	Given then a spring of external radius $R$ (or of diameter $D$), of shear modulus $G$, with a body diameter $d$ (diameter of the folded cylinder of which the spring is composed):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/helicoidal_spring.jpg}
		\caption{Spiral spring under load}
	\end{figure}
	For the analysis we will simply need to mix several of the relations demonstrated so far. In the first place, the torsion angle of a beam of length $L$ (length of the spring in this case!):
	
	With:
	
	and:
	
	Moreover, the moment of torsion is written:
	
	We thus arrive at:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It would be pretentious to claim to do with this section as well and also complete as the \textit{Statique} free French PDF of Nicolet Gaston Raymond that is a priori unrivaled in content and quality to this date (even compared to non-free books on the subject!). It is therefore strongly recommended to refer to it if the reader wants to drive full information about civil engineering (see the download section of the companion website).
	\end{tcolorbox}
	The ratio $M/\theta$, as well as for the drive shaft, is named the "\NewTerm{spring stiffness}\index{spring stiffness}" or  "\NewTerm{spring rigidity}\index{spring rigidity}".

	The displacement (deformation) $x$ is equal to (\SeeChapter{see section Trigonometry}):
	
	We finally arrive at:
	
	Which brings us to the worldwide known relation in the world in the strength of material regading to springs:
	
	where $k$ is the "\NewTerm{spring stiffness}\index{spring stiffness}" constant also named "\NewTerm{spring rigidity}\index{spring rigidity}"! If now we use the expression of the elastic potential energy of a spring demonstrated in the section of Classical Mechanics:
	
	We can then determine the energy a spiral spring can absorb.
	
	\subsubsection{Buckling}
	We conclude this study of the material strengths in this book for now, with "\NewTerm{buckling}\index{buckling}" (classical study case in construction and mechanics) which consists in determining (in a particular simple case) the minimum force $F_0$ from which a bar of length $L$, of Young's modulus $E$ fixed at its two ends can fold (with a curvature radius $R$) until it breaks without the need to increase the force $F_0$ (this is again an indication value!).

	Buckling is characterized by a sudden sideways failure of a structural member subjected to high compressive stress, where the compressive stress at the point of failure is less than the ultimate compressive stress that the material is capable of withstanding. Mathematical analysis of buckling often makes use of an "artificial" axial load eccentricity that introduces a secondary bending moment that is not a part of the primary applied forces being studied. As an applied load is increased on a member, such as a column, it will ultimately become large enough to cause the member to become unstable and is said to have buckled. Further load will cause significant and somewhat unpredictable deformations, possibly leading to complete loss of the member's load-carrying capacity. If the deformations that follow buckling are not catastrophic the member will continue to carry the load that caused it to buckle. If the buckled member is part of a larger assemblage of components such as a building, any load applied to the structure beyond that which caused the member to buckle will be redistributed within the structure.
Theoretically, buckling is caused by a bifurcation in the solution to the equations of static equilibrium. At a certain stage under an increasing load, further load is able to be sustained in one of two states of equilibrium: a purely compressed state (with no lateral deviation) or a laterally-deformed state.

	The ratio of the effective length of a column to the least radius of gyration of its cross section is named the "\NewTerm{slenderness ratio $\lambda$}\index{slenderness ratio }". This ratio affords a means of classifying columns. Slenderness ratio is important for design considerations. All the following are approximate values used for convenience.
	\begin{itemize}
		\item A "\NewTerm{columns!short steel column}\index{short steel column}" is one whose slenderness ratio does not exceed $50$; an intermediate length steel column has a slenderness ratio ranging from about $50$ to $200$, and its behavior is dominated by the strength limit of the material, while a long steel column may be assumed to have a slenderness ratio greater than $200$ and its behavior is dominated by the modulus of elasticity of the material.
	
		\item A "\NewTerm{short concrete column}\index{columns!short concrete column}" is one having a ratio of unsupported length to least dimension of the cross section equal to or less than $10$. If the ratio is greater than $10$, it is considered as a "\NewTerm{long column}\index{columns!long column}" (sometimes referred to as a "\NewTerm{slender column}\index{slender column}).
	
		\item "\NewTerm{Timber}\index{columns!timber}" columns may be classified as short columns if the ratio of the length to least dimension of the cross section is equal to or less than $10$. The dividing line between intermediate and long timber columns cannot be readily evaluated. One way of defining the lower limit of long timber columns would be to set it as the smallest value of the ratio of length to least cross sectional area that would just exceed a certain constant of the material.
	\end{itemize}
	If the load on a column is applied through the center of gravity (centroid) of its cross section, it is named an "\NewTerm{axial load}\index{axial load}". A load at any other point in the cross section is known as an "\NewTerm{eccentric load}\index{eccentric load}":
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/buckling.jpg}
		\caption{A column under a concentric axial load exhibiting the characteristic deformation of buckling (source: Wikipedia)}
	\end{figure}
	A short column under the action of an axial load will fail by direct compression before it buckles, but a long column loaded in the same manner will fail by buckling (bending), the buckling effect being so large that the effect of the axial load may be neglected.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/buckling_in_real_life.jpg}
		\caption{Real life buckling (not to be confus with "crashing"!)}
	\end{figure}
	 The intermediate-length column will fail by a combination of direct compressive stress and bending.

	In 1757, mathematician Leonhard Euler derived a relation that gives the maximum axial load that a long, slender, ideal column can carry without buckling. An ideal column is one that is perfectly straight, homogeneous, and free from initial stress (see proof further below). The maximum load, sometimes named the "\NewTerm{critical load}\index{critical load}", causes the column to be in a state of unstable equilibrium; that is, the introduction of the slightest lateral force will cause the column to fail by buckling. However, if lateral forces are taken into consideration the value of critical load remains approximately the same.

	For the study of this phenomenon, we consider that as soon as the bar begins to bend we have then $F_0$ (and we are then very far from the force allowing to break it):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/buckling_study.jpg}
		\caption{Example of buckling}
	\end{figure}
	When the bar begins to bend we then have a force $\vec{F}_0$ that applies to each volume element of volume of the bar but since these are not distributed in the same way along the $z$ axis, they do not create the same moment of force (torque)!

	At the equilibrium of the buckling force, the bar is subjected to a moment of recall. We then have:
	
	Expressing the moment of bending $M$ by means of the relation (see beginning of this section)
	
	It comes:
	
	Using the equation of the elastic line and substituting, we get:	
	
	thus:
	
	Which is the "\NewTerm{buckling differential equation}\index{buckling differential equation}" for calculating the buckling force with the initial conditions that are:
	
	Let us indicate that the relation:
	
	If often written in the following form in text books:
	
	The resolution of the second order differential equation:
	
	is relatively easy (\SeeChapter{see section Differential and Integral Calculus}) since the characteristic equation is:
	
	We then have the homogeneous solution:
	
	The condition  $y=0\Rightarrow z=0$ imposes:
	
	Then it comes:
	
	So it comes immediately that:
	
	Hence the famous relation given in many text books without proof:
	
	This relation is sometimes referred to as the "\NewTerm{Euler's formula}\index{Euler's formula}" (not to be confused with the formula of the same name proved in the section of Graph Theory) and the load limits the "\NewTerm{Euler's critical load}\index{Euler's critical load}" for a beam perfectly embedded at the ends. The whole study being the "\NewTerm{Euler's buckling}".
	
	Examination of this formula reveals the following interesting facts with regard to the load-bearing ability of slender columns:
	 \begin{enumerate}
		\item Elasticity $E$ and not the compressive strength $\sigma$ of the materials of the column determines the critical load.

		\item The critical load is directly proportional to the second moment of area of the cross section.

		\item The boundary conditions have a considerable effect on the critical load of slender columns. The boundary conditions determine the mode of bending and the distance between inflection points on the deflected column. The inflection points in the deflection shape of the column are the points at which the curvature of the column change sign and are also the points at which the internal bending moments are zero. The closer together the inflection points are, the higher the resulting capacity of the column.
	\end{enumerate}
	
	In the relation above it is physically logic that $k$, named the "\NewTerm{column length factor}\index{column length factor}", to consider the following cases:
	\begin{itemize}
		\item $k=1$: Both ends pinned (hinged, free to rotate)
		\item $k=0.5$: Both ends fixed
		\item $k\cong 0.7071$: One end fixed and the other end pineed
		\item $k=2.0$: One end fixed and the other end free to move laterally
	\end{itemize}
	Now the reader should know that in structural dynamics sometimes we rewrite:
	
	as:
	
	where $r_g$ is the classical radius of gyriation (\SeeChapter{see section Classical Mechanics}). That is to say we consider that for all beam that we are able to concentrate all the mass at the location of radius of gyration so that we get the same effect and satisfy the same equation of dynamics as that of a complex body.

	For example if we were trying to study the equation of motion of a cylinder about it's major axis then instead of developing complex equations you can rather concentrate all the mass at a point located at the distance calculated from above formula and still get same result.

	So now if we rewrite Euler's formular using this radius:
	
	we see the slenderness ratio that we have introduced earlier above that appears:
	
	
	\pagebreak
	\paragraph{Self-buckling}\mbox{}\\\\
	A column can buckle due to its own weight with no other direct forces acting on it, in a failure mode named "\NewTerm{self-buckling}\index{self-buckling}". In conventional column buckling problems, the self-weight is often neglected since it is assumed to be small when compared to the applied axial loads. However, when this assumption is not valid, it is important to take the self-buckling into account.
	
	One interesting example for the use of the equation was suggested by A. G. Greenhill in his paper (1881) after a request of the son of Horace Darwin (son of the famous Charles Darwin). The request aw estimated the maximal height of a pine tree.
	
	For this study, let us suppose a uniform column fixed in a vertical direction at its lowest point, and carried to a height $l$, in which the vertical position becomes unstable and flexure begins:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/self_buckling.jpg}
		\caption{A column exhibiting a compressive buckling load due to its own weight (source: Wikipedia)}
	\end{figure}
	The reader can notice that we take the origin O at the top of the pole in its vertical position and the axis $\text{O}x$ directed vertically downward!
	
	There is a body force $f$ per unit length ($f=F/d$):
	
	where $S$ is the cross-sectional area of the column, $g$ is obivously the acceleration due to gravity and $\rho$  is its mass density.

	The column is slightly curved under its own weight, so the curve $w(x)$ describes the deflection of the beam in the $y$ direction at some position $x$.
	
	Looking at any point on the column, we can write the moment equilibrium:
	
	where the right-hand side of the equation is sum of all moment of the weight of $\overline{BP}$ about the point $P$ or coordinate $y$.

	According to Euler–Bernoulli beam theory (see earlier above):
	
	Where $E$ is the Young's modulus of elasticity of the substance, $I$ is the moment of inertia.

	Therefore, the differential equation of the central line of $\overline{BP}$ is:
	
	Differentiating with respect to $x$, we get:
	
	Or:
	
	That is:
	
	We get that the governing equation is the third order linear differential equation with a variable coefficient. 
	
	The way to solve the problem is to use new variables in a very clever way. First we put:
	
	Therefore:
	
	And we multiple left and right by $x^2$ and we divide by $EI$ to get:
	
	We put now $p=\sqrt{x}z$ then:
	
	That is:
	
	and:
	
	we divide by $\sqrt{x}$:
	
	We simplify a bit:
	
	Therefore:
	
	Finally:
	
	Now, we put $x=r^{2/3}$ and then:
	
	Therefore:
	
	By derivating relatively to $x$ the expression above we get:
	
	But we remember that $r=x^{3/2}$ and therefore that:
	
	We replace this $\mathrm{d}r/\mathrm{d}x$ we have just get into the prior previous relation and we get:
	
	Finally the two terms:
	
	becomes after having replace $\mathrm{d}^2z/\mathrm{d}x^^2$ and $\mathrm{d}z/\mathrm{d}x$ by the expression we just get:
	
	Therefore:
	
	Can be written:
	
	After rearranging:
	
	This is of the form of Bessel's differential equation
	
	Let us now solve that the Bessel equation
	
	with $x(0)=0$.
	
	Now the first step is to consider a solution $y(x)$ expanded in the generalized power series in the vicinity of $x(0)=0$ :
	
	with $a_0\neq 0$ and $\left|x\right|\le \infty$.
	
	The second step is to use:
	
	and also:
	
	Third step is to inject this both relations in the original:
	
	gives:
	
	To continue we use:
	
	and therefore we can simplify:
	
	Now we rearrange in a very clever way (I'm always surprise that some people get such ideas...):
	
	with $a_0\neq 0$.
	
	Now to get this equal to zero a trivial solution is that each facto is equal to zero such that:	
	
	That last relation can be rewritten as:
	
	with obviously $n=2,3,\ldots$.
	
	The characteristic equation is of the first factor:
	
	of the set of three equations is obviously:
	
	If we consider this solution, it means that we must accept that $a_1=0$ for the second relation to be also equal to zero. The same reasoning apply for the third relation and therefore we must have $a_{2k+1}=0$  $k = 0,1,2,\ldots$!
	
	If we put the positive solution into $a_n$, we get:
	
	still with the conditions that $n=2,3,\ldots$ and $a_0\neq 0$.
	
	The denominator of this last can be rewritten. First let us develop:
	
	and now we put $n=2k$, therefore the previous expression can be written:
	
	Therefore:
	
	still with the condition that $a_0\neq 0$ and now with $k=1,2,3,\ldots$.
	
	Now let us write a few recursive developments of the previous relation:
	
	still with $k=1,2,\dots$ and $a_0\neq 0$.
	
	So finally for summary we have:
	
	with $k=1,2,\dots$ and $a_0\neq 0$ and:
	
	with $k=0,1,2,\ldots$.
	
	So for the first partial solution $(\rho=1/3)$ we have:
	
	
	Using the same reasoning but with $\rho=-1/3$ we get:
	
	
	The general solution is then the sum of the both solution, that:
	
	We can extract of the both constants a factor $2^{-\frac{1}{3}}$ to be able to write:
	
	That is:
	
	The solution of the transformed equation is:
	
	Where $J_n$ is the Bessel function of the first kind. Then, the solution of the original equation is if we remember that:
	
	Then:
	
	Now, we will use the boundary conditions. 
	
	\begin{itemize}
		\item As we consider no moment at $x=0$ (since there is no material above this point), then:
		
		So to see what must be the values of the constants $A$ and $B$ let us rewrite the stuff explicitly:
		
		Therefore:
		
		Therefore we see quickly that when $x=$ everything vanish expect a term $1/2AC_1$. Therefore this first condition impose $A=0$.
	
		\item Since it is fixed at $x=l$, we have  $w(x=l)=0$, and therefore also $\mathrm{d}w/\mathrm{d}x=0$, thus:
		
		And after a small simplification this reduce to:
		
		If we denote by $c$ the first root of $J_{-\frac {1}{3}}$, that means:
		
		with:
		
	\end{itemize}
	Therefore from the second boundary condition, we can get the critical length in which a vertical column will buckle!
	
	Indeed, from the first that we have chosen to denote $c$, we get:
	
	as for recall:
	
	Therefore:
	
	Now with Maple 17.00 we can determiner the smallest root of $J_{-1/3}$ to get $c$:
	
	\texttt{>evalf(BesselJ(-1/3, 1));}
	
	that gives $0.6068875051$. 
	
	Therefore:
	
	Hence:
	
	This done let us do a numerical application!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For a column of steel (Structural ASTM-A36) of radius $r=0.1$ [m] we have $E=2.1\cdot 10^{11}\;[\text{N}\cdot\text{m}^{-2}]$ (some steel go to $E=4.8\cdot 10^{11}\;[\text{N}\cdot\text{m}^{-2}]$) and:
	
	and $f=2481\;[\text{N}\cdot \text{m}^{-1}]$. Therefore:
	
	With a square column of $55\times 132$ [cm] side (as biggest World Trade Center building beams in New York) we would get as $I=bh^3/12$ the value $x_{\max}\cong 30.13$[m].
	\end{tcolorbox}
	So what can we conclude so far:
	\begin{enumerate}
		\item The development above has for now not been peer-reviewed so he may contain errors. We have compared with different sources that have a slightly different result but they all have missing details that avoid us to make a strict comparison.
		
		\item We don't know any laboratory that can confirm the value that we get and some people on Internet with different approach get $80$ meters (seems a priori more accurate than our result) or a few kilometers...
	\end{enumerate}
	
	\subsubsection{Traction}
	Let us now consider the case of a bar hanging only to its own weight. The surface of its circular cross-section is $S$ and $h$ the total height of this bar. The Young's modulus of its material is denoted $E$ ({see section of Continuum Mechanics}) and its density $\rho$.

	It is easy to see that a section situated at an altitude $z$ supports the weight of the piece of bar under it:
	
	The constraint is therefore not constant in the bar:
	
	And the deformation either:
	
	where $z$ is the abscissa on the bar, the inhomogeneous deformation is related to the displacement by the relation:
	
	After integration, we get the general form of the displacement:
	
	where the constant is to be determined using any bonding conditions at the ends of the bar. If the upper end is embedded, the displacement there is therefore zero:
	
	The displacement at any point of the bar is thus expressed by:
	
	The elongation of the bar is the difference in displacement between the two ends of the bar:
	
	We then have trivially:
	

	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{20} & \pbox{20cm}{\score{2}{5} \\ {\tiny 32 votes,  78.75\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Electrical Engineering}	
	\lettrine[lines=4]{\color{BrickRed}W}e will see in this section the study - under its mathematical aspect of sure - of general electronic circuits, chips, and electronic machinery, transmitter / receivers that the engineer must know formalize, analyze, understand, build and simulate after his studies . For this reason we have chosen in this book to focus - as always - on mathematical case of practical (useful!)  applications In the every days life, mentioning the potential pitfalls and dangers of the electronic assembly when necessary.
	
	We will deal here first with analogic electronics, then power electronics (electrotechnic), digital electronics and of the physics of semiconductor  to understand the foundations of some components.
	
	Electrical engineering is therefore hierarchy of models. This is the only way to approach the design of complex systems. In principle, the operation of many devices can certainly always be reduced to the application of Maxwell's equations (\SeeChapter{see section Electrodynamics}) or Schrödinger Equation for more complicated case, however it is humanly impossible to understand the design of some systems by staying at this theoretical level.
	
	It is therefore customary in the industry to perform the analysis in 5 stages:
	\begin{enumerate}
		\item[L0.] Solid State Physics: This model is essential for the analysis of electric and magnetic properties of matter. It is based on the laws of quantum physics and essentially leads to the description of energy bands and to the calculation of their degree of occupation. This model explains for example the fundamental properties of semiconductors.
		
		\item[L1.] Electromagnetism: This model is essential for the analysis of devices working at microwave frequencies and the electromagnetic devices. It is based on Maxwell's relations and uses the mathematical theory of partial differential equations. This model don't allows anymore to analyze the influence of an atom as the studied objects are at a macroscopic level, described by their dimensions, their permittivity, their conductivity, etc.
		
		\item[L2.] Circuit Theory: This model is essential for the analysis of electronic devices in the very common case where the dimensions of the device are well below the wavelength of the phenomenon studied. This model is based on Kirchhoff's lemma and the definition of a half-dozen of discrete elements, resistance, capacitance, inductance, etc. There is nore more geometries  in such a model but only a topologic (structural) approach. One can calculate the current and voltage, scalar quantities, and the fields have most of time meaning anymore at this scale. Mathematical techniques are those of ordinary differential equations, Laplace transformations, complex computation and matrices, etc.
		
		\item[L3.] Block Diagrams: At this level, we do not take into account currents or voltages, and we don't care about the geometry (topology) of the system. This is constituted by the block connections doing functions/works characterized by the relations between output and input variables.
		
		\item[L4.] Systems: At this level, we schematize as a functional block a set of blocks of level 3. A computer is such an interconnection of various logical systems each performing a particular function.
		
		\item[L5.] Software: From this level, the engineer don't add any more additional devices, don't combines them anymore into larger systems, but program the machine. The theoretical methods then are usually closer to the linguistic than to mathematics.
	\end{enumerate}
	
	\subsection{Elementary Primitive Electrical Symbols}	
	This section will help you become familiar with the language and some methods of calculation used by engineers of the industry for the first 4 levels. However and strictly speaking, any training in this area should be completed by practical laboratory work!
	Sadly still in 2016 there are several national and international standards for graphical symbols in circuit diagrams, in particular:
	\begin{itemize}
		\item IEC 60617 (also known as British Standard BS 3939).
		\item ANSI standard Y32.2 (also known as IEEE Std 315).
		\item IEEE Std 91/91a: graphic symbols for logic functions (used in digital electronics). It is referenced in ANSI Y32.2/IEEE Std 315.
Australian Standard AS 1102.
	\end{itemize}
	
	We have tried to respect at best in diagrams the norm NF EN 60617 for electrical components symbols. As they are very numerous, we propose below a list only of the components used or mentioned so far in this book. This table will therefore evolve over time!
	
	First some generators:
	\vspace{1cm}
	% First row	
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\vst{25,10}{E}
	\cn{25,10}
	\tx{25,0}{vst}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vdst{25,10}{E}
	\cn{25,10}
	\tx{25,0}{vdst}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vba{25,10}{E}
	\cn{25,10}
	\tx{25,0}{vba}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vdba{25,10}{E}
	\cn{25,10}
	\tx{25,0}{vdba}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vj{25,10}{J}
	\cn{25,10}
	\tx{25,0}{vj}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vdj{25,10}{J}
	\cn{25,10}
	\tx{25,0}{vdj}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vln{25,10}{50}
	\cn{25,10}
	\tx{25,0}{vln}
	\end{picture}
	\end{center}
	
	% Second row
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\hst{0,5}{E}
	\cn{0,5}
	\tx{25,-25}{hst}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hlst{0,5}{E}
	\cn{0,5}
	\tx{25,-25}{hlst}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hba{0,5}{E}
	\cn{0,5}
	\tx{25,-25}{vba}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hlba{0,5}{E}
	\cn{0,5}
	\tx{25,-25}{vdba}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hj{0,5}{E}
	\cn{0,5}
	\tx{25,-25}{vj}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hlj{0,5}{E}
	\cn{0,5}
	\tx{25,-25}{vdj}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hln{0,5}{50}
	\cn{0,5}
	\tx{25,-25}{vln}
	\end{picture}
	\\[2cm]
	\end{center}
	
	% Third row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\ve{25,10}{E}
	\cn{25,10}
	\tx{25,0}{ve}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vde{25,10}{E}
	\cn{25,10}
	\tx{25,0}{vde}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\he{0,35}{E}
	\cn{0,35}
	\tx{25,0}{he}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hle{0,35}{E}
	\cn{0,35}
	\tx{25,0}{hle}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vcvs{25,10}{Au}
	\cn{25,10}
	\tx{25,0}{vcvs}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vccs{25,10}{gu}
	\cn{25,10}
	\tx{25,0}{vccs}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vnst{25,10}{e}
	\cn{25,10}
	\tx{25,0}{vnst}
	\end{picture}
	\\[2cm]
	\end{center}
	
	%Fourth row
	\pagebreak
	And now some passive elements (impedance):
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\vz{25,10}{Z}
	\cn{25,10}
	\tx{25,0}{vz}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vc{25,10}{C}
	\cn{25,10}
	\tx{25,0}{vc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vl{25,10}{L}
	\cn{25,10}
	\tx{25,0}{vl}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vlr{25,10}{L}
	\cn{25,10}
	\tx{25,0}{vlr}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vx{25,10}{X}
	\cn{25,10}
	\tx{25,0}{vx}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vr{25,10}{R}
	\cn{25,10}
	\tx{25,0}{vr}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\veng{25,10}{M}
	\cn{25,10}
	\tx{25,0}{veng}
	\end{picture}
	\end{center}
	
	% Fifth row
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\hz{0,5}{Z}
	\cn{0,5}
	\tx{25,-25}{hz}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hc{0,5}{C}
	\cn{0,5}
	\tx{25,-25}{hc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hl{0,5}{L}
	\cn{0,5}
	\tx{25,-25}{hl}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hld{0,5}{L}
	\cn{0,5}
	\tx{25,-25}{hld}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hx{0,5}{X}
	\cn{0,5}
	\tx{25,-25}{hx}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hr{0,5}{R}
	\cn{0,5}
	\tx{25,-25}{hr}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hrold{0,5}{R}
	\cn{0,5}
	\tx{25,-25}{hrold}
	\end{picture}
	\\[2cm]
	\end{center}
	
	%6. row
	And some transformators:
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\vmlc{0,10}
	\cn{0,10}
	\tx{20,0}{vmlc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vml{0,10}{L_1}{L_2}{M}
	\cn{0,10}
	\tx{25,0}{vml}
	\end{picture}
	%
	\begin{picture}(70,50)(-10,0)
	\vt{0,10}{L_1}{L_2}{M}
	\cn{0,10}
	\tx{25,0}{vt}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vm{0,10}{L_1}{L_2}{M}
	\cn{0,10}
	\tx{25,0}{vm}
	\end{picture}
	%
	\begin{picture}(70,50)(-10,0)
	\vlm{0,10}{M}{L_1}{L_2}
	\cn{0,10}
	\tx{25,0}{vlm}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vlmi{0,10}{M}{L_1}{L_2}
	\cn{0,10}
	\tx{25,0}{vlmi}
	\end{picture}\hfill\\[1cm]
	%
	\end{center}
	
	% 7th  row
	Some domestic elements:
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\vlamp{25,10}{L}
	\cn{25,10}
	\tx{25,0}{vlamp}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hso{0,35}{t=0}
	\cn{0,35}
	\tx{25,0}{hso}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hsc{0,35}{S}
	\cn{0,35}
	\tx{25,0}{hsc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vso{25,10}{S}
	\cn{25,10}
	\tx{25,0}{vso}
	\end{picture}
	\end{center}
	
	Some measurement elements:
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\vavo{25,10}{A}{I}
	\cn{25,10}
	\tx{25,0}{vavo}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\havo{0,35}{V}{U}
	\cn{0,35}
	\tx{25,0}{havo}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vpm{25,35}{W}{P}
	\cn{25,35}
	\tx{25,0}{vpm}
	\end{picture}\\[1cm]
	\end{center}
	
	%8th row
	\pagebreak
	Some equipotentials:
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\uu{25,10}{U}
	\cn{25,10}
	\tx{25,0}{uu}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\du{25,10}{U}
	\cn{25,10}
	\tx{25,0}{du}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\ru{0,35}{U}
	\cn{0,35}
	\tx{25,0}{ru}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\lu{0,35}{U}
	\cn{0,35}
	\tx{25,0}{lu}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\dcru{25,10}{U}
	\cn{25,10}
	\tx{25,0}{dcru}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\ucru{25,10}{U}
	\cn{25,10}
	\tx{25,0}{ucru}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\lcuu{0,35}{U}
	\cn{0,35}
	\rcuu{0,10}{U}
	\cn{0,10}
	\tx{25,0}{lcuu}
	\tx{25,-10}{rcuu}
	\end{picture}
	\end{center}
	
	%10th row
	Some current nodes:
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\ri{25,25}{I}
	\cn{25,25}
	\tx{25,0}{ri}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\li{25,25}{I}
	\cn{25,25}
	\tx{25,0}{li}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\ui{25,25}{I}
	\cn{25,25}
	\tx{25,0}{ui}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\di{25,25}{I}
	\cn{25,25}
	\tx{25,0}{di}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\rui{25,25}{I}
	\cn{25,25}
	\tx{25,0}{rui}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hcj{25,25}
	\cn{25,25}
	\tx{25,0}{hcj}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vcj{25,25}
	\cn{25,25}
	\tx{25,0}{vcj}
	\end{picture}\\[2cm]
	\end{center}
	
	%11th row
	Some ground elements and bipoints measurements:
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\hg{25,25}
	\cn{25,25}
	\tx{25,0}{hg}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vg{25,25}
	\cn{25,25}
	\tx{25,0}{vg}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hgp{25,25}
	\cn{25,25}
	\tx{25,0}{hgp}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hgpu{25,25}
	\cn{25,25}
	\tx{25,0}{hgpu}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hp{25,25}{a}{b}
	\cn{25,25}
	\tx{25,0}{hp}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vp{25,50}{a}{b}
	\cn{25,50}
	\tx{50,0}{vp}
	\end{picture}\\[2cm]
	\end{center}
	
	
	%14th row
	Various diodes (simple and capacitive Diodes, Zener diodes, Schottky diodes):
	\vspace{0.5cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\ud{25,25}{D}
	\cn{25,25}
	\tx{25,0}{ud}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\dd{25,25}{D}
	\cn{25,25}
	\tx{25,0}{dd}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\rd{0,50}{D}
	\cn{0,50}
	\tx{25,0}{rd}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\ld{0,50}{D}
	\cn{0,50}
	\tx{25,0}{ld}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\rdb{0,50}{D}
	\cn{0,50}
	\tx{25,0}{rdb}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\rde{0,50}{D}
	\cn{0,50}
	\tx{25,0}{rde}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vrd{0,50}{D}
	\cn{0,50}
	\tx{25,0}{vrd}
	\end{picture}
	\\[2cm]
	\end{center}
	
	%15th row
	\vspace{0.5cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\zud{25,25}{Z}
	\cn{25,25}
	\tx{25,0}{zud}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\zdd{25,25}{Z}
	\cn{25,25}
	\tx{25,0}{zdd}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\srd{0,50}{S}
	\cn{0,50}
	\tx{25,0}{srd}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\sld{0,50}{S}
	\cn{0,50}
	\tx{25,0}{sld}
	\end{picture}
	\end{center}
	
	Some diode transistors:
	
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\snpn{0,50}{T}
	\cn{0,50}
	\tx{25,0}{snpn}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\npnb{25,25}{T}
	\cn{25,25}
	\tx{25,0}{npn}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pnpb{25,25}{T}
	\cn{25,25}
	\tx{25,0}{pnp}
	\end{picture}
	%
	\end{center}
	
	%16th row
	\vspace{2cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\npnc{0,50}{T}
	\cn{0,50}
	\tx{25,0}{npnc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pnpc{0,50}{T}
	\cn{0,50}
	\tx{25,0}{pnpc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pnpcr{50,50}{T}
	\cn{50,50}
	\tx{25,0}{pnpcr}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\npr{50,50}{T}
	\cn{50,50}
	\tx{25,0}{npr}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pnr{50,50}{T}
	\cn{50,50}
	\tx{25,0}{pnr}
	\end{picture}
	\begin{picture}(50,50)(0,0) % + 5 for a little space at the top of the page
	\npnnc{0,50}{T}
	\cn{0,50}
	\tx{25,0}{npnnc}
	\end{picture}
	\end{center}
	where he circle on the gate indicates that the transistor is in "depletion mode".  You can think of a "normally-on" switch where applying voltage to the gate turns OFF the channel conductance.  
	
	Conversely the symbol without the circle is "enhancement mode".
Equivalent to a "normally-off" switch where applying voltage to the gate turns ON the channel conductance.
	%17th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\npnwoc{0,50}{T}
	\cn{0,50}
	\tx{25,0}{npnwoc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pnpwoc{0,50}{T}
	\cn{0,50}
	\tx{25,0}{pnpwoc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\npnwocu{0,50}{T}
	\cn{0,50}
	\tx{25,0}{npnwocu}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pnpc{0,50}{T}
	\cn{0,50}
	\tx{25,0}{pnpc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\nprwoc{50,50}{T}
	\cn{50,50}
	\tx{25,0}{nprvoc}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\nprwocu{50,50}{T}
	\cn{50,50}
	\tx{25,0}{nprwocu}
	\end{picture}\\[2cm]
	\end{center}
	
	%18th row
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\enmosna{0,50}{F}
	\cn{0,50}
	\tx{25,0}{enmosna}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\enmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{enmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\epmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{epmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\dnmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{dnmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\dpmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{dpmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\njfet{0,50}{F}
	\cn{0,50}
	\tx{25,0}{njfet}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\pjfet{0,50}{F}
	\cn{0,50}
	\tx{25,0}{pjfet}
	\end{picture}\\[2cm]
	\end{center}
	
	
	%19th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\enmosrna{50,50}{F}
	\cn{50,50}
	\tx{25,0}{enmosna}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\enmosr{50,50}{F}
	\cn{50,50}
	\tx{25,0}{enmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\epmosr{50,50}{F}
	\cn{50,50}
	\tx{25,0}{epmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\epmosdd{0,50}{F}
	\cn{0,50}
	\tx{25,0}{epmosdd}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	%%%
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\fet{0,50}{F}
	\cn{0,50}
	\tx{25,0}{fet}
	\end{picture}\\[2cm]
	\end{center}
	
	
	%20th row
	\pagebreak
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\benmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{benmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\bdnmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{bdnmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\bepmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{bepmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\bdpmos{0,50}{F}
	\cn{0,50}
	\tx{25,0}{bdpmos}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\njfetm{0,50}{F}
	\cn{0,50}
	\tx{25,0}{njfetm}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\njfetr{0,50}{F}
	\cn{0,50}
	\tx{25,0}{njfetr}
	\end{picture}\\[2cm]
	\end{center}
	The most common type of transistor is named "bipolar transistor" and these are divided into NPN and PNP types.
Their construction-material is most commonly silicon (their marking has the letter B) or germanium (their marking has the letter A). Original transistor were made from germanium, but they were very temperature-sensitive. Silicon transistors are much more temperature-tolerant and much cheaper to manufacture.

	Some operational amplifier: 
	%21th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\hoa{0,25}{O}
	\cn{0,25}
	\tx{25,0}{hoa}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\ho{0,25}{O}
	\cn{0,25}
	\tx{25,0}{ho}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hop{0,00}{O}
	\cn{0,00}
	\tx{25,0}{hop}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hopi{0,00}{O}
	\cn{0,00}
	\tx{25,0}{hopi}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hoar{50,25}{O}
	\cn{50,25}
	\tx{25,0}{hoar}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hod{0,75}{O}
	\cn{0,75}
	\tx{25,0}{hod}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hou{0,25}{O}
	\cn{0,25}
	\tx{25,0}{hou}
	\end{picture}\\[2cm]
	\end{center}
	
	Some logic (boolean) gates:
	%22th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\hand{0,25}
	\cn{0,25}
	\tx{10,0}{hand}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hnand{0,25}
	\cn{0,25}
	\tx{10,0}{hnand}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hor{0,25}
	\cn{0,25}
	\tx{10,0}{hor}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hnor{0,25}
	\cn{0,25}
	\tx{10,0}{hnor}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hxor{0,25}
	\cn{0,25}
	\tx{10,0}{hxor}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hxnor{0,25}
	\cn{0,25}
	\tx{10,0}{hxnor}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hnot{0,25}
	\cn{0,25}
	\tx{10,0}{hnot}
	\end{picture}\\[2cm]
	\end{center}
	
	
	%23th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\sr{0,25}
	\cn{0,25}
	\tx{10,0}{sr}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\jk{0,25}
	\cn{0,25}
	\tx{10,0}{jk}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\jkff{0,25}
	\cn{0,25}
	\tx{10,0}{jkff}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\dff{0,25}
	\cn{0,25}
	\tx{10,0}{dff}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hnott{0,25}
	\cn{0,25}
	\tx{10,0}{hnott}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hnots{0,25}
	\cn{0,25}
	\tx{10,0}{hnots}
	\end{picture}\\[2cm]
	\end{center}
	%

	%23th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(125,50)(0,0)
	\htp{0,25}{N}
	\cn{0,25}
	\tx{50,0}{htp}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hstp{0,25}{N}
	\cn{0,25}
	\tx{25,0}{hstp}
	\end{picture}
	%
	\\[2cm]
	\end{center}
	
	%24th row
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\hs{50,25}{Z}
	\cn{50,25}
	\tx{25,0}{hs}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\hsr{0,25}{N}
	\cn{0,25}
	\tx{25,0}{hsr}
	\end{picture}
	%
	\begin{picture}(50,50)(-10,0)
	\vtl{0,25}{Z}
	\cn{0,25}
	\tx{25,0}{vtl}
	\end{picture}
	%
	\begin{picture}(125,50)(-25,0)
	\htl{0,25}{Z}
	\cn{0,25}
	\tx{50,0}{htl}
	\end{picture}\\[2 cm]
	\end{center}
	
	%25th row
	\pagebreak
	\vspace{1cm}
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\amp{0,25}{A}
	\cn{0,25}
	\tx{25,0}{amp}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vo{0,25}{A}{15}
	\cn{0,25}
	\tx{25,0}{vo}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\voi{0,25}{Z}{15}
	\cn{0,25}
	\tx{25,0}{voi}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vor{50,25}{Z}{15}
	\cn{50,25}
	\tx{25,0}{vor}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\node{25,35}{A}
	%\cn{25,25}
	\tx{25,0}{node}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\bridge{0,50}
	\cn{0,50}
	\tx{35,0}{bridge}
	\end{picture}
	\\[2 cm]
	\end{center}
	
	%26th row
	\begin{center}
	\begin{picture}(50,50)(0,0)
	\htf{0,35}{T}
	\cn{0,35}
	\tx{25,0}{htf}
	\end{picture}
	%
	\begin{picture}(50,50)(0,0)
	\vtf{25,10}{T}
	\cn{25,10}
	\tx{25,0}{vtf}
	\end{picture}
	\end{center}

	\subsection{Alternative current VS Direct current}
	The reader will notice that throughout this section, we will mainly work with alternating current. It seems important to explain the origin of this trend of the contemporary industrial world for the alternating current before going further.

	In fact, the origin of this trend is relatively simple:

	When power plants came into being, especially in remote areas of urban centers, it was necessary to transport electric power over long distances. But the cables that carry the electricity have some resistance and this posed a major problem.

	Indeed, an average city can largely need a power of about $10$ [MW]. If this quantity had to be transported under a modest voltage of about $100$ [V], as we have $P=UI$ (\SeeChapter{see section Electrokinetics}), the current had to be enormous: $100,000$ [A]!

	But the Joule effect in the copper of $1$ [cm] diameter has a linear resistance of $R\cong 0.4\;[\Omega\cdot \text{km}^{-1}]$. With a current of $100,000$ [A], the energy loss per Joule effect would be about (neglecting the potential drop):
	
	we see quite quick the problem if we not have any supra-conductors...

	At the price of $0.1 \;[\$\cdot\text{kW}\cdot \text{h}]$, this represents a cost (loss) of about:
	
	humm ...!
	
	There were no other economic choices than to lower the current. Clearly, if the voltage reached $10^5$ [V], the same power could be carried efficiently by $100$ [A]. Thus, by raising the voltage by a factor of $1,000$, we can reduce the current by a factor of $1,000$ as well, and thus the Joule loss by a factor $10^6$.

	As there was already a simple device for raising and lowering the AC voltage (transformers) without any comparable device for DC voltage (at least at the time), the race was won by the adepts of alternating current.

	It should also be added as a second interest that some linear electrical components (see below) do not have much interest in DC ... we will come back to it!

	Let's see a simple assembly to generate single-phase alternating current:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/ac_generator_principle.jpg}
		\caption{Schematic diagram of an alternating current generator}
	\end{figure}
	The voltage (respectively the current) is determined by the Faraday law demonstrated in the section Electrokinetics:
	
	Which therefore gives the induced electromotive force (or voltage in the case of a generator without resistance...) .

	We obviously have in the situation above if the magnet is permanent and the length of the square turn is $L$:
	
	We already see that in order to obtain a given electromotive force it will be preferable to play with the frequency of rotation rather than with the surface or the intensity of the magnetic field... Or to increase the number of turns by a mounting allowing to arrive at the following relation:
	
	It should be pointed out for the skeptics that there is conservation of energy in this system! Indeed, the energy needed to turn the coil will be that used in part by the system (and that is why dams run turbines with water and nuclear power plants with steam and wind turbines with wind...).
	
	Obviously the opposite case as an alternating current injected into the coil will cause it to rotate in the magnetic field. So in a situation, we have an electric generator and in the opposite case an electric motor.

	It is possible with similar equipment to produce a nearly reliable DC voltage in the following manner named "\NewTerm{dynamo}\index{dynamo}":
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/dynamo.jpg}
		\caption{Example of direct current generator (dynamo)}
	\end{figure}
	The simple generator given first with some horseshoe magnets producing the magnetic field was widely used at the beginning of the era of electrical technology but at high voltages (several [kV]) and high internal currents too (more that $50$ [A]). Metal brushes and slip rings produced sparks and deteriorated relatively quickly. Currently, this rotating armature machine is no longer widely used.

	To avoid the difficulties associated with voltages exceeding about $600$ [V], we now turn electromagnets around a stationary armature. The current supplying the rotating electromagnets (which may also be permanent magnets) is relatively low and does not pose any problem to the operation of the rings and brushes. This configuration is then referred to as an "\NewTerm{alternator}\index{alternator}".

	With linear electrical components it is also possible to straighten the tension (we will see this much further). Then we fall back on a dynamo again!
	
	\subsubsection{Average power}
	We have defined in the section Electrokinetics that Joule power was given in constant supply for a conductor by:
	
	In alternating regime system and at low frequency (in order to consider the resistance as constant) we have in the purely resistive case:
	
	It then comes (\SeeChapter{see section Differential and Integral Calculus}) for the mean value of a periodic signal of period $T$:
	
	The term in brackets can therefore be compared to the value that would have a direct current producing the same Joule power. Therefore:
	
	We name this equivalent current the "\NewTerm{effective current value}\index{effective current value}" or "\NewTerm{current root mean square}\index{current root mean square}" (abbreviated "\NewTerm{RMS current}\index{RMS current}") and we denote it by:
	
	The RMS values being what the multimeters measure. For the sinusoidal regimes, we then have:
	
	Therefore we have in this special case:
	
	And using the same method, wishing to calculate the average voltage, we get in sinusoidal regime:
	
	Therefore for a voltage and current having the same phase, we can write:
	
	Strictly speaking, we must generalize this last relation for situations where the current and the voltage are out of phase with an angle (or time) $\varphi$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Unfortunately, many electronic books or even research articles have defined a power that has no physical meaning and that is named "\NewTerm{RMS power}\index{RMS power}" and measures "\NewTerm{RMS watts}\index{RMS watter}" defined by analogy by:
	
	Even though this term is used by advertisers and some editors, it has no place in good technical publications. It often appears to give a semblance of technical expertise...
	\end{tcolorbox}	
	Let us consider now another approach of the Power this time from the instantaneous point of view in a more general than purely resistive case. In a more general case, we have the right to write the voltage and the current in the sinusoidal steady state with cosines in the following form (which implicitly also takes into account the phase shift of current and voltage):
	
	We then have using the remarkable trigonometric relations (\SeeChapter{see section Trigonometry}):
	
	where we are free to to put:
	
	such that this term $\varphi$ is positive or null (we can always choose to subtract one of the two terms to the other in order to have a strictly positive or zero $\varphi$ value without changing the developments and conclusions that follow below) and where we used the results obtained previously:
	
	Therefore, in the expression:
	
	the instantaneous power thus comprises a constant component (first term) and a pulsed component with a frequency double that of the current and the voltage. 

	By using:
	
	as well as the following remarkable trigonometric relationship (\SeeChapter{see section Trigonometry}):
	
	We then have:
	
	Denoted:
	
	This last relation highlights that the instantaneous power can always be reduced in a permanent sinusoidal regime as the sum of two terms where:
	\begin{itemize}
		\item The first term is a pulsed component, always positive (by construction), which oscillates around the mean value $U_\text{eff}I_\text{eff}\cos(\varphi)$ and which translates a unidirectional energy exchange between a source and a load.

		\item The second term is an alternating component that varies sinusoidally with an amplitude $U_\text{eff}I_\text{eff}\sin(\varphi)$  and a zero average value. It is therefore alternately positive and negative and translates an oscillatory exchange of energy between a source and a charge.
	\end{itemize}

	When $\varphi=0$ (perfect purely resistive load), we then have:
	
	The mean value $U_\text{eff}I_\text{eff}\cos(\varphi)$ is then maximum and equal to $U_\text{eff}I_\text{eff}$, while the second term (the alternative component) is zero.

	On the other hand, when $\varphi=\pm\dfrac{\pi}{2}$ (purely reactive load as an ideal inductance or capacity), then:
	
	and in this case the instantaneous power is reduced to the only alternative component. From this arises the following definitions:
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{active power}\index{active power}" the term:
		
		and this quantity is measurable by a wattmeter and which corresponds to a real supply of energy convertible to work or heat and which is therefore maximum in the case of a purely resistive (ideal) load and zero in the case of a purely reactive (ideal) charge.

		\item[D2.] We name "\NewTerm{reactive power}\index{reactive power}" the term:
		
		which is not measurable by a wattmeter since equal to zero as it is alternative.

		\item[D3.] We name "\NewTerm{apparent power}\index{apparent power}" the product $U_\text{eff}I_\text{eff}$ that is in faced denoted most of time as:
		
		hence the famous "\NewTerm{power triangle}\index{power triangle}":
		\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/electric_power_triangle.jpg}
		\caption{Electric Power triangle}
	\end{figure}
		Which is in apparence a power but does not necessarily provide work, hence its name ... Traditionally it is the apparent power that is indicated on large industrial installations. It can be recognized because the units are often indicated in volts-amps [VA] rather than in watts [W] (because this is not totally a power)!
	\end{enumerate} 	
	
	\subsection{Transformers}
	A static electric transformer is an electrical machine which makes it possible to modify the voltage and current intensity values delivered by an alternative electrical power source into a voltage and current system of different values but with the same frequency and the same form.
	
	Since the invention of the first constant potential transformer in 1885, transformers have become essential for the transmission, distribution, and utilization of alternating current electrical energy. A wide range of transformer designs is encountered in electronic and electric power applications. Transformers range in size from RF transformers less than a cubic centimeter in volume to units interconnecting the power grid weighing hundreds of tons for Nuclear Plant.
	Let us recall that we have proved in the section of Electrokinetics that:
	
	If we can in one way or another to pass the flow of a first solenoid, then named "\NewTerm{primary winding}\index{primary winding}", having:
	
	to a second solenoid, then named "\NewTerm{secondary winding}\index{secondary winding}", without any loss (or at least a negligible loss) such as:
	
	Then it comes by term-by-term identification:
	
	And if the internal resistances are negligible the induced electromotive force is then equal to the voltage at the terminals, then it comes:
	
	And if all the energy of the magnetic field is transmitted in the secondary winding (without loss) by the conservation of enery, we have:
	
	from where:
	
	Thus, the ratio of the number of primary turns to the number of secondary turns totally determines the transformation ratio of the transformer which can then be used as a voltage transformation station by raising or lowering it as a function of the number of turns of l Secondary winding. It should also be noted that a transformer which increases the voltage simultaneously decreases the current and vice versa.

	A typical historical and pedagogical instrument to do this is the single-phase transformer with a ferromagnetic core:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/schematic_monophasic_transformer.jpg}
		\caption{Typical example of a single-phase transformer with a ferromagnetic core}
	\end{figure}
	In practice, monophasic transformers have windings that are concentric to minimize flux leakage. An insulation is inserted between the primary circuit and the secondary:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/transformer_3d_monophasic_concentric.jpg}
		\caption{Concentric monophasic transformer type (source Wikipedia)}
	\end{figure}
	Closed-core transformers are constructed in "core form" or "shell form". When windings surround the core, the transformer is core form; when windings are surrounded by the core, the transformer is shell form. Shell form design may be more prevalent than core form design for distribution transformer applications due to the relative ease in stacking the core around winding coils. Core form design tends to, as a general rule, be more economical, and therefore more prevalent, than shell form design for high voltage power transformer applications at the lower end of their voltage and power rating ranges (less than or equal to, nominally, $230$ [kV] or $75$ [MVA]). At higher voltage and power ratings, shell form transformers tend to be more prevalent. Shell form design tends to be preferred for extra-high voltage and higher [MVA] applications because, though more labor-intensive to manufacture, shell form transformers are characterized as having inherently better kVA-to-weight ratio, better short-circuit strength characteristics and higher immunity to transit damage:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/schematic_transformer_type.jpg}
		\caption{Schematic representation of various Transformers (source: Wikipedia, author: Spinningspark)}
	\end{figure}
	Transformers by their ability to increase voltage and reduce current (saving by Joule loss as we have already mentioned) play an important role in the transmission of electricity from domestic infrastructures (low voltage: LV, medium voltage: MV, high voltage HV). Thus we find the step-up transformers (SUT) and step-down transformers (STD) in everyday life:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/transformer_cascade.jpg}
		\caption{Typical civilian voltage transformation cascade installation in Switzerland}
	\end{figure}
	\pagebreak
	Here is an example of the largest triphasic step-up transformer (SUT) in the world in 2014:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/transformer_largest_2014.jpg}
		\caption{Largest plant-oil-based Transformer in the World in 2014 (source: TransnetBW GmbH)}
	\end{figure}
	And a primary triphasic step-down transformer with a slice cut:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.3]{img/engineering/transformer_triphasic_stepdown_cut_slice.jpg}
		\caption{Cutaway view of liquid-immersed construction transformer. The conservator (reservoir) at top provides liquid-to-atmosphere isolation as coolant level and temperature changes. The walls and fins provide required heat dissipation balance (source: Wikipedia)}
	\end{figure}
	And a typical USA pole-mounted distribution step-down transformer  with center-tapped secondary winding used to provide split-phase power for residential and light commercial service:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.25]{img/engineering/transformer_monophasic_step_down.jpg}
		\caption{Typical pole-mounted distribution transformer in USA with center-tapped secondary winding (source: Wikipedia)}
	\end{figure}
	And finally typical small size domestic transformers for computer, TV, Hi-Fi, etc. :
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/transformer_family.jpg}
		\caption{Small domestic size Transformers (source: Mohawk Electro Techniques Inc.)}
	\end{figure}
	
	\subsubsection{Transformer universal EMF equation}
	If the flux in the core is purely sinusoidal, the relationship for either winding between its RMS voltage $e_\text{eff}$ of the winding, and the supply frequency $f$, number of turns $N$, core cross-sectional area $S$ and peak magnetic flux density $B_\text{peak}$ is given by the universal EMF equation that we will prove now!
	
	The derivation of EMF Equation of the transformer is shown below. First remember that we have:
	
	Explicitly:
	
	So we see again that the induced EMF lags flux by $90^\circ$.

	The maximum value of EMF is obviously equal to:
	
	The root mean square RMS value is therefore:
	
	As $\Phi=B_\text{peak}\cdot S$ we get finally the "\NewTerm{transformer universal EMF equation}\index{transformer universal EMF equation}":
	
	
	\pagebreak
	\subsection{Steady State linear circuits}
	We will see here circuits composed of simple elements such as resistance, capacitance and impedance. These circuits, whose representative equation is a linear differential equation, are named "\NewTerm{linear circuits}\index{linear circuits}". Moreover, they are an excellent example to see the cumbersome developments using classical mathematical tools as opposed to other more flexible and powerful techniques (representation by phasors and Laplace transforms).
	
	\subsubsection{RC series circuit}
	Any circuit having a capacitor also have a resistance, if only that of the connection wires. Such RC (Resistance-Capacitor) series circuits are very common and sometimes of great importance (pacemaker for example). Indeed, when we close a circuit which contains only resistors (purely resistive circuit), the current rises to its nominal value in an extremely short time, so that we can consider that the current and the voltage are constant with an excellent approximation a short time after the circuit was closed (at least in common civil applications...). Thus, the permanent regime is established after a very brief transitional regime. On the contrary, in a series RC circuit, voltage and current take a long time to reach their nominal values. This dependence over time has multiple applications and allows to produce a whole range of signals that can be modulated over time as required.
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rc_serie_circuit.jpg}
		\caption{RC serie circuit}
	\end{figure}
	We assume that initially the capacitor is charged and that no current flows (open switch):
	
	When we close the switch the electrons go away from the capacitor $C$. We then have at the terminals of the resistor:
	
	At the terminals of the capacitor:
	
	The equation of the circuit is then:
	
	Therefore:
	
	Trivial differential equation (but we can detail on readers request) whose solution is with the initial conditions:
	
	Therefore:
	
	The current then has the following form:
	
	It is therefore a system in which the current decreases exponentially and this even faster than the RC factor named the "\NewTerm{time constant}\index{time constant}" is small. We see this kind of system when the light inside a car turns off slowly after closing the doors.

	When this regime is placed under a constant voltage equation, then we have an equation of the form:
	
	An obvious particular solution is then:
	
	We then have the general solution:
	
	either for the current:
	
	And for the voltage at the terminals of the capacitor:
	
	Which therefore represents the voltage at the terminals of the capacitor during charging. So in the end we have the two relations:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rc_serial_voltage_profile.jpg}
		\caption[]{Charging and discharging of the capacitor when opening / closing the switch}
	\end{figure}
	Let us now study the energetic aspect of this circuit that is important in engineering, since power consumption or loss of power is a major selling factor in some applications!

	For this purpose let us take up the relation:
	
	And let us multiply by $i$:
	
	What we write:
	
	As:
	
	Where we see that as soon as the transient charge or discharge is completed, the voltage at the terminals of the capacitor being zero then the current is also zero.

	We have then:
	
	where the first term is the power supplied by the generator to the circuit, the second term is the Joule effect term and the third is the power stored in the capacitor.

	The energy supplied by the generator is stored in the capacitor and dissipated by the resistance by Joule effect.

	What is the most important is to make an energetic balance study over the whole duration of charge of the capacitor to indicate the power dissipated in the characteristics of sale (it better for must buyer than to put equations in it...). To do this, it suffices to integrate the preceding relation from $0$ to infinity to obtain the energy dissipated.

	The first term to the left of the equality gives:
	
	The second term is integrated using $i(t)$:
	
	The third term integrates immediately since we already have the primitive:
	
	Finally, we get:
	
	Thus, for long durations, half of the energy supplied by the generator is dissipated by the Joule effect and the other stored in the capacitor.
	
	
	\subsubsection{RL series circuit}
	Let us consider now the following circuit:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rl_serie_circuit.jpg}
		\caption{RL serie circuit}
	\end{figure}
	When we close the switch, we then have at the terminals of the resistor:
	
	and at the terminals of the inductance:
	
	and $U_0$ at the terminals of the DC voltage generator.
	
	The equation of the circuit is then:
	
	Therefore:
	
	By reversing:
	
	Let us do a change of variable:
	
	Then:
	
	It then comes after integration:
	
	Let us multiply the two members by $-R$, then let us take the exponential of the two members:
	
	and:
	
	Then:
	
	Where we have the time constant defined by:
	
	Thus, when the switch is closed, the current increases exponentially with an asymptote at the value $U_0/R$. Thus, unlike the RC circuit, the current tends to a non-zero fixed value when $t$ tends to infinity!
	
	We therefore have:
	
	So in the end we have the two relations:
	
	Let us now study the energy aspect that is important in engineering, since power consumption or loss of power is a major selling factor in some applications! As for the RC circuit, we will directly make a energetic balance study of the entire duration of the transient regime to signal the power dissipated in the sales characteristics (it passes better than to put equations in it...):
	
	Let us multiply the terms of the differential equation by $i(t)$:
	
	What we will write:
	
	To calculate the dissipated energy, we proceed in the same way as for the series RC circuit. We have after integration:
	
	Therefore:
	
	Unlike the case of the RC circuit, we can not integrate above with the given terminals because of the "$1-$" which is in front of the exponential because it causes the consumed power to reach the infinite, which is logical, unlike the RC circuit which ends up blocking itself after the capacitor has been charged (the current $i$ tending towards zero very quickly).

	Thus, we only integrate up to a sufficiently large limit time with respect to the values of the passive elements (two or three $\tau$), or we are interested purely in terms of the instantaneous value of the power. We have then:
	
	And therefore at the end of the transitional regime when $t\rightarrow +\infty$:
	
	Therefore, in a steady state, the resistance is the only energy dissipative element in the circuit and it is sufficient to multiply the power dissipated by the desired time interval in order to have an estimate of the energy dissipated.
	
	\subsubsection{RLC circuit}
	An electrical wire (an antenna for example) is not a perfect conductor. In reality it can be assimilated to a resistance, a capacitance and an internal inductance in series. If we take the case, for example, of the generators, we often consider only the internal resistance as non-negligible, and this obviously reduces the nominal voltage of the generator by a factor to a first approximation proportional to the current flowing through it.

	To study the behavior of such an element often named "\NewTerm{RLC circuit}\index{RLC circuit}", we first represent it in the following form:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rlc_circuit_serie.jpg}
		\caption{RLC serie circuit}
	\end{figure}
	We assume that initially the capacitor is charged and that no current flows (open switch):
	
	When we close the switch the electrons go away from the capacitor $C$. We then have at the terminals of the resistor:
	
	and at the terminals of the condensator:
	
	and at the terminal of the inductance:
	
	The sum of the potential differences of the circuit is equal to the initial difference of potential, hence:
	
	Or written differently:
	
	It is a second-order linear differential equation very well known in physics (we find it identically in other domains with just different constants). To solve it, we must look for the roots of the associated characteristic equation (\SeeChapter{see section Differential and Integral Calculus}):
	
	The latter has for discriminant:
	
	The resistance value for which this discriminant is zero is named the "\NewTerm{critical resistance}\index{critical resistance}":
	
	We can also write the discriminant in the following form:
	
	The solutions of the differential equation are different according to the number and type of the roots of the characteristic equation.
	
	\pagebreak
	\paragraph{Critically damped response}\mbox{}\\\\
	This is the case where $R=R_C$. The characteristic equation then admits a real double root since:
	
	We then have:
	
	with:
	
	The differential equation then admits a solution of the following type when the discriminant is zero (\SeeChapter{see section Differential and Integral Calculus}):
	
	by omitting the delay.
	
	Which gives for the intensity:
	
	The constants are defined by the initial conditions:
	
	We thus obtain for the global solution:
	
	So in the end, we have the two relations:
	
	The following figures illustrate the trend of the time evolution of the charge of the capacitor and of the current through the inductance. The intensity is maximum for $t=\tau$ in the inductance:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rlc_discharge_behavior_plot.jpg}
		\caption[]{Behavior of the current in the circuit during the discharge of the capacitor}
	\end{figure}
	
	\paragraph{Overdamped response (hypercritic)}\mbox{}\\\\
	This is the case where $R>R_C$. The characteristic equation then admits two distinct real roots:
	
	Therefore:
	
	The two roots are of the same sign, because by using the relations of Viete (\SeeChapter{see section Calculus}) we have:
	
	The two roots are therefore necessarily negative. We denote their absolute values:
	
	that therefore satisfies:
	
	We have seen in the section of Differential and Integral Calculus that at this moment the solution (without phase shift) is of the form:
	
	Which gives for intensity:
	
	The constants $A$ and $B$ are defined by the initial conditions:
	
	Which gives us:
	
	Either in conventional form:
	
	Let us refer to the expressions of the charge and of the intensity:
	
	The following figures illustrate the temporal evolution of these functions (remember that the roots are negative!):
	 \begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rlc_discharge_behavior_plot.jpg}
		\caption[]{Behavior of the current in the circuit during the discharge of the capacitor}
	\end{figure}
	
	\paragraph{Underdamped response (decaying oscillation) }\mbox{}\\\\
	This is the case where $R<R_C$. The characteristic equation then admits two conjugate complex roots:
	
	Which are assimilated to the resistance of the circuit. We name it "\NewTerm{complex impedance}\index{complex impedance}".

	We will see that contrary to the intuition of that time they were invented (as often in pure mathematics) complex roots have a real physical meaning.

	Let us denote for this $\alpha$ and $\omega$ the absolute values of the real and imaginary parts of these roots:
	
	with:
	
	and:
	
	We have seen in the chapter of Differential and Integral Calculus  that the solution of the differential equation is then written:
	
	What gives us for the intensity:
	
	The constants $C'$ and $\phi$ are determined by the initial conditions:
	
	Which give us:
	
	Therefore:
	
	Thus by reporting in the expressions of the charge $q$ and of the current $i$:
	
	and:
	
	Let us try to simplify this last equality a little. First we ahve proved just before:
	
	Therefore:
	
	Which gives:
	
	But, we also have:
	
	and:
	
	We then have:
	
	We have in the end the following two relations:
	
	So a plot of the current $i$ in the inductance and $q$ of the capacitance will give:
	 \begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rlc_discharge_behavior_underdamped_response_plot.jpg}
		\caption[]{Behavior of the current in the circuit during the discharge of the capacitor in underdamped response}
	\end{figure}
	where:
	
	is the "\NewTerm{amortization factor}\index{amortization factor}". If we wish to have beautiful oscillations damped a but, it is advantageous to have this term as small as possible and therefore have a small value of $R$.

	When $R$ is zero we then have:
	
	with therefore:
	
	which we name the "\NewTerm{resonance pulsation}\index{resonance pulsation}\index{resonance frequency}". Either a period of:
	
	It is then necessary to play with $C$ or $L$ to obtain the desired period in the case where the resistance is zero. Note also that this particular situation is named a "\NewTerm{harmonic oscillator}\index{harmonic oscillator}".

	Finally, from the results obtained, we thus have the generalization of the RC, RL or LC series circuits.

	Now, suppose that in the circuit we were placing a continuous supply in series in the circuit. We have then:
	
	The linear differential equation with constant coefficients now has a second member (constant in this case). We then immediately find a particular solution which it is then sufficient to add to all the solutions which we have obtained previously.

	A particular solution is therefore:
	
	Hence:
	
	Therefore:
	
	This particular solution, which is to be added to the preceding solutions, has no influence on the equations of the current (its derivative being zero). On the other hand, it shifts the plot of $q(t)$ upwards. This is the effect of adding a constant voltage source (such as a simple battery).
	
	So for summary to compare the three configurations:
	 \begin{figure}[H]
		\centering
		\includegraphics{img/engineering/rlc_modes.jpg}
		\caption[]{RLC modes}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It seems that the first practical use for RLC circuits was in the 1890s in spark-gap radio transmitters to allow the receiver to be tuned to the transmitter.
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Linear circuit in forced regime}
	he objective here will be to study the behavior of a series linear RLC circuit excited by a sinusoidal voltage generator since it is a generalization of the $RL$ or $RC$ circuits  (it is enough to cancel $L$ or $C$ respectively to back on the solutions of an $RC$ or $RL$ circuit).

	We then have:
	
	which is a differential equation of known form and therefore important in the field of acoustics because we find it identically for the study of loudspeakers.

	We could very well add a phase shift to the sine on the right  of the equality (phase arbitrary). This would not change the developments that follow, and let us recall that the cosine is only a sinus with a very precise phase shift!

	Finally, the most important thing is that if we find a particular solution to the differential equation above, then since the amplitude and the pulse can take any value at an arbitrary phase shift, then we have an infinity of particular solutions. And as we have shown in our study of differential equations that the sum of particular solutions is also a solution then it means that an excitation obtained with a Fourier series will also have a solution and by passing to the limit we have a Fourier transform (\SeeChapter{see section Sequences and Series})!!!

	So let's move on to our study. To do this, let us derive this relation with respect to $t$:
	
	Let us then seek for a particular solution of the form:
	
	We notice that this proposition of solution is in every point identical to the fundamental of a Fourier series whose term $a_0$ is zero (which is the mean of the signal or the continuous component if it exists) as the reader can check by going back to the section of Sequences and Series where we have introduced Fourier series.!

	Then let us inject these relationships into:
	
	By grouping the trigonometric terms of the same nature:
	
	What is equivalent to:
	
	Hence by identifying the terms:
	
	We can factorize:
	
	And simplifying by $\omega$:
	
	and by changing the sign of the second line:
	
	It is therefore a system of two equations with two unknowns $a$, $b$ which we solve by writing:
	
	where we thus fall back on the "inductive reactance" and the "capacitive reactance" introduced in the section of Electrokinetics.

	Which immediately gives us:
	
	hence:
	
	and therefore:
	
	We also put traditionally that (we shall see later that this expression can be assimilated to the concept of impedance by analogy with the norm of a vector):
	
	This gives the following particular solution:
	
	to a given abritrary phase value.
	
	It is possible to find $\theta$ such as:
	
	Or otherwise written (thus one sees better that we browse all the possible values outside singularities):
	
	We then have using the remarkable trigonometric relations (\SeeChapter{see section Trigonometry}):
	
	$\theta$ is therefore the phase of the current, that is to say the advance or the delay of the current on the voltage. If $\theta=0$ then we have:
	
	and then:
	
	We then say that there is "resonance of the circuit" with therefore:
	
	Either when the inductive reactance is equal to the capacitive reactance.
	
	\subsubsection{Low-pass filter}
	Let us consider the case where $L$ is zero. We have then:
	
	Therefore:
	
	Hence:
	
	Finally:
	
	We then have at the terminals of the capacitor:
	
	We see then that the voltage at the terminals of the capacitor acts as what we name a "\NewTerm{low-pass filter}". That is to say that the amplitude of the voltage across the capacitor with respect to the excitation voltage of the circuit will be reduced, and this is all the more as the frequency will be high.

	This type of tool is very useful for example to eliminate the high-frequency harmonics of a periodic signal obtained by Fourier series or for cleaning a high-frequency noise. Cascaded low-pass filters can also be used to perform spectrum analyzers.

	Here is the plot of the factor:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/low_pass_filter_plot.jpg}
	\end{figure}
	We can see that at low frequencies (on the left) the amplitude is preserved (the low-pass filter therefore let pass the low frequencies). Beyond that, the signal is turned off.
	
	The ratio:
	
	Is often expressed in decibels Db either by definition using the measure:
	
	And is then referred to as the "\NewTerm{transfer function}\index{transfer function}" of the filter.
	
	\subsubsection{High-pass filter}
	With regard to the voltage at the terminals of the resistor, we have:
	
	Which is traditionally modified in the following form:
	
	So we see that the voltage at the terminals of the resistor acts as what we name a "\NewTerm{high-pass filter}". That is to say that the amplitude of the voltage across the resistor with respect to the excitation voltage of the circuit will be reduced, and the more so as the frequency will be small.

	Here is the plot of the factor:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/high_pass_filter_plot.jpg}
	\end{figure}
	We can see that at small frequencies (on the right) the amplitude is preserved (the high-pass filter allows to pass the high frequencies). Beyond this the signal is switched off.

	The ratio:
	
	Is often expressed in decibels either by definition using the measure:
	
	and is then also referred to as the "\NewTerm{transfer function}" of the filter.

	With different types of assembled filters we can thus remove (but never completely) frequency ranges. We're talking about a "\NewTerm{band-pass filter}\index{band-pass filter}". This is the technique used, for example, for the reception of a certain radio or television channel in a specific frequency range, or also in electronic music to attenuate low pitched or high pitched sounds. Or to separate the ADSL signal from the voice of a telephone line.

	A "\NewTerm{passive filter}\index{passive filter}" is characterized by the exclusive use of linear passive components (resistors, capacitors, coupled coils or not). Therefore, their gain (power ratio between output and input) can not exceed one. They can therefore only partly attenuate signals, but not amplify them, because this would require an input of energy (which is the role of an "active filters").
	
	\subsubsection{Integrator and differentiator}
	We therefore have at the terminals of the capacitor:
	
	Now, if $\omega \gg (RC)^{-1}$, we have:
	
	If we make things so that $\theta=0$ we must have:
	
	Therefore:
	
	Since then:
	
	The circuit is then what we name logically enough ... an "\NewTerm{integrator}\index{integrator}".

	Let us now look at the resistance side:
	
	But we have:
	
	Therefore:
	
	As:
	
	We have then:
	
	If $\omega \ll (RC)^{-1}$ then:
	
	If we make things so that $\theta=\pi/2$ we must have:
	
	The circuit is then what we name logically enough ... a "\NewTerm{derivativator}".

	The utility of an integrating circuit is, for example, to transform a periodic signal into a constant (since the time average of a periodic signal having an offset will never be zero).
	
	\pagebreak
	\subsection{Amplifiers}
	An "\NewTerm{amplifier}\index{amplifier}" or "\NewTerm{electronic amplifier}\index{electronic amplifier}" is an electronic device that can increase the power of a signal (a time-varying voltage or current). An amplifier functions by using electric power from a power supply to increase the amplitude of the voltage or current signal.
	
	 An amplifier can either be a separate piece of equipment or an electrical circuit contained within another device. Amplification is fundamental to modern electronics, and amplifiers are widely used in almost all electronic equipment. Amplifiers can be divided into voltage amplifiers, current amplifiers, transconductance amplifiers, and transresistance amplifiers. 
	
	We can make a very simple amplifier using the circuit shown:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/basic_amplifier_follower.jpg}
		\caption{Simple amplifier (follower)}
	\end{figure}
	This circuit is called a follower because the output follows closely the input voltage.  The voltage gain is slightly less than $1$, but there is a high current gain.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.45]{img/engineering/basic_amplifier_config.jpg}
		\caption{Basic operational amplifier configurations}
	\end{figure}

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{20} & \pbox{20cm}{\score{3}{5} \\ {\tiny 23 votes,  64.35\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Civil Engineering}
	\lettrine[lines=4]{\color{BrickRed}C}ivil Engineering represents all the techniques for civil constructions and tools associated with it. Civil engineers are involved in the design, implementation, operation and rehabilitation of construction works and urban infrastructure that they manage to meet the needs of society, while ensuring public safety and environmental protection at least in theory. Very interesting and varied, their achievements are distributed mainly in five major areas: structures, geotechnics, hydraulics, transport and environment. As usual in this book, we will focus only on the mathematical formalization of very common cases that have a practical application in everyday life.
	
	Civil Engineering is traditionally broken into several sub-disciplines including architectural engineering, environmental engineering, geotechnical engineering, control engineering, structural engineering, earthquake engineering, transportation engineering, forensic engineering, municipal or urban engineering, water resources engineering, materials engineering, wastewater engineering, offshore engineering, facade engineering, quantity surveying, coastal engineering, construction surveying, and construction engineering. Civil engineering takes place in the public sector from municipal through to national governments, and in the private sector from individual homeowners through to international companies.
	
	Letus notice that in civil engineering it is sometimes made use of the calculation of minimal surfaces. This has been already covered by a concrete examples in the section of Analytical Mechanics. Regarding the efforts of the heat on beams, this is also already been discussed in the section of Thermodynamics.
	
	We will limit ourselves in this section to provable accurate calculations and not on experimental formulas an this in order to have a general introduction to civil engineering techniques and become familiar with the language and some methods of calculation used by engineers in this field. In practice, civil engineers make use of a standard packages with formulas including coefficients or based on tables or maps. For example, in Switzerland, the civil engineers refer to SIA standards (Swiss Society of Engineers and Architects), which contain among others a lot of empirical formulas that engineer use without knowing where they come from. The theoretical approach is obviously insufficient and any training in this field should be obviously completed by practical laboratory work.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It would be pretentious to claim to do with this section as well and also complete as the \textit{Statique} free French PDF of Nicolet Gaston Raymond that is a priori unrivaled in content and quality to this date (even compared to non-free books on the subject!). It is therefore strongly recommended to refer to it if the reader wants to drive full information about civil engineering (see the download section of the companion website).
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Static}
	Civil engineering uses a lot the tools of the Static for constructions. We're not going here obviously to rewrite the whole section here of Classical Mechanics with the fundamental principle of statistics and everything that goes with it, nor static analysis of beams that has already be done in the section of Mechanical Engineering, but only present some applicative aspects of the principle.
	
	To begin this section, let's look at least the smaller non-trivial cases encountered in practice. We would like to put agains a wall a solid object and we would like to know the strength that will endure this wall. We can represent this basically by the following scheme:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/object_against_wall.jpg}
		\caption{Massive object against a wall}
	\end{figure}
	For the wall holds it is necessary (but not  a sufficient condition, it is just necessary!!!) that the force momentum of gravity equalizes the moment of force of the wall. Then we have:
	
	Therefore:
	
	Verbatim, the foundations of the wall must be able to opposite to the force momentum.
	
	\pagebreak
	\subsection{Pulleys}
	In the context of the study of static forces, there is an industrial example that is famous and that we meet almost every week by walking or driving in front of working sites (construction cranes), train stations (stretchers), ports (ships) or by going to fitness hall or garages: the pulley! Its origin is once again an Archimedes idea (it seems...) who used it for the movement of large masses needed in various projects of his time. The relation with the civil engineering is then completely justified! Let us see this more closely (exceptionally there are very few equations).
	
	Let u consider the following situation named "\NewTerm{simple fixed pulley}\index{simple fixed pulley}" with a mass of $10$ [kg] (ie a force of $100$ [N] with Earth gravity rounded gravity to the nearest ten) hung to rope slipped into the gutter of a pulley:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/simple_fixed_pulley.jpg}
		\caption{Simple fixed pulley (source: Wikipedia)}
	\end{figure}
	A simple fixed pulley has the only mechanical advantage to be able to exert the force in a different direction to that of the movement, the force that has to be applied is the same as that required to move the object without the pulley!
	
	The anchor point of the pulley must support the force required to move the object plus the traction force, thus twice this force in the worst case. Otherwise the total load to must support the anchor point is a function of the "\NewTerm{pull angle}\index{pull angle}" of the rope (between $90^{\circ}$ and $180^{\circ}$ ) of course:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/pull_angle_180.jpg}
		\caption{Pull angle at $180^{\circ}$}
	\end{figure}
	For an angle of $180^{\circ}$ the load factor is $200\%$. A load of $10$ [kg] on the rope will represent a load of $20$ [kg] on the pulley.
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/pull_angle_90.jpg}
		\caption{Pull angle at $90^{\circ}$}
	\end{figure}
	For an angle of $90^{\circ}$, the load factor is $140\%$. A load of $10$ [kg] on the rope is a load of $14$ [kg] on the pulley.
	
	Now let consider a situation where we set one end of the rope to the support and we draw with the other end to move both the pulley AND the load of $10$ [kg]. This configuration is named "\NewTerm{simple moving pulley}\index{simple moving pulley}" or "\NewTerm{reverse pulley}\index{reverse pulley}" (the legend says that it is this system that Archimedes used it to pull an entier boat):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/simple_mobile_pulley.jpg}
		\caption{Single mobile pulley (source: Wikipedia)}
	\end{figure}
	In fact in this system (set up vertically or horizontally whatever!) it is as if there were two people who shared the effort of displacement: the wall and the free end of the rope (which is pulled).
	
	The single mobile pulley therefore reduces the force required to move the load to half (the anchor supporting the other half) and thus adding other mobile pulleys, we continue to divide the action to apply! It's stupid but he had to be think!
	
	But this system requires a pull movement of the rope end equal to twice the distance of movement of the load regardless of the radius of the pulley.
	
	Let us also indicate that more the pulley has a large radius, more the force momentum will be big too! So in the case of very heavy loads we will favor large radii for the pulleys if the system that pulls can provide only weak force.
	
	A more realistic configuration (as we will rarely put us above the anchor point to pull the rope and furthermor the previous system is not very stable mechanically speaking ...) of the single mobile pulley presented above is as follows:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/simple_mobile_pulley.jpg}
		\caption{Single fixed and mobile mixed pulley (source: Wikipedia)}
	\end{figure}
	Obviously, when we represent systems like these in schools, we neglect for simplification purposes the mass of the pulleys themselves that we should strictly speaking take into consideration!
	
	When we use several pulleys systems working together, we say that we have a configuration of "\NewTerm{composed pulleys}\index{composed pulleys}". The most common type of such a configuration is named a "\NewTerm{hoist}\index{hoist}": the pulleys are distributed in two groups (or block), one fixed and one mobile:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/pulley_system_fixed_and_mobile.jpg}
		\caption{System of combined fixed and moving pulleys (source: Wikipedia)}
	\end{figure}
	In each group we set an arbitrary number of pulleys that demultiply by the same factor the initial load. The load is obviously united with the mobile group.
	
	So we have $25$ [N] at the end of the rope. The reader can try to have fun finding the $4$ anchor points in the previous illustration and the two pulleys which each divide by $2$ the necessary pull force ... If necessary here is the same configuration but represented in and "unfolded" way:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/pulley_system_fixed_and_mobile_unfolded.jpg}
		\caption{Unfolded system of combined fixed and moving pulleys (source: Wikipedia)}
	\end{figure}
	We already see that the big top pulley is useless except to change the direction of the pulling force. In fact, the two pulleys that are used to divide the force by $2$ are the two lower one, the remainder being provided only for convenience for the movement of the rope.
	
	Let us recall to the reader that in reality we would have to take into account the friction of the rope on the pulley (science of tribology) and we have proven in the section of Classical Mechanics that the real force (stress) of a rope end relatively to the other (end) was given by:
	
	and therefore that the real useful force (stress) to lift a load was given by:
	
	So the force (stress) supplied to lift the object being equal to $T_2$, the force (real stress) lifting the weight being $T_1$, the difference gives the force (stress) making the pulley turn by the intermediate of the friction. Then we have:
	
	The moment of force of the pulley of radius $R$ is:
	
	Let us see an another well know application of a system of fixed and mobile pulleys in train stations (an not only!!!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/pulley_system_fixed_and_mobile_real.jpg}
		\caption{System of mixed fixed and mobile pulley}
	\end{figure}
	This is a hoist to tension electrical cables with a non-visible counterweight on the photograph (bottom right) that ensures a certain strength so some tension. The advantage of this system is that it allows you to add loads gradually as the cable relaxes and these loads are $4$times higher at the electrical cable level thanks to the wo two movable pulleys (on the left). The pulleys to the right are only here for convenience for the movement of the rope and... the pulling direction for the pulley at the right end.
	
	In the case of a horizontal or vertical load lift, it is easy to determine the gear ratio $D$. Indeed, if we consider $F$ as  the force required to lift vertically the object from a height $h$ by pulling the rope over a length $d$ and $F_g$ the gravitational force on the mass drawn, we then have neglecting the friction and weight of all mobile pulleys:
	
	Finally, let us notice that it is possible to play with the radius of the deflection (deviation) pulley to reduce the force to provide while keeping constant the moment of force (we then speak of "\NewTerm{differential hoist}\index{differential hoist}") but in the end the energy used remain always the same to lift an object at the same height (and we will to pull the rope even more to lift the load at the same height).
	
	\pagebreak
	To conclude about pulley the illustration below and the corresponding gives an excellent summary of what wee saw until now:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.96]{img/engineering/pulley_summary.jpg}
		\caption{Summary typical pulleys (source: Wikipedia)}
	\end{figure}
	\begin{enumerate}
		\item[(1)] Fixed Pulley:
		
	
		\item[(2)] Mobile Pulley:
		
		
		\item[(3)] Simple hoist:
		

		\item[(4)] Double hoist:
		
		That can be generalized to the $n$ case!
	\end{enumerate}
	
	\pagebreak
	We can also considered the differential hoist:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/pulley_differential_hoist.jpg}
		\caption{Summary typical pulleys (source: Wikipedia)}
	\end{figure}
	with:
	
	
	\subsubsection{Windlass}
	The windlass is an apparatus for moving heavy weights. Typically, a windlass consists of a horizontal cylinder (barrel), which is rotated by the turn of a crank or belt. A winch is affixed to one or both ends, and a cable or rope is wound around the winch, pulling a weight attached to the opposite end.
	
	Mathematically all equations about windlass can be derivative by analogy from pulleys.
	
	Consider for example the simple following windlass:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/windlass_simple.jpg}
		\caption{Simple Windlass (source: Fortec, Charles Pache)}
	\end{figure}
	For this case we have obviously (gears rules applies as seen in the section of Mechanical Engineering):
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/windlass_differential.jpg}
		\caption{Differential Windlass (source: Fortec, Charles Pache)}
	\end{figure}
	For this case we have obviously the same as the differential hoist:
	
		
	\pagebreak
	\subsection{Cornu spiral}
	The "\NewTerm{clothoid}\index{clothoid}" is a plane transcendental curve\footnote{A "\NewTerm{transcendental curve}\index{transcendental curve}" is a curve which is not defined by an algebraic equation (typically polynomial or trigonometric), but by a transcendental equation, that is to say, the unknowns are not finite quantities, but differential ($\mathrm{d}x$, $\mathrm{d}y$, representing an infinitesimal variation of the variables $x$ and $y$). Such a curve can not be constructed geometrically precisely because its equation can not be reduced in a simple form $y = f (x)$.}  which the curvature is proportional to the curvilinear abscissa. It is also named "\NewTerm{Cornu spiral}\index{Cornu spiral}", referring to Alfred Cornu, the French physicist who invented it. More rarely, it may appear under the name of "\NewTerm{radioïde arc}\index{radioïde arc}" or "\NewTerm{Euler spiral}\index{Euler spiral}" or "\NewTerm{Fresnel spiral}\index{Fresnel spiral}".
	
	This geometry is also suitable for curves for railways because a train following the path of this geometry at a constant speed will be under constant angular acceleration whatever the point of the curve, which reduces both the stresses on the rail or on the wheels and the discomfort of passengers of the train:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/clothoide_bernina_express.jpg}
		\caption{Train loop with a clothoid shape}
	\end{figure}
	Same for car:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/clothoide_highways.jpg}
		\caption{Highway bifurcation with a clothoid shape}
	\end{figure}
	Finally, the railway cable of pylons of cable car and supporting the suspension cable, take this form. Like this, it is possible to circulate the cab at maximum speed on the pylon, without inconveniencing the passengers.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/clothoide_sabot.jpg}
		\caption{Cable car pylon with a clothoid shape}
	\end{figure}
	Also this curve is used for vertical loops or loops in the roller coaster for passenger comfort, so that the vertical acceleration is continuous.
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/clothoide_roller_coaster.jpg}
		\caption{Roaller cooaster vertical loop with a clothoid shape}
	\end{figure}
	When a vehicle moves in a circular motion, he will undergo a force $\vec{F}=m\vec{a}$ perpendicular to its direction (centrifugal force) having form norm (\SeeChapter{see section Classical Mechanics}):
	
	from the start of it entry into the circular curve. This effect is problematic because for some small cars on a highway, the centrifugal force can equal the force of gravity ot the car itself (when speed is within the legal values!).

	Thus, the acceleration rise brutally from $0$ to $v^2/R$, then the engineers build sometimes the curves with an declination to improve the adhesion, but it is also possible to try to find curves for which the acceleration is more gradual. For example if the curvature $C$ given by (\SeeChapter{see section Differential Geometry}):
	
	is proportional to the path $s$ (curvilinear abscissa) traveled in the curve, we will have at the beginning of the curve $C=0$ so the acceleration will be zero.

	So what we seek are then curves such that:
	
	For this, let us recall that we can also write naturally for a circle, the curvature into the following form:
	
	Indeed, if we turn of an angle $\mathrm{d}\theta$ the we move of a length $\mathrm{d}s=R\mathrm{d}\theta$ (\SeeChapter{see section Trigonometry}).

	Then we have the relation:
	
	Thus:
	
	hence:
	
	But let us recall that the parametric equation of the circle is (\SeeChapter{see section Analytical Geometry}):
	
	We then have:
	
	Therefore:
	
	We can now write:
	
	Therefore:
	
	with a small change of variables:
	
	it comes:
	
	by taking $x_0=y_0=0$ (we can always do a translation later).
	
	The two integrals are named "\NewTerm{Fresnel integrals}\index{Fresnel integrals}" and cannot calculated in a closed form as far as we know. However, we can express them in a Taylor expansion form (\SeeChapter{see section Sequence and Series}) as:
	
	The plot of the Fresnel integral in Maple 4.00b gives:
	
	\texttt{>plot([FresnelC(t),FresnelS(t),t=-5..5]);}
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/fresnel_integral_plot_maple.jpg}
		\caption{Plot of the Fresnel integral in Maple 4.00b}
	\end{figure}
	By zooming on the part relevant for us:
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/fresnel_integral_plot_zoom_origin_maple.jpg}
		\caption[]{Focus on the origin of the Fresnel integral}
	\end{figure}
	The same thing to a given constant using the Taylor series previously presented:
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/fresnel_integral_taylor_development_plot_maple.jpg}
		\caption[]{Equivalent Taylor series expansion}
	\end{figure}
	The engineering office use special software incorporating clothoids spirals in 2D or 3D environments based on topographic surveys done by GIS specialists.
	
	\pagebreak
	\subsection{Overhead cable}
	An overhead cable is a cable for the transmission of information, laid on utility poles. Overhead telephone and cable TV lines are common in North America. Elsewhere, overhead cables are laid mainly for telephone connections of remote buildings and temporary mechanisms, as for example building sites. The same poles sometimes carry overhead power lines for the supply of electric power. Power supply companies may also use them for an in-house communication network. Sometimes these cables are integrated in the ground or power conductor. Otherwise an additional line is strung on the masts.
	
	Galilee was probably the first to be interested in the chain ovearhead cable shape that he interpreted for a parabolic arc. Jean Bernoulli, Huygens and Leibniz found (independently) in response to the challenge of Jakob Bernoulli, his true nature in 1691: it generated by a hyperbolic cosine.
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/cable_overhead_high_voltage.jpg}
		\caption[]{High-voltage overhead cable (source: chronomaths)}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/cable_suspended_pipeline.jpg}
		\caption[]{Suspended Pipeline over a river}
	\end{figure}
	
	
	\pagebreak
	\subsubsection{Free overhead cable (catenary)}
	Let us consider (source: ChronoMath) for the study a homogeneous free, flexible cable attached at two points $A$ and $B$. In its equilibrium position, the cable hangs in a vertical plane and seems to take a parabolic shape. In fact, not really ...
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/free_overhead_cable_configuration_study.jpg}
		\caption{Configuration for studying suspended cables}
	\end{figure}
	Let us create in this plane an orthonormal reference frame $(\text{O},\vec{i},\vec{j})$, where O denotes the lowest point of the cable and let us denote as always the field of gravity $\vec{g}$ that we consider as uniform.

	Let us denote by $\vec{T}_0$ the tension (force) at the point O which defeats the tension at the point $M$ so that the cable portion $\overline{\text{O}M}$ of length $L$, subjected to its linear weight $\mu\vec{g}$ at the point $G$, is in equilibrium in the static sense:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The product $\mu\vec{g}$ of the linear mass $\mu$ with the gravitational acceleration $\vec{g}$ is also very often denoted $\vec{w}$ asw as we have already see in the section of Mechanical Engineering to represent the linear load.
	\end{tcolorbox}
	Let us project on the coordinate axes by denoting by $\alpha$ the angle $\widehat{\vec{i}\vec{T}}$. We then have the following decomposition:
	
	We can then write the following system:
	
	Either after simplification:
	
	Therefore:
	
	By calculating the ratio:
	
	To get the differential equation... (here this is subtle...):
	
	where $\mathrm{d}L$ is the curvilinear abscissa of the cable (often denoted $\mathrm{d}s$ in the literature according to what is done in Differential Geometry).

	Then:
	
	But the tangent is also the derivative of the function describing the chain. So:
	
	Therefore it comes:
	
	Following the intervention of one of our reader, we will propose two ways to solve this differential equation. The first is the original one and it is a bit complicated and the second one (available much further below) is that proposed by a reader and that  is perhaps more elegant and simple.
	\begin{itemize}
		\item First approach:

		Let us put $u=y'$ and seek the primitive of the left member at first (that of the right member being obvious). The calculations made in the section of Differential and Integral Calculus in the determination of the usual primitives give us:
		
		So we have:
		
		By passing to the exponential:
		
		observing that in our problem at $x=0$ we have indeed $y'=0$.
		To find $y'$ we use a trick: We know that the function is symmetric. So if we replace $x$ by $-x$ the tangent also changes sign and passes from $y'$ to $-y'$:
		
		By subtracting:
		
		it comes:
		
		So after integration:
		
		where the expression after the first equality is named the "\NewTerm{catenary curve}\index{catenary curve}". It comes to the end:
		
		Where the constant will be determined by the initial conditions.
	
		We see with Maple 4.00b the difference between a parabola and the chain easily:
	
		\texttt{>plot([x\string^2,cosh(x)],x=-4...4);}
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.8]{img/engineering/catenary_vs_parabola.jpg}
			\caption{Plot between a chain and parabola with Maple 4.00b}
		\end{figure}
		In real life as an artwork:
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.4]{img/engineering/st_louis_catenary.jpg}
			\caption{The Gateway Arch St. Louis Arch (source: Wikipedia, authot: Daniel Schwen}
		\end{figure}
		Let us consider now two points $(x_1,y_1)$ and $(x_2,y_2)$ in the plane and let us determine the equation of the chain of length $L$ having these two points as ends.

		We have the two equations:
		
		We obtain a third equation using the length $L$ which is known. Indeed (\SeeChapter{see section of Analytical Mechanics}):
		
		where we always have:
		
		Thus, we get a nonlinear system of three equations with three unknowns $(k,c_1^{te},c_2^{te})$:
		We have the two equations:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Let us determine for example the chain of length $38$ [cm] passing through the points $(-9,0)$ and $(9,10)$.\\
	
		The following system must then be solved:
		
		Here are the Maple 4.00b commands that allow us to get the result.\\
		
		\texttt{> E1: = 0 = k * cosh (-9 / k + c1) + c2;\\
		> E2: = 10 = k * cosh (9 / k + c1) + c2;\\
		> E3: = 38 = k * (sinh (9 / k + c1) -sinh (-9 / k + c1));
		\\> Fsolve ({e1, e2, e3}, {k, c1, c2}, {k = 0..infinity});
		}
		Maple gives:
		\begin{center}
			\texttt{K = 4.073758798, c1 = .2694982504, c2 = -14.46356329}
		\end{center}
		Graphically we have then:
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.75]{img/engineering/overhead_cable_small_plot_maple.jpg}
			\caption{Plot of a small overhead cable in Maple 4.00b}
		\end{figure}
		\end{tcolorbox}	
		
	
		\item Second approach:
		For this second approach of solving the differential equation, we will keep the notation proposed by the reader who contacted us. We start from the differential equation:
		
		We make the following change of variable:
		
		Therefore it comes:
		
		and then:
		
		The integration gives according to the usual primitive proved in the section of Differential and Integral Calculus:
		
		And the condition:
		
		on $x=0$ (lowest point of the overhead string for recall) imposes that the integration constant is zero and therefore that the points of attachment are by symmetry located at the same height. We have then:
		
		Therefore:
		
		that has to be compared with the previous method where we had obtained:
		
		The integration constant is determined by the points of attachment of the overhead cable distant from a distance $D$ where we have $y=0$ on $x=-D/2$ and $x=D/2$ (so the ends are on the same horizontal). We have then:
		
		It is under this last form that the overhead cable was obtained independently by Leibniz, Huyghens and Bernoulli at the end of the 17th century...!
	
		We can now easily calculate the maximum deviation (the "arrow" as the engineers in civil engineering say, that we will denoted by the letter $f$ as in the section of Mechanical Engineering) with respect to the horizontal passing through the points of attachment:
		
		Finally it can be interesting to calculate the length of the cable at equilibrium. For this, let us recall that we have demonstrated above that:
		
		In the case where the fasteners are at the same height, the chain is symmetrical and we then have:
		
		It remains to determine the constant! If $D$ is zero then $L$ must be zero. The constant is then zero and it remains:
		
		In the case of electrified railway lines, we solve the problem of the arrow by a main cable that carries of the catenary: the upper cable (below on the photo) undergoes an accepted deflection, which reduces the tension between the pylons. The catenary thus remains very linear thanks to the multiple auxiliary hooks to an auxiliary cable.
		\begin{figure}[H]
			\centering
			\includegraphics[scale=1]{img/engineering/overhead_cable_train.jpg}
			\caption{Overhead cable of Railways (source: Chronomaths)}
		\end{figure}
		Otherwise let us also notice that we also find the overhead cable in all the places of the life of every day where a cable is suspended between two points on the same horizontal.
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Consider a suspended cable with the following data:
		
		We have since the linear load of the cable is constant (taking the notation of the section of Mechanical Engineering on the way...):
		
		and therefore:
		
		Thus the arrow of the cable is $6$ meters below the horizontal of the two hangers.
		\end{tcolorbox}	
	\end{itemize}
	
	\subsubsection{Charges overhead cable (suspended bridge)}
	Let us consider the following suspended bridge carrying a constant linear load, hence the arrow $h$ is imposed by the architect, as well as the distance $D$ between the two pillars:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/suspended_cable_bridge.jpg}
		\caption{Suspension bridge (source: ISBN 0-13-814929-1)}
	\end{figure}
	The subtlety of this case study lies in the fact that the linear load is no longer in the cable itself but in the span of the bridge which has a much higher linear mass. We can no longer use the analyzes given above. Development done earlier above for normal suspended cable remains valid only until the relation:
	
	Now, we must remember that we also have:
	
	by rearranging and deriving again by $\mathrm{d}x$:
	
	On the bridge however, each portion $\mathrm{d}L$ of the cable is negligible in front of the portion $\mathrm{d}x$ of the bridge. This marks the difference between the suspended bridge and the simple overhead cable as we need to replace $\mathrm{d}L$ by $\mathrm{d}x$ (it is quite subtle with this approach but there are several possible approaches to this development). We then have the preceding relation which becomes:
	
	Either after rearrangement:
	
	By integrating a first time it comes then:
	
	And a second and last time:
	
	The constants are determined by the initial conditions. The place where we have placed the reference frame imposes that $y = 0$ in $x = 0$ and that $\mathrm{d}y / \mathrm{d}x = 0$ in $x = 0$, we have the two constant which are zero and it then remains:
	
	It is therefore the equation of a parabola and not of a ovearhead cable anymore! A small concern is that we do not know the tension (stress) in the cable. We should get rid of it. Now, because the architect requires that the arrow be $h$ at $x = D / 2$, then it comes:
	
	therefore:
	
	Again this remains a parable regardless of the weight! This is due to the fact that it is uniform in this case.

	We then have to determine the length of the cable. For this we take up the relation already recalled above (proved in the chapter section Analytical Mechanics and of Geometric Shapes):
	
	By the vertical symmetry of the function of the suspension bridge we can write:
	
	In the section of Differential and Integral Calculus in our proofs of the usual primitives, we have demonstrated for recall that:
	
	Therefore it comes:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the characteristics of the Golden Bridge of San Francisco. Its arrow $h$ is about $230$ meters and its main bearing is $1280$ [m]. We have then:
	
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Very tense cable}
	Let us recall that in the general case we get the following relation:
	
	Let us take the differential:
	
	Thus for the component $y$:
	
	But we also have:
	
	So injected into the previous relation this gives:
	
	Therefore:
	
	And under the assumption that the cable is very tight (under hight stress):
	
	Therefore it comes:
	
	After a first integration it comes:
	
	Before going further, let us notice that the constant is easy to determine since in $x = L / 2$, the derivative must be zero. We have then:
	
	After a second integration (where the integration constant is zero):
	
	We then for the deformation of the string:
	
	Thus, in the middle of the cable (string), the arrow is then:
	
	Or respectively if the arrow is known and the tension is sought to be determined:
	
	And we could also calculate the length of the cable (string) using the same technique as for the suspension bridge.
	
	\pagebreak
	\subsection{Falling chimney (naive approach)}
	The blasting of a falling chimney problem is a well know study in high-school and an interesting application of circular motion kinematics and moment of inertia. It is especially interesting as we can found many approach to this problem on Internet and textbooks but many known one are vicious as they hide important assumptions or are even wrong but still gives the correct final result.
	
	Our purpose here is to give the simple possible way to determine the result without forgetting to enumerate any assumption! Our purpose is also to take this opportunity to introduce first a physical phenomenon that also appears withing the falling chimney study framework that some people consider as "magic": the Hinged Stick and Ball trick!
	
	Let us consider a plank that is able to pivot about one end, with the other end free to move. As in figure below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/falling_plank.jpg}
	\end{figure}
	 the free end of the plank is raised a certain angle $\theta$ above a table, with a ball set on the raised end. The plank is then released. All points on the plank (except those at the pivot) will follow circular trajectories, each with a different radius. The linear velocity of each point, directed tangentially to the circular path of the point, is then dependent upon its distance from the fixed end of the plank. The ball, however, falls independently of the plank.
	 
	Then we assume:
	\begin{itemize}
		\item There is no friction at the pivot point
		\item We are in vacuum
		\item The plank is infinitely rigid
		\item The gravitational force is constant at any point
		\item The plank is assimilated to a cylinder
	\end{itemize}

	It is of interest to find the acceleration of various points on the plank. First, consider the rotational dynamics of the system while the plank is in motion. Once the plank is released from its rest position, the two forces acting on it are gravity and a normal force. The latter is exerted by the table at the pivot, so it does not exert a torque about the pivot. The gravitational force does exert a torque on the plank about the pivot, giving the net torque $\tau$ (we change the notation of the torque that is normally $M$ in this book because later we will have $M$ for the notation of the whole mass so it would be a mess if we don't change now!) of on the plank as:
		
	where $m$ is the mass of the plank, $g$ is the acceleration due to gravity $g = 9.81\;[\text{m}\cdot \text{s}^{-2}]$, $l$ is the plank's length and $\theta$ is the angle between the raised plank and the table.
	
	The rotational equivalent of Newton's second law applies to the plank’s motion, namely:
	
	And we have proven in the section of Geometrical Shapes that the moment of inertia of a cylinder rotating around a fixed extremity point was given by:
	
	Therefore:
	
	Next, the linear acceleration $a(x,\theta)$ of a point on the plank that is at a distance $x$ from the pivot is given by (dimensional analysis):
	
	Therefore we can write:
	
	And we can equate the both $M$ to get:
	
	from which it is possible to solve for $a$, with the result being:
	
	Let $x = L$, so that the point of consideration is the moving end of the stick. Then it remains (notice that the mass $m$ does not appear!):
	
	Therefore we have that $a>g$ for the of the stick when:
	
	As we know that
	
	and in our case the plank has $x_0=0$ and $v_0=0$ it comes:
	
	The distance $x$ we are looking for depends on the initial angle $\theta$ but is trivially given (definition of the radian!) by for the extremity of the plank:
	
	Therefore (notice that the mass $m$ again... does not appear!):
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If we consider a plastic ruler of $l=30$ [cm] and of mass $m= 68$ [g] with starting angle $\theta=\pi/4$ [rad] we get:
	
	\end{tcolorbox}
	OK this basic stuff being done. Let us now focus on the chimney rupture point.
	
	In the frame of the falling chimney we have for a perfect rigid chimney:
	
	But first we have:
	
	Therefore:
	
	or what remains the same:
	
	But in reality what we can observe is that a falling object consisting on a of smaller parts (bricks) breaks apart on some points:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/falling_chimney.jpg}
		\caption{A special case of falling chimney}
	\end{figure}
	Obviously for chimneys the smaller bricks are not independent and the point of rupture depends on the material properties. So we will imagine a chimney made of a stack of bricks without any strengthening. So in this situation we observe that a falling stacks of bricks always break into parts. So the previous relations is not complete. We don't know the origin of the force that breaks apart but we know that we can insert a term that contains all possible sources of internal forces just by writing:
	
	Or we also could have written:
	
	where $\mathrm{d}F$ is the force due to the mass that is above $\mathrm{d}m$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Another way to explain this is to remember that we have proved earlier above that:
	
	Therefore the angular acceleration depends on the length of the chimney. Now if the force of gravity were the only force acting on $\mathrm{d}m$, the angular acceleration on $\mathrm{d}m$ would be independent of the length of the chimney.
	\end{tcolorbox}
	
	Now we know that what breaks such a hypothetical chimney is a torque!
	
	An element at distance $x_0$ from $0$ experiences a torque due to the rest of the chimney above it. This torque is given by:
	
	The extremal torques (maxima and minima) are found by:
	
	Therefore after derivation by $x_0$ it remains only:
	
	Or simplified a bit more:
	
	So there is a first obvious visible solution $x_0=L$ that gives:
	
	So this is the minima!
	
	Now remember that:
	
	Therefore:
	
	After simplification (assuming $\cos(\theta)\neq 0)$:
	
	Or:
	
	Hence:
	
	Or:
	
	This is a second degree polynomial on $x_0$. Therefore (\SeeChapter{see section Calculus}):
	
	
	The maximum torque is therefore found at distance $x_0 = L/3$
from $0$. This is where the chimney might POSSIBLY break.
	
	\pagebreak
	\subsection{Dams}
	A dam is a barrier that impounds water or underground streams. Reservoirs created by dams not only suppress floods but also provide water for activities such as irrigation, human consumption, industrial use, aquaculture, and navigability. Hydropower is often used in conjunction with dams to generate electricity. A dam can also be used to collect water or for storage of water which can be evenly distributed between locations. Dams generally serve the primary purpose of retaining water, while other structures such as floodgates or levees (also known as dikes) are used to manage or prevent water flow into specific land regions.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/dam_dixence.jpg}
		\caption{Dixence (Switzerland) Dam}
	\end{figure}
	We will consider here, in a first time, only a "\NewTerm{gravity dam}\index{gravity dam}" that is by definition a massive sized dam fabricated from concrete or stone masonry. Such dams are designed to hold back large volumes of water. By using concrete, the weight of the dam is actually able to resist the horizontal thrust of water pushing against it. This is why it is named a "gravity dam". Gravity essentially holds the dam down to the ground, stopping water from toppling it over.

	Gravity dams are well suited for blocking rivers in wide valleys or narrow gorge ways. Since gravity dams must rely on their own weight to hold back water, it is necessary that they are built on a solid foundation of bedrock.
	
	Therefore let us consider the following gravity dam (consisting of a solid with infinite rigidity and perfectly waterproof...) of height $z$, of length $L$ and storing water of density $\rho$:	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/gravity_dam.jpg}
		\caption[]{Simplified approach for the study of pressure on a gravity dam}
	\end{figure}
	We have proved in the section of Continuum Mechanics that the hydrostatic pressure was given by:
	
	But in this situation we obviously have:
	
	So when we place ourselves on the surface of the water at $z=h$:
	
	thus the pressure of the air at the surface of the dam sea...

	On an surface element $\mathrm{d}S$ it applies an elementary force:
	
	But:
	
	Therefore:
	
	hence after integration
	
	It is therefore the force exerted on the immersed face. The force on the emerged face (left in the illustration) is simply given by putting $\rho=0$. We have therefore for the part due to atmospheric pressure alone:
	
	and for the part due to water only:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	On average between the empty state and full state (following what we can read on the Internet) the top of a gravity dam of certain height would sometimes move with an amplitude of the order of $80$ [cm].
	\end{tcolorbox}

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{70} & \pbox{20cm}{\score{3}{5} \\ {\tiny 25 votes,  54.40\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Aerospace Engineering}
	\lettrine[lines=4]{\color{BrickRed}I}n this section we will see some useful and simple mathematical practical cases that we studied in the section of Analytic Geometry, Classical Mechanics and Continuum Mechanics and even Astronomy.
	
	Aerospace engineering is the primary field of engineering concerned with the development of aircraft and spacecraft. It is divided into two major and overlapping branches: "aeronautical engineering" and "astronautical engineering". the Aerospace Engineering (astrodynamics) which is then scientific discipline that brings together aerospace engineering techniques  (travel in the atmosphere, using airplanes or helicopter, for example) and those of astronautics (space travel, that is to say outside the atmosphere and interplanetary journeys, using shuttles space and rockets).
	
	The reader will certainly notice that the examples below are only the examples that we often find in the text textbooks as exercises.
	
	We will not here come back on the theory of conical (very important for the orbiting satellites) seen in the section Analytic Geometry, neither the gyroscope theory very useful in guiding / force the axis of rotation (spin) of satellites and already seen in the section of Classical Mechanics (by cons we can not use the theory of ballistic that can be found in the same section as we had assumed there the constant initial speed...) or theory of Lagrangian points sometimes useful to put satellites into orbit on distant stars (which does not mean that we will not use the theoretical results of these subjects we studied). Also hypothetically we will consider the body in a non-relativistic motion which is so far the most frequent case... at least actually...
	
	In this section we will neglect many things like friction, vibration, the cases with more than two bodies (stars, planets ...) and many other factors. We will also do not see the tricks specific to space engineering (like the fact that some satellites have weights attached by adjustable cables to increase or decrease their gyro momentum).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some details may seem insignificant to the reader but he must be aware that the price per kilo of rocket launch is between $20,000$ and $30,000$ on this beginning of the 21st century and this excluding insurance... So anything that can be optimized without increasing the risk hast to be optimized!
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Airfoil Lift}
	A fluid flowing past the surface of a body exerts a force on it. The "\NewTerm{lift}\index{lift}" is the component of this force that is perpendicular to the oncoming flow direction. It contrasts with the "\NewTerm{drag force}\index{draf force}", which is the component of the surface force parallel to the flow direction. If the fluid is air, the force is named an "\NewTerm{aerodynamic force}\index{aerodynamic force}". In water, it is named a "\NewTerm{hydrodynamic force}\index{hydrodynamic force}".
	
	Because lift is a force, it is a vector quantity, having both a magnitude and a direction associated with it. Lift acts through the center of pressure of the object and is directed perpendicular to the flow direction. There are several factors which affect the magnitude of lift.
	
	There are many explanations for the generation of lift found in encyclopedias, in basic physics textbooks and on Web sites. Unfortunately, many of the explanations are misleading or partially incorrect (because oversimplified) and does not quantify the value of the corresponding lift. Therefore it is naturally because this lack of scientific approach that the generation of lift have become a source of great controversy and a topic for heated arguments (this is why NASA has a special sub-site dedicated to this subject that has significantly inspired us for the below pages).

	Lift occurs when a moving flow of gas is turned by a solid object. The flow is turned in one direction, and the lift is generated in the opposite direction, according to Newton's Third Law of action and reaction. Because air is a gas and the molecules are free to move around, any solid surface can deflect a flow. For an aircraft wing, both the upper and lower surfaces contribute to the flow turning. Neglecting the upper surface's part in turning the flow leads to an incorrect theory of lift.
	
	The proponents of the arguments usually fall into two main camps: 
	\begin{enumerate}
		\item Those who support the "Bernoulli" position that lift is generated by a pressure difference across the wing

		\item Those who support the "Newton" position that lift is the reaction force on a body caused by deflecting a flow of gas
	\end{enumerate}
	Notice that we place the names in quotation marks because neither Newton nor Bernoulli ever attempted to explain the aerodynamic lift of an object. 
	
	Lift is generated by the difference in velocity between the solid object and the fluid. There must be motion between the object and the fluid: no motion, no lift.... It makes no difference whether the object moves through a static fluid, or the fluid moves past a static solid object. 
	
	Which camp is correct? How is lift generated? In fact there is no right or wrong camp. All different effects are superposed and have more or left amplitude depending on the configuration of the wing (Angle Of Attack (AOA)\index{angle of attack}).

	When a gas flows over an object, or when an object moves through a gas, the molecules of the gas are free to move about the object; they are not closely bound to one another as in a solid. Because the molecules move, there is a velocity associated with the gas. Within the gas, the velocity can have very different values at different places near the object. Bernoulli's equation, which was named for Daniel Bernoulli, relates the pressure in a gas to the local velocity; so as the velocity changes around the object, the pressure changes as well. Adding up (integrating) the pressure variation times the area around the entire body determines the aerodynamic force on the body. The lift is the component of the aerodynamic force which is perpendicular to the original flow direction of the gas. The drag is the component of the aerodynamic force which is parallel to the original flow direction of the gas. Now adding up the velocity variation around the object instead of the pressure variation also determines the aerodynamic force. The integrated velocity variation around the object produces a net turning of the gas flow. From Newton's third law of motion, a turning action of the flow will result in a re-action (aerodynamic force) on the object. So both "Bernoulli" and "Newton" are correct. Integrating the effects of either the pressure or the velocity determines the aerodynamic force on an object. We can use equations developed by each of them to determine the magnitude and direction of the aerodynamic force.
	
	What is the argument?

	Arguments arise because people mis-apply Bernoulli and Newton's equations and because they over-simplify the description of the problem of aerodynamic lift. The most popular incorrect theory of lift arises from a mis-application of Bernoulli's equation. The theory is known as the "equal transit time" or "longer path" theory which states that wings are designed with the upper surface longer than the lower surface, to generate higher velocities on the upper surface because the molecules of gas on the upper surface have to reach the trailing edge at the same time as the molecules on the lower surface. The theory then invokes Bernoulli's equation to explain lower pressure on the upper surface and higher pressure on the lower surface resulting in a lift force. The error in this theory involves the specification of the velocity on the upper surface. In reality, the velocity on the upper surface of a lifting wing is much higher than the velocity which produces an equal transit time. If we know the correct velocity distribution, we can use Bernoulli's equation to get the pressure, then use the pressure to determine the force. But the equal transit velocity is not the correct velocity. Another incorrect theory uses a Venturi flow to try to determine the velocity. But this also gives the wrong answer since a wing section isn't really half a Venturi nozzle. There is also an incorrect theory which uses Newton's third law applied to the bottom surface of a wing. This theory equates aerodynamic lift to a stone skipping across the water. It neglects the physical reality that both the lower and upper surface of a wing contribute to the turning of a flow of gas.

	The real details of how an object generates lift are very complex and do not lend themselves to simplification. For a gas, we have to simultaneously conserve the mass, momentum, and energy in the flow. Newton's laws of motion are statements concerning the conservation of momentum. Bernoulli's equation is derived by considering conservation of energy. So both of these equations are satisfied in the generation of lift; both are correct. The conservation of mass introduces a lot of complexity into the analysis and understanding of aerodynamic problems. For example, from the conservation of mass, a change in the velocity of a gas in one direction results in a change in the velocity of the gas in a direction perpendicular to the original change. This is very different from the motion of solids, on which we base most of our experiences in physics. The simultaneous conservation of mass, momentum, and energy of a fluid (while neglecting the effects of air viscosity) are called the Euler Equations after Leonard Euler. Euler was a student of Johann Bernoulli, Daniel's father, and for a time had worked with Daniel Bernoulli in St. Petersburg. If we include the effects of viscosity, we have the Navier-Stokes Equations. To truly understand the details of the generation of lift, one has to have a good working knowledge of the Euler Equations.
	
	Keep in mind that we may explain the same phenomena in many ways, one doesn't exclude the other. So now let us focus on all common possible calculations of lift in a special configuration case to be able to compare their effect numerically!
	
	\subsubsection{Newton's lift argument (skipping stone argument)}
	About paper planes... even a flat surface can produce a lift with an angle of attack. But the "equal time argument" (Bernoulli's equation) cannot explain such phenomenon.
	
	The Newton's lift is given by the relation (\SeeChapter{see section Continuum Mechanics}) that we have determined during out study of Stoke's law:
	
	
	Let's use the information we've just learned to evaluate the "skipping stone argument":
	\begin{enumerate}
		\item This theory is concerned with only the interaction of the lower surface of the moving object and the air. It assumes that all of the flow turning (and therefore all the lift) is produced by the lower surface. Experiments shows the upper surface also turns the flow. In fact, when one considers the downwash produced by a lifting airfoil, the upper surface contributes more flow turning than the lower surface. This theory does not predict or explain this effect.

		\item Because this theory neglects the action/reaction of molecules striking the upper surface, it does not predict the negative lift present in our experiment when the angle of attack is negative. On the top of the airfoil, no vacuum exists. Molecules are still in constant random motion on the upper surface (as well as the lower surface), and these molecules strike the surface and impart momentum to the airfoil as well.
		
		\item The upper airfoil surface doesn't enter into the theory at all. So using this theory, we would expect two airfoils with the same lower surface but very different upper surfaces to give the same lift. We know this doesn't occur in reality. In fact, there are devices on many airliners named "spoilers" which are small plates on the upper surface, between the leading and trailing edges. They are used to change the lift of the wing to maneuver the aircraft by disrupting the flow over the upper surface. This theory does not predict or explain this effect.
		
		\item If we make lift predictions based on this theory, using a knowledge of air density and the number of molecules in a given volume of air, the predictions are totally inaccurate when compared to actual measurements. The chief problem with the theory is that it neglects the physical properties of the fluid. Lift is created by turning a moving fluid, and all parts of the solid object can deflect the fluid.
	\end{enumerate}
	But.... this theory is not totally inaccurate. In certain flight regimes, where the velocity is very high and the density is very low, few molecules can strike the upper airfoil surface and the Newtonian theory gives very accurate predictions. These are the conditions which occur on the Space Shuttle during the early phases of its re-entry into the Earth's atmosphere at altitudes above about $80$ [km] and at velocities above $13$ Mach. For these flight conditions, the theory gives a good prediction. However, for most normal flight conditions, like those on an airliner ($10$ [km], $800\;[\text{km}\cdot \text{h}^{-1}]$), this argument does not give the right answer.
	
	\subsubsection{Bernoulli's lift argument (equal time argument)}
	The "equal time argument" states that airfoils are shaped with the upper surface longer than the bottom. The air molecules have farther to travel over the top of the airfoil than along the bottom. In order to meet up at the trailing edge, the molecules going over the top of the wing must travel faster than the molecules moving under the wing. Because the upper flow is faster, then, from Bernoulli's equation (\SeeChapter{see section Continuum Mechanics}):
	
	the pressure is lower. The difference in pressure across the airfoil produces the lift.
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/airfoil_bernoulli_argument.jpg}
		\caption{Bernoulli force pressure lift effect illustration (source: HyperPhysics, author: Rod Nave)}
	\end{figure}
	This approach is a beautiful and simple way to explain lift but... in fact, as we have seen in it in the section of Continnum Mechanics, the Bernoulli's equation must be applied on a unique stream line and therefore it is wrong to take for the starting point and endpoint by mixing the both path as they are distinct stream lines!
	
	However would be a shame that a reader thinks the second Bernoulli is never applicable to flow around an airfoil. As already mentioned we must be careful with the assumptions of this theorem, but there are still relatively easy to satisfy:
	\begin{itemize}
		\item The flow has to be incompressible. With air, it is not strictly accurate to zero speed. However, as long as one remains in low subsonic, air can reasonably be considered an incompressible fluid (in practice, as long as one remains at a Mach number of less than $0.2$ or $0.3$). And even the first Bernoulli remains available for all the barotropic compressible fluids along a streamline.
		
		\item The fluid must be perfect. The air is a viscous fluid, though the phenomena related to the viscosity remain confined to the boundary layer outside the boundary layer, the fluid can be considered perfect.
		
		\item The flow must be irrotational. In fact, it is sufficient that the flow is irrotational globally (this is the general case). The fact that there is locally turbulence does not stop to apply more generally the theorem.
	\end{itemize}
	Let's use the information we've just learned to evaluate the various parts of this theory:
	\begin{itemize}
		\item Lifting airfoils are designed to have the upper surface longer than the bottom: This is not always correct. The symmetric airfoil also generates plenty of lift and its upper surface is the same length as the lower surface. Think of a paper airplane. Its airfoil is a flat plate, top and bottom exactly the same length and shape and yet they fly just fine. This part of the theory probably got started because early airfoils were curved and shaped with a longer distance along the top. Such airfoils do produce a lot of lift and flow turning, but it is the turning that's important, not the distance. There are modern, low-drag airfoils which produce lift on which the bottom surface is actually longer than the top. This theory also does not explain how airplanes can fly upside-down which happens often at air shows and in air-to-air combat. The longer surface is then on the bottom!

		\item Air molecules travel faster over the top to meet molecules moving underneath at the trailing edge: Experiments shows us that the flow over the top of a lifting airfoil does travel faster than the flow beneath the airfoil. But the flow is much faster than the speed required to have the molecules meet up at the trailing edge. Two molecules near each other at the leading edge will not end up next to each other at the trailing edge as shown experiments. This part of the theory attempts to provide us with a value for the velocity over the top of the airfoil based on the non-physical assumption that the molecules meet at the aft end. We can calculate a velocity based on this assumption, and use Bernoulli's equation to compute the pressure, and perform the pressure-area calculation and the answer we get does not agree with the lift that we measure for a given airfoil. The lift predicted by the "equal time argument" theory is much less than the observed lift, because the velocity is too low. The actual velocity over the top of an airfoil is much faster than that predicted by the "equal time argument" theory and particles moving over the top arrive at the trailing edge before particles moving under the airfoil.

		\item The upper flow is faster and from Bernoulli's equation the pressure is lower. The difference in pressure across the airfoil produces the lift: Experiments show that this part of the theory is correct. In fact, this theory is very appealing because many parts of the theory are correct. In our discussions on pressure-area integration to determine the force on a body immersed in a fluid, we mentioned that if we know the velocity, we can obtain the pressure and determine the force. The problem with the "Equal Transit" theory is that it attempts to provide us with the velocity based on a non-physical assumption as discussed above.
	\end{itemize}
	 \begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The misconception that wings must be curved on top and flat on the bottom is commonly associated with the previously-discussed misconception that the air is required to pass above and below the wing in equal amounts of time. In fact, an upside-down wing produces lift by exactly the same principle as a rightside-up wing.
	\end{tcolorbox}
	

	\subsubsection{Euler's lift argument}
	The simultaneous conservation of mass, momentum, and energy of a fluid (while neglecting the effects of air viscosity) use the Euler Equations and we have proved in the section of Continuum Mechanics:
	
	that was the Bernoulli equation was a special case of Euler's equation (as it is itself a special case of Navier-Stokes equations).
	
	\subsubsection{Coandă lift argument}
	In its original sense, the Coandă effect refers to the tendency of a fluid jet to stay attached to an adjacent surface that curves away from the flow, and the resultant entrainment of ambient air into the flow.
	
	More broadly, some consider the effect to include the tendency of any fluid boundary layer to adhere to a curved surface, not just the boundary layer accompanying a fluid jet. It is in this broader sense that the Coandă effect is used by some to explain why the air flow remains attached to the top side of an airfoil.
	
	\subsubsection{Kutta-Joukowski lift argument}
	Many discussion of airfoil lift invoke a vortex around the moving airfoil and it well know and experimentally proven that that there is a lift around a spinning cylinder. This is named the "\NewTerm{Kutta-Joukowski theorem}\index{Kutta-Joukowski theorem}" (Kutta–Joukowski theorem relates lift to circulation much like the Magnus effect relates side force (named Magnus force) to rotation).
	
	The two early aerodynamicists, Kutta in Germany and Joukowski in Russia, worked to quantify the lift achieved by an airflow over a spinning cylinder. The lift relation is given by:
	
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/kutta_joukowski_lift_effect_illustration.jpg}
		\caption{Kutta-Joukowski lift effect illustration (source: HyperPhysics, author: Rod Nave)}
	\end{figure}
	Like all aerodynamic lift, this seems a bit mysterious, but it can be looked at in terms of a redirection of the air motion. If the cylinder traps some air in a boundary layer at the cylinder surface and carries it around with it, shedding it downward, then it has given some of the air a downward momentum. That can act to give the cylinder an upward momentum in accordance with the principle of conservation of momentum. Another approach is to say that you have exerted a downward component of force on the air and by Newton's 3rd law there must be an upward force on the cylinder. Yet another approach is to say that the top of the cylinder is assisting the airstream, speeding up the flow on the top of the cylinder. Then by the Bernoulli equation, the pressure on the top of the cylinder is diminished, giving an effective lift.
	
	We have seen that several physical principles are involved in producing lift. Each of the following statements is correct as far as it goes:
	\begin{itemize}
		\item The wing produces lift "because" it is flying at an angle of attack.
		\item The wing produces lift "because" of circulation.
		\item The wing produces lift "because" of Bernoulli's principle.
		\item The wing produces lift "because" of Newton's law of action and reaction.
	\end{itemize}

	\pagebreak
	\subsection{Cosmic speeds}
	Placing takeoff of a rocket near the equator will be of significant assistance since the tangential velocity (horizontal) of the rotation of the Earth is given by (applying the trivial circular kinematics studied in the section of Classical Mechanics):
	
	That is to says slightly more than the speed of sound on the ground in standard conditions of temperature and pressure, which represents about $5\%$ of the first cosmic speed we will deal with just a little further below (that's already something to optimize fuel usage!). And obviously, as the Earth rotates on itself from west to east it is better to leave the rocket to the East rather than straight or even worse ... to the West (note in passing that for security reasons of people on the ground it would be even better if the rocket can be sent to the East over the ocean...).
	
	Also note that we are seeking also to acquire of course vertical speed to quickly leave the dense layers of the atmosphere, but we also want to communicate to the launcher a horizontal velocity component: that is to say the velocity that allows to put satellites into orbit of the Earth or of something else less or far away (hence another reason to launch to the East).
	
	As about the difference of the gravitational acceleration it is also a factor to be taken into account. Indeed some specialized book provide:
	
	Thus, a difference of about $0.527\%$ of the weight of a space shuttle and this is not negligible to once again save fuel.
	
	c
	The "\NewTerm{first cosmic speed}\index{first cosmic speed}" or "\NewTerm{low-level orbit speed}\index{low-level orbit speed}" is the minimum velocity at which a body should be brought to lie in low orbit around the Earth. It is determined by the relation balance between centrifugal and centripetal force (\SeeChapter{see section Classical Mechanics}):
	
	from which we deduce trivially:
	
	In fact in our present time (...) an orbit escapes to the Earth's atmosphere usury only if it is at an altitude greater Than $200$ [km]. We then have (speed that approximately corresponds to that of the International Space Station after-several overlapping):
	
	That is to say an orbital period of:
	
	
	The "\NewTerm{second cosmic speed}\index{second cosmic speed}" corresponds to the release velocity of a body leaving Earth. This is the minimum speed beyond which a body can move away permanently from Earth, at least... as we neglect the presence of the Sun, the Moon and of our Galaxy... We have already proven the corresponding relation in the section of Classical Mechanics so its no use to do it again. Let us just that the following relation was obtained:
	
	What gave for Earth (taking the average radius at the sea level...)
	
	
	Because of the Earth's atmosphere, it is difficult (and not very useful) to bring an object close to its surface at this speed, this speed being too far in the hypersonic regime to be achievable by most existing propulsion systems. In addition, it will cause a destruction of most objects by atmospheric friction or compression. In practice, an object is first placed in circular orbit of the Earth orbit and then accelerated from that altitude as the friction is there almost zero, the thrust has then a very good yield (see further below the study of Hohmann transfer orbit). In addition, the following table provided by NASA shows this is their strategy they use (you may notice that starting from $105$ kilometers of altitude the speed gain is very effective):
		
	
	\subsection{Fundamental Equation of Propulsion (Tsiolkovsky rocket equation)}
	A space launcher has for mission most of time to place a load into orbit, for that it must operate in the atmosphere and vacuum. The principles used are those of the action and reaction of Newton, and the conservation of momentum (\SeeChapter{see section Classical Mechanics}): roughly we can say that a rocket accelerates high speed by ejecting gas (the balloon that deflates gives a good idea of the phenomenon). After a simple mechanical study, we can obtain the fundamental equation of propulsion.
	
	The "\NewTerm{Tsiolkovsky rocket equation}\index{Tsiolkovsky rocket equation}", or "\NewTerm{ideal rocket equation}\index{ideal rocket equation}", describes the motion of vehicles that follow the basic principle of a rocket: a device that can apply acceleration to itself (a thrust) by expelling part of its mass with high speed and thereby move due to the conservation of momentum. The equation relates the $\Delta v$ (the maximum change of velocity of the rocket if no other external forces act) with the effective exhaust velocity and the initial and final mass of a rocket (or other reaction engine).
	
	Considering the different mobile and independent parts of the rocket (in fact the main structure that ejected gas separately), we can state thanks to the principle of action and reaction (\SeeChapter{see section  Classical Mechanics}) for a given system, the sum of the external forces are:
	
	Thus, taking the rocket system and gas together, we have:
	
	That is to say:
	
	The principle of propulsion is then stated through the conservation of linear momentum that we have just established.
	
	Let us consider a mass $m$ and of speed $v_f$ rocket, the ejection velocity of the gases being $v_g$. At time $t$, we have:
	
	at the time $t+\mathrm{d}t$ we have:
	
	but the $\mathrm{d}m$ is a mass loss so we must change its sign otherwise the above relation does not correspond to the interpretation of reality. So:
	
	based the principle of conservation of linear momentum:
	
	Therefore after simplification:
	
	hence:
	
	which by integration (\SeeChapter{see section Differential and Integral Calculus}) gives the "\NewTerm{fundamental equation of propulsion}\index{fundamental equation of propulsion}" or the "\NewTerm{delta-v}\index{delta-v}" as often said in American literature and for a (non-relativistic ...) rocket outside from gravity field and in vacuum with a constant speed  of ejection of gas ...:
	
	The difference between the initial mass $m_i$ and final mass $m_f$ is often named "\NewTerm{dead mass}\index{dead mass}".
	
	Then we understand easily why rockets are composed of several staged propulsion elements. This allows them to increase their final speed in getting rid of the mass of tanks that they initially take with them.

	We conclude therefore that a launcher accelerates especially since the gas velocity is large, the thrust depends on the amount of gas supplied and speed, and the ratio of initial and final masses must be at maximum to ensure a good propulsion, that is to say that the final system structure must be as negligible as possible (final mass is then minimal).
	
	Let us notice that we also by extension the following relation between the acceleration of the rocket and the mass flow rate ejected:
	
	However, in a gravitational field (of course it is necessary that the acceleration of the rocket is greater than that of gravity at any time ...) we must add the term which slows down the rocket (the "loss by gravity") and this will give us the expression:
	
	if we assume the gravity $g$ as constant during the main acceleration phase. The latter relationship is the "\NewTerm{Tsiolkovsky formula}\index{Tsiolkovsky formula}". Denoting by $D_e$ the mass flow of propellant, we sometimes find this last relation in the following form where time no longer explicitly intervenes:
	
	as:
	
	However, it should also take into account the variation of gravity depending on the distance as we have proved it in the section Astronomy. Then we have:
	
	So if we assume the ejection speed of the gas as constant and the trajectory path in straight line from the main body of attraction, then we have that more time passes more will the rocket speed increases due at its mass decrease (of course it will at the end be constant) but at the same time less the influence of gravity is high.
	
	The theoretical value of the altitude reached, even if its expression is very simple to determine, is so false in our point of view that it is not useful to present it.
	
	The rockets do not go at the release velocity with only one propulsion stage and even ... they often have for only objective of going only to the low-orbit speed... (first cosmic speed ). The rochet is therefore in practice not freed of gravity, far from it!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate the final velocity of the first phase of Ariane 5 launch assuming constant gravity, knowing the ejection gas velocity (assumed as constant speed), the initial total mass of the rocket, the ejected mass and the mass flow (...also assumed constant). Then we have:
	
	with:
	
	While the actual communicated value is between $2,000$ and $2,800\; [\text{m}\cdot\text{s}^{-1}]$ after the cross-checking of the information given by several websites and videos of the launch control center of Ariane (so we are very far from the first cosmic speed! ). If we add the variation of gravity with altitude the calculated result would be even closer to $2,800\; [\text{m}\cdot\text{s}^{-1}]$ (for information separation of the first stage boosters seems to be about $80$ [km] of altitude and after the cross-checking of several sources about $132$ [s] after ignition).\\
	
	However, this remains an acceptable order of (and similar to the US shuttle takeoffs!!) given that we did ot take into account of the friction of the air (that for recall was proven in the section of Continuum Mechanics as being proportional to the square of the velocity in the subsonic case... !!!).
	\end{tcolorbox}
	Let us now determine the distance reached after a given time $t$ in the case of constant gravitational field approximation. We then have obviously in a first time (we change a little bit the notation to condense the developments):
	
	Therefore:
	
	We will do the following change of variable:
	
	Therefore:
	
	Which give us (\SeeChapter{see section Differential and Integral Calculus}):
	
	
	Let us determine the integration constant the fact that at time $t = 0$, we must have $z (t)$ that is zero. It then comes immediately that:
	
	That is to say finally:
	
	where we can again get rid of the explicit time variable by reusing that:
	
	Which gives:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate the height of the first launch phase of Ariane 5 assuming constant gravity with the same vaéies data as the previous example. This then gives:
	
	Which according to the cross-check of many website and videos of the Ariane  launch control center would not be too wrong because the main propellers (solid stages accelerators) are dropped at an altitude about $70$ to $125$ [km] (between $132$ and $205$ [s] after ignition) and even the cap that protects the satellites and the propulsion stage (main cryogenic stage) are sometimes dropped just a few seconds after (it always makes a few tons less! ).
	\end{tcolorbox}
	So as we have seen with the calculations of the previous example, there are still about $100$ [km] to reach to go to the low orbit and its corresponding cosmic speed using the storable propellant (EPS).
	
	Let us notice that if we take off the rocket with an acceleration equal to that of gravity (assuming the gravity constant), we would obtain a height equal to:
	
	So we can approximatively conclude that the propulsion acceleration of Ariane 5 is significantly higher than that of gravity.
	
	We can also calculate the change of the gravity at the altitude of $100$ [km] to see if the variation is significant or not in $\%$ of that at the ground level:
	
	So we see that this is $3\%$, which is quite significant.
	
	Furthermore, here is a diagram supposedly taken from a book of Arianespace showing the horizontal acceleration of the Ariane 5 in multiple of the local gravity on the launch center:
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/cosmology/ariane_5_acceleration_profile.jpg}
		\caption{Horizontal acceleration profile of Ariane 5 (source: ?)}
	\end{figure}
	Now suppose that based on the relation:
	
	that we know the limited total mass ejected. We will then also have the combustion period that will be limited. Let us denote this by:
	
	Using the relations obtained earlier in the case of a constant gravitational field, we then have of obviously:
	
	After reaching the final velocity, the speed of the rocket will be supposed given by the classical relation of the rectilinear kinematics (\SeeChapter{see section Classical Mechanics}):
	
	and also the height:
	
	By injecting the relations obtained above, we first have for the speed:
	
	and for the height:
	
	But when the maximum height is reached, the speed is zero, then we have:
	
	Then injected into the prior previous relation we get:
	
	The first two terms are positive. The third and last term is negative. Therefore, we see that to maximize the height reached, it is best to make the mass flow $D_e$ tends to infinity (ie: give all the linear momentum from the start!). Therefore, we have:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us alculate the maximum height of the first launch phase of Ariane 5 assuming constant gravity with the same values data as the previous example. This then gives:
	
	So this is the altitude at which the rocket would have a zero vertical speed and begin to fall. As we can see, it is well above the low orbit but well below the geostationary orbit as we will calculate thereafter.
	\end{tcolorbox}
	
	\subsection{Geostationary orbit}
	The geostationary orbit is an orbit at $35'786$ [km] altitude above the equator of the Earth in the equatorial plane and zero orbital eccentricity. This is a particular case of the geosynchronous orbit (otherwise the orbital period is always the period of revolution of the Earth orbit but the orbit then deviates north and south of the equator describing a analemma in the sky when viewed from a fixed point on the surface of the Earth).
	
	An object in such an orbit has an orbital period equal to the Earth's rotational period (one sidereal day) and thus appears motionless, at a fixed position in the sky, to ground observers. Communications satellites and weather satellites are often placed in geostationary orbits, so that the satellite antennas (located on Earth) that communicate with them do not have to rotate to track them, but can be pointed permanently at the position in the sky where the satellites are located. Using this characteristic, ocean color satellites with visible and near-infrared light sensors (e.g. the Geostationary Ocean Color Imager (GOCI)) can also be operated in geostationary orbit in order to monitor sensitive changes of ocean environments.
	
	Geostationary satellites are then necessarily located vertically or at the zentih of a point on the equator or, in other words, located in the equatorial plane of the Earth.
	To calculate the position of the geostationary orbit, we will first use the Newton's second law (\SeeChapter{see section Classical Mechanics}):
	
	and we have shown in the chapter of classical mechanics when the movement is circular, we have for the centrifugal force:
	
	And we will use the law of gravity presented in the section of Classical Mechanics (and proved in the section General Relativity):
	
	We will use these relations with the mass of the Earth $M_\text{Earth}=5.9736\cdot 10^{24}$ [kg], $m_s$ the mass of the satellite, the average radius of the Earth $R_\text{Earth}=6,378.14$ [km] at the equator, $h$ the height of the satellite relatively to the ground and $v$ the speed of the satellite.
	
	On geostationary orbit, there is therefore by definition a balance between the attractive gravitational forces of the planet and the centrifugal force of the satellite:
	
	By adopting the notations given above so this gives:
	
	We see then that the mass of the satellite $m_s$ can be simplified. So the geostationary orbit is independent of it! It is also important to notice that since the speed $v$ is after simplification independent of the mass of the satellite in a circular orbit, while an astronaut inside it will have the same speed and will be weightless inside thereto (it is the same for all nearby objects inside or outside of the satellite).
	
	The speed for a circular path is the ratio of the circumference of the circle on the period of time to travel the whole circumference. So we have:
	
	Therefore $T$ being equal by definition in the context geostationary orbit at the time of the day on Earth, we have taking the available tables:
	
	Let us come back to prior-previous relation after simplification:
	
	and by injecting in it the explicit relation of speed we get:
	
	Therefore:
	
	Therefore it comes finally:
	
	and the speed of the satellite in geostationary orbit is then:
	
	Note also that using the previous relations, if the radius of the orbit of a non-geostationary satellite is given and also the mass of the Earth is known to us, we can then also determine the period of revolution without having to know its speed:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.4]{img/engineering/beoing_2006_geostationnary_satellites.jpg}
		\caption{Geostationary satellites as given by public data in 2006 (source: Boeing)}
	\end{figure}
	But let us now calculate the valueo of $g$ at the alititue of the International Space Station (ISS):
	
	We often see video of astronauts in space stations, apparently weightless. But clearly, the force of gravity is acting
on them. Comparing the value of $g$ we just calculated to that on Earth ($9.80\;[\text{m}\cdot \text{s}^{-1}]$) , we see that the astronauts in the International Space Station still have $88\%$ of their weight. In fact they only appear to be weightless because they are in free fall!!
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/space_station_over_earth.jpg}
		\caption{International Space Station (source: NASA)}
	\end{figure}
	Centrifugal force is obviously often used in space engineering calculations. For example, to calculate the speed that should have a circular space laboratory of radius $r$ to simulate terrestrial gravity for attached item on the ground as illustrated below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/cosmology/spatial_station_simulate_gravity.jpg}
	\end{figure}
	we simply apply the analysis of centrifugal force again:
	
	where $g$ is the desired gravity to simulate.
	
	and we see immediately that the mass vanish to get only:
	
	Now if we considered that $2$ revolutions per minute is safe psychologically for an astronaut (it does not disturbs our brain too much). Then we get using the above relation $r\cong 200$ [m] (for comparison, in the movie \textit{2001: A Space Odyssey}, the rotating ring had a radius of $280$ [m]). So we can imagine the cost of such a structure in space and why it has not been realized until now...
	
	Once again, this circular cylinder only works as artificial gravity if you are touching it. So if you placed an basketball anywhere where it is not touching the cylinder, it would stay exactly still. If a person jumped to the center of the cylinder, he would indeed keep going!

	\subsection{Vis-Viva Equation}
	 The vis-viva equation, also referred to as "\NewTerm{orbital-energy-invariance law}\index{orbital-energy-invariance law}", is one of the equations that model the motion of orbiting bodies. It is the direct result of the principle of conservation of mechanical energy which applies when the only force acting on an object is its own weight.
	 \begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Vis viva (Latin for "live force") is a term from the history of mechanics, and it survives in this sole context. It represents the principle that the difference between the aggregate work of the accelerating forces of a system and that of the retarding forces is equal to one half the vis viva accumulated or lost in the system while the work is being done.
	\end{tcolorbox}
	In the vis-viva equation the mass $m$ of the orbiting body (e.g., a spacecraft) is taken to be negligible in comparison to the mass $M$ of the central body (e.g., the Earth). In the specific cases of an elliptical or circular orbit, the vis-viva equation may be readily derived from conservation of energy and momentum.

	Specific total energy is constant throughout the orbit. Thus, using the subscripts $a$ and $p$ to denote apoapsis (apogee) and periapsis (perigee), respectively (see the study of conics in the section of Analytical Geometry) we know that on any point of an elliptic orbit we have by conservation of energy:
	
	But using the first cosmic speed:
	
	We get (we will focus only on the apogee as the developments are the same for the perigee):
	
	Therefore:
	
	Notice that $E_\text{tot} < 0$. This is named a "\NewTerm{bound orbit}\index{bound orbit}".
	
	Let's equate the last special case expression with for any point of the orbit:
	
	Doing a little algebra and putting $a:=r_a$ we get:
	
	which gives the object's speed at any point along the orbit. This is the "\NewTerm{vis-viva equation}\index{vis-viva equation}". We often gives only the apogee version of this equation as it is the position used in Hohmann transfer orbit (see further below). It is interesting to notice that if $a=r$ (circular orbit) we fall back on the first cosmic speed (low-level orbit speed):
	

	This powerful equation does not depend on orbital eccentricity! For instance, if we observe a new object in the solar system and know its current velocity and distance, we can determine its orbital semi-major axis and thus have some idea where it came from.	
		
	Notice that the second cosmic speed (release velocity) can be obtained from the Vis-viva equation by taking the limit as $a$ approaches $+\infty$:
	
	
	\pagebreak
	\subsection{Hohmann Transfer orbit}
	 A Hohmann transfer is a transfer orbit which uses an ellipse where one end of the ellipse is at the starting planetary orbit and the other end is at the destination planet orbit in the same plane as shown in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/cosmology/hohmann_transfer_between_earth_and_mars.jpg}
		\caption{Hohmann Transfer Between Earth and Mars}
	\end{figure}
	The orbital maneuver to perform the Hohmann transfer uses two engine impulses, one to move a spacecraft onto the transfer orbit and a second to move off it.
	
	 In the above the transfer orbit meets Earth's orbit at periapse and Mars' orbit at apoapse. The velocities needed at these two points can easily be found using the Vis Viva proved previously.
	 
	The figure above show that the initial circular orbit around the Earth and departure (transfer) orbit share a common point. Therefore the satellite must apply a maneuver at this point in order to change its velocity. The initial velocity of the spacecraft is that of an object in circular orbit a given as we know by the first cosmic speed (low-level orbit speed):
	
	But at the same time we know thanks to the Vis-Viva equation what must be the speed at a given point $r$ of an ellipse so that the satellite we later be on point $a$ (transfer orbit) :
	
	So as we know $r$ this is the common point between the Earth's orbit and transfer orbit and that we know $a$ that is the common point of the Transfer orbit and Mars' orbit the difference gives us the speed to gain:
	
	And as we have (kinematics):
	
	where $a$ is the acceleration and not the apoapse, we can know what is the acceleration (and therefore the force) that has to be applied to the satellite as quick as possible to put it on its transfer orbit.
	
	Finally, one can calculate the transit time for the Hohmann transfer to complete. As seen in the figure above, the satellite on the transfer orbit travels through $\pi$ ($180^\circ$) of true anomaly. Thus the transfer time can be found by taking half of the elliptic orbit period given by the third's Kepler law (\SeeChapter{see section Astronomy}):
	
	The Hohmann transfer for which the above analysis applies is often used because it is a highly efficient transfer. Because the Hohmann travels through $\pi$ of true anomaly, it allows the satellite to apply tangential burns which achieve maximum $\Delta v$ for minimum fuel consumption. However, some missions may wish to consider other transfer variables, such as launch window or transfer time, over $\Delta v$. These missions may consider type I or II trajectories. A type I trajectory travels through less than $\pi$ of true anomaly while a type II trajectory travels through greater than $\pi$.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{25} & \pbox{20cm}{\score{3}{5} \\ {\tiny 22 votes,  59.09\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Software Engineering}
	\lettrine[lines=4]{\color{BrickRed}S}oftware Engineering will be defined in this section as the set of techniques or approaches to solving problems using university-level mathematics and we find frequently implemented in many IT solutions.
	
	The reader will find in the texts that will follow mathematical tools whose details have all been proved in other chapters and sections, but for whose the computer applications had no place in the theoretical parts of this book. We have also choosen to focus only algorithms discuessed and used in our trainings that seems to make problems to our students and trainees. The reader must indeed keep in mind that there are so many algorithms that their are jobs around the world and at least as many as their are humans...
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If the reader is looking for mathematical methods to simulate 3D objects on a screen, he will have to refer to the section of Projective Geometry and if he is looking for machine learning/data mining/deep learning techniques he will have to refer to the section of Theoretical Computing.
	\end{tcolorbox}
	
	\subsection{Algorithm}
	In mathematics and computer science, an algorithm  is a self-contained step-by-step set of operations to be performed. Algorithms perform calculation, data processing, and/or automated reasoning tasks.

	An algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

	Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.

	In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to produce output from given input (perhaps null). An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.
	
	\subsection{Dichotomic Search algorithm}
	In computer science, a dichotomic search is a search algorithm that operates by selecting between two distinct alternatives (dichotomies) at each step. It is a specific type of divide and conquer algorithm. A well-known example is "\NewTerm{"binary search}\index{binary search}".

	Abstractly, a dichotomic search can be viewed as following edges of an implicit binary tree structure until it reaches a leaf (a goal or final state). This creates a theoretical tradeoff between the number of possible states and the running time. We have proved in the section of Theoretical Computing that the complexits of such an algorithm was $\mathcal{O}(\log_2(n))$

	In the case of the search of a root of a function the dichotomic search is equivalent to the bisection method (\SeeChapter{see section Theoretical Computing}) as the $x$-axes is ordered by construction.
	
	But let us do here a small recall first on the bisection method:
	
	\subsubsection{Bisection algorithm}
	
	The bisection method in mathematics is also root-finding method that repeatedly bisects an interval and then selects a subinterval in which a root must lie for further processing. It is a very simple and robust method, but it is also relatively slow. Because of this, it is often used to obtain a rough approximation to a solution which is then used as a starting point for more rapidly converging methods. The method is also named the "\NewTerm{interval halving method}\index{interval halving method}", the "\NewTerm{binary search method}\index{binary search method}", or the "\NewTerm{dichotomy method}\index{dichotomy method}".
	
	The method is applicable for numerically solving the equation $f(x) = 0$ for the real variable $x$, where $f$ is a continuous function defined on an interval $[a, b]$ and where $f(a)$ and $f(b)$ have opposite signs such that $f(a)f(b)<0$. In this case $a$ and $b$ are also said to bracket a root since, by the intermediate value theorem, the continuous function f must have at least one root in the interval $[a, b]$.

	At each step the method divides the interval in two by computing the midpoint $c = (a+b) / 2$ of the interval and the value of the function $f(c)$ at that point. Unless $c$ is itself a root (which is very unlikely, but possible) there are now only two possibilities: either $f(a)$ and $f(c)$ have opposite signs and bracket a root, or $f(c)$ and $f(b)$ have opposite signs and bracket a root. The method selects the subinterval that is guaranteed to be a bracket as the new interval to be used in the next step. In this way an interval that contains a zero of $f$ is reduced in width by $50\%$ at each step. The process is continued until the interval is sufficiently small.

	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/root_bissection_method.jpg}
		\caption{Bisection method scheme}
	\end{figure}

	Explicitly, if $f(a)$ and $f(c)$ have opposite signs, then the method sets $c$ as the new value for $b$, and if $f(b)$ and $f(c)$ have opposite signs then the method sets $c$ as the new $a$. (If $f(c)=0$ then $c$ may be taken as the solution and the process stops.) In both cases, the new $f(a)$ and $f(b)$ have opposite signs, so the method is applicable to this smaller interval.
	
	The implementation, on a computer, of this method is particularly simple. The conditions to be satisfied being only that in the interval $[a,b]$:
	\begin{itemize}
		\item $f$ must be continuous

		\item $f$ must be monotone near the root $\bar{x}$
		
		\item $f(a)f(b)<0$ to sure that there is a root
	\end{itemize}
	
	The algorithm consists therefore in performing the following steps:
	\begin{enumerate}
		\item We fix $\varepsilon>0$ as upper bound of the admissible error tolerance.
		
		\item We calculate $x=(a+b)/2$

		\item We evaluation $f(x)$
		
		\item If $|f(x)|<\varepsilon$ then the job is done, we have to display $x$ and $f(x)$
	
		\item Otherwise we proceed as following
			\begin{enumerate}
				\item we replace $a$ by $x$ if $f(x)f(a)>0$.
	
				\item we replace $b$ by $x$ if $f(x)f(b)>0$ or $f(x)f(a)<0$.
				
				\item we go back in (2)
			\end{enumerate}
	\end{enumerate}
	The previous step (4) imposes the condition for stopping the calculations. Sometimes it is better to choose another criterion calculation ending. It requires the calculated solution to be contained in an interval of length equation containing the root $x^{*}$. This test is enunciate as follows:
	\begin{enumerate}
		\item[4'.] If $|b-a|<\varepsilon$, the job is finished and $x=(a+b)/2$ is displayed. It is for sure obvious that $|x-x^{*}|<\varepsilon/2$
	\end{enumerate}
	In pseudo-code (non-unique and not optimized):\\\\
	\begin{algorithm}[H]
	 \KwData{$a$,$ b$, $\varepsilon$ expression of $f$ }
	 \KwResult{$x^{*}$}
	 initialization\;
	$x=(a+b)/2$\;
	\While{$|f(x)|>\varepsilon$}{
	    \uIf{$f(x)f(a)>0$}{
     		$a:=x$\;
	 	}
		\uElseIf{$f(x)f(b)>0\; \vee \; f(x)f(a)<0$}{
			$b:=x$\;
		}
		$x=(a+b)/2$\;
	 }
	 Display $x,f(x)$\;
	 \caption{Proportional Parts bissection pseudo-code algorithm}
	\end{algorithm}
	The equivalent Maple 4.00b code is given by:
	
	\texttt{>zero:=proc(f,a,b,pre)
	local M;\\
	M:=f((a+b)/2);\\
	if abs(M)<pre then \\
	     RETURN((a+b)/2)\\
	elif M>0 then\\
	     zero(f,a,(a+b)/2,pre)\\
	else zero(f,(a+b)/2,b,pre)\\
	     fi\\
	end:}
	
	\pagebreak
	\subsubsection{Binary search algorithm}
	"\NewTerm{Binary search}\index{binary search}", also known as "\NewTerm{half-interval search}\index{half-interval search}" or "\NewTerm{logarithmic search\footnote{because of its complexity that we proved in the section of Theoretical Computing as being of order $\mathcal{O}(\log_2(n))$}}\index{logarithmic search}", is a search algorithm that finds the position of a target value within a sorted array. It compares the target value to the middle element of the array; if they are unequal, the half in which the target cannot lie is eliminated and the search continues on the remaining half until it is successful.

	Although the idea is simple, implementing binary search correctly requires attention to some subtleties about its exit conditions and midpoint calculation.
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/binary_search.jpg}
		\caption[]{Visualization of the binary search algorithm where 4 is the target value (source: Wikipedia)}
	\end{figure}
	
	So as we said a binary search works on sorted arrays. A binary search begins by comparing the middle element of the array $A$ with the target value $T$. If the target value matches the middle element, its position in the array is returned. If the target value is less or more than the middle element, the search continues the lower or upper half of the array respectively with a new middle element, eliminating the other half from consideration.

	Here is the pseudo-code:
	
	\begin{algorithm}[H]
		\KwData{Array $A=[A_0,\ldots,A_i,\ldots,A_{n-1}]$, $T$ }
		\KwResult{$i$}
		initialization\;
		$L:=0$\;
		$R:=n-1$\;
		\uIf{$L>R$}{
	     	Exit \; '\texttt{nothing to found so exit program}  
		}
		$i:=\text{E}[(L+R)/2]$\;
		\While{$A_i<>T$}{
			\uIf{$L>R$}{
	     		Exit \; '\texttt{found nothing so exit program}  
			}
			\uIf{$A_i<T$}{
		    	$L:=i+1$\;
			}
			\uElseIf{$A_i>T$}{
				$R:=i-1$\;
			}
			$i:=\text{E}[(L+R)/2]$\;
		}
			
		Display $i$\;
		\caption{Binary search pseudo-code algorithm}
	\end{algorithm}
	When the computer scientist and professor Jon Bentley assigned binary search as a problem in a course for professional programmers, he found that $90\%$ failed to provide a correct solution after several hours of working on it, and another study published in 1988 shows that accurate code for it is only found in five out of twenty textbooks.

	Furthermore, Bentley's own implementation of binary search, published in his 1986 book \textit{Programming Pearls}, contained an overflow error that remained undetected for over twenty years. The Java programming language library implementation of binary search had the same overflow bug for more than nine years.

	In a practical implementation, the variables used to represent the indices will often be of fixed size, and this can result in an arithmetic overflow for very large arrays. If the midpoint of the span is calculated as $(L+R)/2$, then the value of $L+R$ may exceed the range of integers of the data type used to store the midpoint, even if $L$ and $R$ are within the range. This can be avoided by calculating the midpoint as $L+(R-L)/2$.

	\pagebreak
	\subsection{Tower of Hanoi algorithm}
	The Tower of Hanoi is a mathematical childish game or puzzle. It consists of three rods, and a number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks in a neat stack in ascending order of size on one rod, the smallest at the top, thus making a conical shape.

	The objective of the puzzle is to move the entire stack to another rod, obeying the following simple rules:
	\begin{itemize}
		\item Only one disk can be moved at a time.
		\item Each move consists of taking the upper disk from one of the stacks and placing it on top of another stack i.e. a disk can only be moved if it is the uppermost disk on a stack.
		\item No disk may be placed on top of a smaller disk.
	\end{itemize}
	With three disks, the puzzle can be solved in $7$ moves. The minimum number of moves required to solve a Tower of Hanoi puzzle is $2^n - 1$, where $n$ is the number of disks.

	Many people working in corporations and that never learned programming and wish to do so (typically to learn VBA) don't know before attending a programming course that most of time they don't have the capacity the conceptualize any algorithm. An excellent exercise used by teacher or trainers to exclude people from corporate training class is to ask them before the training as a test to write a general algorithm for the Hanoi tower as it is a very childish algorithm that a normal constituted brain should be able to write after $30$ minutes of reflection. The purpose is not tow write the most optimal algorithm must just to be able to write one...
	
	Approximately following my experience only $20-30\%$ of people are first able to write the solution in a human language as below:
	
	\begin{algorithm}[H]
	 \KwData{$n$ : number of disks}
	 initialization\;
	 \If{$n$ $\mathrm{is\;even}$}{
		 \nl\Repeat{until complete}{
		make the legal move between pegs A and B\;
		make the legal move between pegs A and C\;
		make the legal move between pegs B and C\;
	 	}
	   }
	   \If{$n$ $\mathrm{is\; odd}$}{
		\nl\Repeat{until complete}{
	 	make the legal move between pegs A and C\;
		make the legal move between pegs A and B\;
		make the legal move between pegs C and B\;
		}
	   }
	 \caption{Human intuitive pseudo-code algorithm of Hanoi Tower}
	\end{algorithm}
	
	Or a little bit better:
	
	\begin{algorithm}[H]
	\KwData{$n$ : number of disks}
	initialization\;
	\eIf{$n$ $\mathrm{is\; even}$}{
		\nl\Repeat{until complete}{
		make the legal move between pegs A and B\;
		make the legal move between pegs A and C\;
		make the legal move between pegs B and C\;
	 	}
	}{
		\nl\Repeat{until complete}{
	 	make the legal move between pegs A and C\;
		make the legal move between pegs A and B\;
		make the legal move between pegs C and B\;
		}
	}
	\end{algorithm}
	That is to say:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/hanoi_algorithm_1.jpg}
		\includegraphics{img/computing/hanoi_algorithm_2.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/hanoi_algorithm_3.jpg}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/hanoi_algorithm_4.jpg}
		\caption{Hanoi Tower $n=4$ algorithm illustration (source: Wikipedia)}
	\end{figure}
	
	But this is not a computer oriented pseudo-code!! If we request to the students or trainees to write a pseudo-code that is "computer oriented" that is to say to use arrays, instead of "pegs"... and use numbers instead of "disks" and write in a mathematics form the "is even" as computers don't know what is a even or odd number. When add such as constraint the number of people that successfully pass this test decrease to almost $10\%$ and this show an inability to have a structured reasoning.
	
	For example to move $n$ disks we will use numbers to identify uniquely each disk (as when first humans invented numbers to identify uniquely items in a collection):\\
	
	\begin{algorithm}[H]
	\small
	\KwData{$n$ : number of disks}
	$A$ = dim([\space],$n$)\;
	$B$ = dim([\space],$n-1$)\;
	$C$ = dim([\space],$n$)\;
	initialization\;
	\For{$i=1$ \KwTo $n$}{ 
	 $A[i]=(n+1)-i$
	}
	\If{$n>0$}{
	 	\eIf{$n\equiv 0 \mod(2)$ }{
		\nl\Repeat{$C$=$A$}{
			\eIf{$A[\mathrm{count}[A]]>B[\mathrm{count}[B]]$}{
				$B[\mathrm{count}[B]]=A[\mathrm{count}[A]]$
			}{
				$A[\mathrm{count}[A]]=B[\mathrm{count}[B]]$
			}
			\eIf{$A[\mathrm{count}[A]]>C[\mathrm{count}[C]]$}{
				$C[\mathrm{count}[C]]=A[\mathrm{count}[A]]$
			}{
				$A[\mathrm{count}[A]]=C[\mathrm{count}[C]]$
			}
			\eIf{$B[\mathrm{count}[B]]>C[\mathrm{count}[C]]$}{
				$C[\mathrm{count}[C]]=B[\mathrm{count}[B]]$
			}{
				$B[\mathrm{count}[B]]=C[\mathrm{count}[C]]$
			}
	 	}
		}{
		\nl\Repeat{$C$=$A$}{
			\eIf{$A[\mathrm{count}[A]]>C[\mathrm{count}[B]]$}{
				$C[\mathrm{count}[C]]=A[\mathrm{count}[A]]$\;
			}{
				$A[\mathrm{count}[A]]=C[\mathrm{count}[C]]$\;
			}
			\eIf{$A[\mathrm{count}[A]]>B[\mathrm{count}[B]]$}{
				$B[\mathrm{count}[B]]=A[\mathrm{count}[A]]$\;
			}{
				$A[\mathrm{count}[A]]=B[\mathrm{count}[B]]$\;
			}
			\eIf{$C[\mathrm{count}[C]]>B[\mathrm{count}[B]]$}{
				$B[\mathrm{count}[B]]=C[\mathrm{count}[C]]$\;
			}{
				$C[\mathrm{count}[C]]=B[\mathrm{count}[B]]$\;
			}
		}
	}
	}
	\caption{Hanoi Tower computer pseudo-code algorithm }
	\end{algorithm}
	This is obviously not optimal but a minimum we can expect from a human being. 
	
	\subsection{Sorting Algorithms}
	A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. 

	There is a consequent list of sorting algorithms  (see Wikipedia for an exhaustive list of almost $20$ of them). As already mention at the beginning of this section we will focus only on the algorithms used by redactor of this book when he teacher computer science in schools or firms.

	\subsubsection{Bubble sort}
	Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller elements "bubble" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems.
	
	Bubble sort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting.
	
	So let us see the idea behind the Bubble sort:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/bubble_sort.jpg}
		\caption{Schematic principle the Bubble Sort algorithm}
	\end{figure}
	
	Here is the pseudo-code of this algorithm:
	
	\begin{algorithm}[H]
	 \KwData{$A$ : list of sortable items }
	 \KwResult{$x_1$}
	 initialization\;
	 $n :=$ length$(A)$\;
	 \nl\Repeat{not swapped}{
	 swapped: = false\;
	 \For{$i=1$ \KwTo $n-1$}{ 
	 \If{$A[i-1]>A[i]$}{
	 	swap($A[i-1]$,$A[i]$)\;
		swapped := true\;
	   }
	}
	}
	display $A$ : list of sorted items\;
	 \caption{Bubble sort pseudo-code algorithm}
	\end{algorithm}
	The bubble sort algorithm can be easily optimized by observing that the $n$-th pass finds the $n$-th largest element and puts it into its final place. So, the inner loop can avoid looking at the last $n-1$ items when running for the $n$-th time:
	
	\begin{algorithm}[H]
	 \KwData{$A$ : list of sortable items }
	 \KwResult{$x_1$}
	 initialization\;
	 $n =$ length$(A)$\;
	 \nl\Repeat{not swapped}{
	 swapped := false\;
	 \For{$i=1$ \KwTo $n-1$}{ 
	 \If{$A[i-1]>A[i]$}{
	 	swap($A[i-1]$,$A[i]$)\;
		swapped := true\;
	   }
	}
	$n:=n-1$\;
	}
	Display $A$ : list of sorted items\;
	 \caption{Alternative optimized Bubble sort pseudo-code algorithm}
	\end{algorithm}

	As the bubble sort has two nested loops, the worst-case and average complexity both $\mathcal{O}(n^2)$, where $n$ is the number of items being sorted (obviously when the list is already sorted (best-case), the complexity of bubble sort is only $\mathcal{O}(n^2)$). There exist many sorting algorithms with substantially better worst-case or average complexity of  $\mathcal{O}(n \log(n))$. Even other $\mathcal{O}(n^2))$ sorting algorithms, such as insertion sort, tend to have better performance than bubble sort. Therefore, bubble sort is not a practical sorting algorithm when $n$ is large.
	
	\subsubsection{QuickSort algorithm}
	Quicksort (sometimes named "partition-exchange sort") is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959,[1] with his work published in 1961,[2] it is still a commonly used algorithm for sorting.

	Quicksort can as bubble sort operate "in-place" on an array, requiring small additional amounts of memory to perform the sorting.

	Quicksort is a divide and conquer algorithm. Quicksort first divides a large array into two smaller sub-arrays: the low elements and the high elements. Quicksort can then recursively sort the sub-arrays.

	The steps are:
	\begin{enumerate}
		\item Pick an element, named a pivot, from the array.

		\item Partitioning: reorder the array so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the partition operation.

		\item Recursively apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values.
	\end{enumerate}

	Here is an illustration of the concept:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/quick_sort_algorithm.jpg}
		\caption{Schematic principle for QuickSort algorithm}
	\end{figure}
	 In pseudocode, a quicksort that sorts elements \texttt{lo} through \texttt{hi} (inclusive) of an array $A$ can be expressed as (Lomuto partition scheme\footnote{This scheme is attributed to Nico Lomuto and popularized by Bentley in his book \textit{Programming Pearls} and Cormen et al. in their book \textit{Introduction to Algorithms}.}
):
	 
	 \begin{algorithm}[H]
		\SetAlgoLined\DontPrintSemicolon
		\SetKwFunction{algo}{quicksort($A$, lo, hi)}
		\SetKwFunction{proc}{proc}
		\KwData{Global $A$ : list of sortable items }
		\SetKwProg{myalg}{Algorithm}{}{}
		\myalg{\algo{}}{
		\If{lo$<$hi}{
	 	p:=partition($A$, lo, hi)\;
		quicksort($A$, lo, $p-1$)\;
		quicksort($A$, $p+1$, hi)\;
	    }
		\nl \KwRet\;}{}
		\setcounter{AlgoLine}{0}
		\SetKwProg{myproc}{Procedure}{}{}
		\myproc{\proc{}}{
		pivot:=A[$hi$]\;
		\For{$j=$lo \KwTo hi$-1$}{ 
			\If{$A[i-1]\leq$ pivot}{
				swap($A[i]$,$A[j]$)\;
				$i:=i+1$\;
			}
		}
		swap($A[i]$,$A[hi]$)\;
		\nl  \KwRet $i$ \;}
  		\caption{QuickSort pseudo-code algorithm with function}
  	\end{algorithm}
  	
  	\subsection{Dijkstra's algorithm }
	The "\NewTerm{Dijkstra's algorithm}\index{Dijkstra's algorithm}" is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.
	
	The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes,[2] but a more common variant fixes a single node as the "source" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.
	
	For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms.
	
	So how does a Dijkstra Algorithm work, I will explain this using four simple steps:
	\begin{enumerate}
		\item Initially assign $\text{Node}(A) = 0$ as the weight of the initial node and $w(x) = \infty$ to all other nodes, where $x$ represents the other nodes.
		
		\item Search $x$ node for which it has the smallest temporary value of $w(x)$. Stop the algorithm if $w(x) = \infty$ or there are no temporary nodes. The node $x$ is now labeled as permanent and as the current node, meaning parent of $x$ and $w(x)$ will stay fixed.
		    
		\item For each node adjacent to $x$ labeled $y$ which are also temporary, apply the following comparison: 
		if $w(x) + Wxy < w(y)$, then $w(y)$ is updated to $w(x) + Wxy$, where W is the cost of the adjacent node. Now assign y to have parent $x$
		\item Repeat the process from Step 2, doing as many iterations as required until the shortest path is found. 
	\end{enumerate} 
	%%%%%%%%%%%%%%%%%%%%% Styles for Dijktra algorithm %%%%%%%%%%%%%%%%%%%%%
	\pgfdeclarelayer{background}
	\pgfsetlayers{background,main}
	\tikzstyle{preno}=[semithick] % arêtes non orientées.
	\tikzstyle{pre}=[->,>=stealth,semithick] % arêtes orientées.
	\tikzstyle{select}=[-,cap=round,style=nearly transparent,line width=6pt] %
	\tikzstyle{fl_actif}=[-,cap=round,style=nearly transparent,line width=6pt,color=black!50!red]
	\tikzstyle{infini} =[circle,thick,inner sep=0pt,minimum size=6mm,node distance=20mm,draw=black!50,fill=blue!5]
	\tikzstyle{encours}=[circle,thick,inner sep=0pt,minimum size=6mm,node distance=20mm,draw=blue!50,fill=blue!20]
	\tikzstyle{fini}   =[circle,thick,inner sep=0pt,minimum size=6mm,node distance=20mm,draw=black!80,fill=black!30]
	\tikzstyle{actif}  =[circle,thick,inner sep=0pt,minimum size=6mm,node distance=20mm,draw=black!80!red,fill=red!20!black!30]
	
	Le but de cette présentation est de faire fonctionner l'{\em algorithme de Dijkstra} sur des exemples concrets.

	Cherchons les plus courts chemins d'origine $A$ dans ce graphe: 
	%%%%%%%%%%%%%%%%%%%%% Présentation du graphe %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[encours,label={[red]left:A}]      (s)                            {};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {};
	  \node[infini,label={[red]below:E}]      (y)        [below right of=s]  {};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]  {};
	  \node[infini,label={[red]below right:D}]      (z)        [right of=y]  {};
	% flèches soulignées.
	
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}
	
	On se place au sommet de plus petit poids, ici le sommet $A$.
	%%%%%%%%%%%%%%%%%%%%% Étape 1 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[encours,label={[red]left:A}]      (s)                            {$0$};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {$\infty$};
	  \node[infini,label={[red]below:E}]      (y)        [below right of=s]  {$\infty$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]        {$\infty$};
	  \node[infini,label={[red]below right:D}]      (z)        [right of=y]        {$\infty$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}

	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	\fbox{$0$} & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet$ & & & & \\ 
	$\bullet$ & & & & \\ 
	$\bullet$ & & & & \\ 
	$\bullet$ & & & & \\ 
	$\bullet$ & & & & \\
	\end{tabular}
	\end{center}
	
	On étudie chacune des arêtes partant du sommet choisi.
	%%%%%%%%%%%%%%%%%%%%% Étape 2.1 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[actif,label={[red]left:A}]      (s)                            {$0$};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {$10$};
	  \node[infini,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]        {$\infty$};
	  \node[infini,label={[red]below right:D}]      (z)        [right of=y]        {$\infty$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[fl_actif] (s) to (t) ;
	\draw[fl_actif] (s) to (y) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}
	
	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	\fbox{$0$} & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & $5_A$ \\ 
	$\bullet$ & & & & \\ 
	$\bullet$ & & & & \\
	$\bullet$ & & & & \\
	$\bullet$ & & & & \\
	\end{tabular}
	\end{center}
	
	Dans les colonnes, on mets la distance à $A$, et le sommet d'où l'on vient.

	On se place de nouveau au sommet de plus petit poids, ici $E$.
	
	%%%%%%%%%%%%%%%%%%%%% Étape 2.2 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[actif,label={[red]left:A}]      (s)                            {$0$};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {$10$};
	  \node[encours,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]        {$\infty$};
	  \node[infini,label={[red]below right:D}]      (z)        [right of=y]        {$\infty$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[fl_actif] (s) to (t) ;
	\draw[fl_actif] (s) to (y) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}
	

	\begin{center}
	\vspace{7mm}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & \fbox{$5_A$} \\ 
	$\bullet$ & & & & $\bullet$\\ 
	$\bullet$ & & & & $\bullet$\\ 
	$\bullet$ & & & & $\bullet$\\ 
	$\bullet$ & & & & $\bullet$\\ 
	\end{tabular}
	\end{center}

	Et ainsi de suite.

	%%%%%%%%%%%%%%%%%%%%% Étape 3.1 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[fini,label={[red]left:A}]      (s)                            {$0$};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {$8$};
	  \node[actif,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]   {$14$};
	  \node[infini,label={[red]below right:D}]      (z)        [right of=y]   {$7$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[select] (s) to (y) ;
	\draw[fl_actif,bend right=20] (y) to (t) ;
	\draw[fl_actif] (y) to (x) ;
	\draw[fl_actif] (y) to (z) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}

	\begin{center}
	\vspace{7mm}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & \fbox{$5_A$} \\ 
	$\bullet $ & $8_E$ & $14_E$ & $7_E$ & $\bullet $\\ 
	$\bullet $ & & &  & $\bullet $\\ 
	$\bullet $ & & &  & $\bullet $\\ 
	\end{tabular}
	\end{center}


	%%%%%%%%%%%%%%%%%%%%% Étape 3.2 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[fini,label={[red]left:A}]      (s)                            {$0$};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {$8$};
	  \node[actif,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]   {$14$};
	  \node[encours,label={[red]below right:D}]      (z)        [right of=y]   {$7$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[select] (s) to (y) ;
	\draw[fl_actif,bend right=20] (y) to (t) ;
	\draw[fl_actif] (y) to (x) ;
	\draw[fl_actif] (y) to (z) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}
	
	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & $5_A$ \\ 
	$\bullet $ & $8_E$ & $14_E$ & \fbox{$7_E$} & $\bullet $\\ 
	$\bullet $ & & & $\bullet $ & $\bullet $\\ 
	$\bullet $ & & & $\bullet $ & $\bullet $\\ 
	\end{tabular}
	\end{center}
	
	

	%%%%%%%%%%%%%%%%%%%%% Étape 4.1 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[fini,label={[red]left:A}]      (s)                            {$0$};
	  \node[infini,label={[red]above:B}]      (t)        [above right of=s]  {$8$};
	  \node[fini,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]        {$13$};
	  \node[actif,label={[red]below right:D}]      (z)        [right of=y]        {$7$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[select] (s) to (y) ;
	\draw[select,bend right=20] (y) to (t) ;
	\draw[fl_actif,bend right=20] (z) to (x) ;
	\draw[select] (y) to (z) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}

	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & $5_A$ \\ 
	$\bullet $ & $8_E$ & $14_E$ & \fbox{$7_E$} & $\bullet $\\ 
	$\bullet $ & $8_E$ & $13_D$ & $\bullet$ & $\bullet $\\ 
	$\bullet$  &  &  & $\bullet$ & $\bullet$ \\ 
	$\bullet$  &  &  & $\bullet$ & $\bullet$ \\ 
	\end{tabular}
	\end{center}

	%%%%%%%%%%%%%%%%%%%%% Étape 4.2 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[fini,label={[red]left:A}]      (s)                            {$0$};
	  \node[encours,label={[red]above:B}]      (t)        [above right of=s]  {$8$};
	  \node[fini,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[infini,label={[red]above right:C}]      (x)        [right of=t]        {$13$};
	  \node[actif,label={[red]below right:D}]      (z)        [right of=y]        {$7$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[select] (s) to (y) ;
	\draw[select,bend right=20] (y) to (t) ;
	\draw[fl_actif,bend right=20] (z) to (x) ;
	\draw[select] (y) to (z) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}


	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & $5_A$ \\ 
	$\bullet $ & $8_E$ & $14_E$ & $7_E$ & $\bullet $\\ 
	$\bullet $ & \fbox{$8_E$} & $13_D$ & $\bullet$ & $\bullet $\\ 
	$\bullet$  & $\bullet$ &  & $\bullet$ & $\bullet$ \\ 
	$\bullet$  & $\bullet$ &  & $\bullet$ & $\bullet$ \\ 
	\end{tabular}
	\end{center}
	

	%%%%%%%%%%%%%%%%%%%%% Étape 5 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[fini,label={[red]left:A}]      (s)                            {$0$};
	  \node[actif,label={[red]above:B}]      (t)        [above right of=s]  {$8$};
	  \node[fini,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[encours,label={[red]above right:C}]      (x)        [right of=t] {$9$};
	  \node[fini,label={[red]below right:D}]      (z)        [right of=y] {$7$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[select] (s) to (y) ;
	\draw[select,bend right=20] (y) to (t) ;
	\draw[select] (y) to (z) ;
	\draw[fl_actif] (t) to (x) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}
	
	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & $5_A$ \\ 
	$\bullet $ & $8_E$ & $14_E$ & $7_E$ & $\bullet $\\ 
	$\bullet $ & $8_E$ & $13_D$ & $\bullet$ & $\bullet $\\ 
	$\bullet$  & $\bullet$ & \fbox{$9_B$} & $\bullet$ & $\bullet$ \\ 
	$\bullet$  & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\ 
	\end{tabular}
	\end{center}

	%%%%%%%%%%%%%%%%%%%%% Étape 6 %%%%%%%%%%%%%%%%%%%%%
	\begin{center}
	\begin{tikzpicture}[>=stealth,scale=1]
	  \node[fini,label={[red]left:A}]      (s)                            {$0$};
	  \node[fini,label={[red]above:B}]      (t)        [above right of=s]  {$8$};
	  \node[fini,label={[red]below:E}]      (y)        [below right of=s]  {$5$};
	  \node[fini,label={[red]above right:C}]      (x)        [right of=t]        {$9$};
	  \node[fini,label={[red]below right:D}]      (z)        [right of=y]        {$7$};
	% flèches soulignées.
	\begin{pgfonlayer}{background}
	\draw[select] (s) to (y) ;
	\draw[select,bend right=20] (y) to (t) ;
	\draw[select] (y) to (z) ;
	\draw[select] (t) to (x) ;
	\end{pgfonlayer}
	% flèches et numéros
	\draw[pre] (s) to node[auto] {$10$} (t) ;
	\draw[pre] (s) to node[auto,swap] {$5$} (y) ;
	\draw[pre] (t) to node[auto] {$1$} (x) ;
	\draw[pre] (y) to node[auto,near end] {$9$} (x) ;
	\draw[pre] (y) to node[auto,swap] {$2$} (z) ;
	\draw[pre,bend right=20] (y) to node[auto,swap] {$3$} (t) ;
	\draw[pre,bend right=20] (t) to node[auto,swap] {$2$} (y) ;
	\draw[pre,bend right=20] (x) to node[auto,swap] {$4$} (z) ;
	\draw[pre,bend right=20] (z) to node[auto,swap] {$6$} (x) ;
	\draw[pre,bend left=80] (z) to node[auto] {$7$} (s) ;
	\end{tikzpicture}
	\end{center}

	\vspace{7mm}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
	A & B & C & D & E \\ \hline
	$0$ & $\infty$ & $\infty$ & $\infty$ & $\infty$ \\ 
	$\bullet $ & $10_A$ & $\infty$ & $\infty$ & $5_A$ \\ 
	$\bullet $ & $8_E$ & $14_E$ & $7_E$ & $\bullet $\\ 
	$\bullet $ & $8_E$ & $13_D$ & $\bullet$ & $\bullet $\\ 
	$\bullet$  & $\bullet$ & $9_B$ & $\bullet$ & $\bullet$ \\ 
	$\bullet$  & $\bullet$ & $\bullet$ & $\bullet$ & $\bullet$ \\ 
	\end{tabular}
	\end{center}

	Si l'on ne considère que les flèches soulignées, on obtient un {\em arbre}, un 	graphe sans cycle.
	
	\begin{verbatim}
	 1  function Dijkstra(Graph, source):
	 2      for each vertex v in Graph: // Initializations
	 3          dist[v] := infinity     
	        // Unknown distance function from source to v
	 4          previous[v] := undefined
	        // Previous node in optimal path from source
	 5      dist[source] := 0           
	        // Distance from source to source
	 6      Q := the set of all nodes in Graph
	        // All nodes in the graph are unoptimized - thus are in Q
	 7      while Q is not empty:       // The main loop
	 8          u := vertex in Q with smallest dist[]
	 9          if dist[u] = infinity:
	10              break               
	        // all remaining vertices are inaccessible
	11          remove u from Q
	12          for each neighbor v of u:
	        // where v has not yet been removed from Q.
	13              alt := dist[u] + dist_between(u, v) 
	14              if alt < dist[v]:    // Relax (u,v,a)
	15                  dist[v] := alt
	16                  previous[v] := u
	17      return previous[]
	\end{verbatim}
	As we can see above, this Dijkstra simple algorithm has a complexity of order $\mathcal{O}(v^2)$ as two loops depending on $v$ (the number of vertices) are inside each other. Some algorithms are faster using various empirical techniques (adjacency lists or quantum computing).	
	
	\subsection{Google PageRank algorithm}
	At its creation in 1998, Google (search engine currently well know by many Internet users) has dominated in 2011 and still dominates the market for Internet search engines. His initial strength is based on the simple idea to sort the results by relevance, what did not make the other major search engines before him.
	
	The main idea behind the initial Google algorithm is interesting to present in this book because it is a practical example of purely mathematical concepts we have presented so far (graph theory and Markov chains in the respective sections of Graph Theory and Probability) and created one of the largest international companies in new technologies in the years 2000's (Google earns billions thanks to linear algebra ...) and even since the beginning of worldwide economy! As it is often the case in a market economy, it is primarily theoretical research (mathematical or physics) that helped to develop new tools!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	\textbf{R1.} Obviously the basic algorithm has changed since 1998 because you can add many mathematical parameters (weighting by a distance for example) or empirical (visits, clicks, advertising, design, etc.).\\
	
	\textbf{R2.} This algorithm is not only used in the field of Internet. Indeed, it can be used to highlight any object in a multiple relation with other objects (e.g. variables in a project that influence each other by having direct or indirect relations: a structural analysis) or also to determiner influencers in a social network.
	\end{tcolorbox}
	
	To find an information in the amorphous and not really structured content that is the Internet (given the few developers who properly comply with the standards set by the W3C...), the user will search by keywords. This obviously requires for sure some preparation to be effective: the search engine copy first in the local memory all web pages and sorts extracted words (or combination of words) in alphabetical order using traditional empirical algorithms index of the IT domain (b trees, b+ trees or even others...). The result is a directory of keywords with their associated web pages.
	
	To analyze how Google identifies the most relevant web pages, we will ignore the content of pages and only count the links between them. What we get then is the structure of a graph (not necessarily connected!).
	
	The following image shows a miniature example (there would $25\cdot 10^9$ websites from Google in the early 2007):
	\begin{figure}[H]
		\centering
		\includegraphics[]{img/engineering/google_pagerank_schematic_idea.jpg}
		\caption{Schematic principle of the graph to study}
	\end{figure}
	For what follow, we will denote the web pages by:
	
	and we will write:
	
	if the page $P_j$ directly quotes the page $P_i$ (directly related citation or link). Thus, in the previous image, we have a link $1\longrightarrow 5$, for example, but no link $5\longrightarrow 1$.
	
	Obviously, a link $jj\longrightarrow i$ is a recommendation of the page $P_j$ to go read the page $P_i$. Thus it is a vote of $P_j$ in favor of the of the page $P_i$ (we then understand easily then easily why exchanging links between two websites with no contextual relation was somehow banned by Google).
	
	Let us analyze the previous image in a more consistent aspect of graph theory (see corresponding section in this book), which hierarchy will have to be justified to be as following:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/google_pagerank_graph_idea.jpg}
		\caption[]{Preceding figure into more mathematical aspect}
	\end{figure}
	For purely educational reasons, we will introduce three theoretical models of ranking web pages. The latest model is say to be the closest to that implemented by Google in 1998 and the first two models were just here to prepare the basis.
	
	\subsubsection{Weighted Count}
	It is highly possible that if a page is important, it gets a lot of links. With a bit of naivety, we can assume that the reverse is true (that is to say: if a page gets a lot of links, it means it is important). Thus, we could define the importance of $\mu_i$ of a page $P_i$ as the number of directly related links $j\rightarrow i$. In mathematical form, we write it in this way:
	In other words, $\mu_i$ is equal to the number of votes for the page $P_i$, where each vote contributes for the same value $1$. This is certainly easy to calculate, but often do not match with the importance felt by the user.

	Therefore in our above example we have:
	
	that is in front of (in terms of ranking):
	
	what is worse, is that this naive count can be too easily manipulated by adding pages (another Internet domain name for example) without interest recommending any other page.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	So the maximum weight is verbatim equal to $n-1$.
	\end{tcolorbox}
	Some pages emit a lot of (hyper)links. They therefore seem less specific (specialized) and their weight will (must) therefore be lower. We therefore share the vote of the page $P_j$ in $l_j$ equal parts, where $l_j$ denotes the number of emitted links. So we can define a finer measurement:
	
	In other words, $\mu_i$ counts now the number of weighted votes for the page $P_i$. It is easy to calculate, but we always still remain with the above problem.
	
	In our example, we have therefore:
	
	and:
	
	and:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If all the pages were directly related (connex), then we would have equation for all $i$.
	\end{tcolorbox}
	
	\subsubsection{Recursive counting}
	Heuristically, a page $P_1$ seems important if many important pages cite it. This leads us to define the $\mu_i$ recursively as follows:
	
	It is therefore a system of homogeneous linear equations with $n$ unknowns, where in the chosen example $n=12$. More traditionally, the system may be denoted:
	
	or more explicitly (\SeeChapter{see section Linear Algebra}):
	\setcounter{MaxMatrixCols}{20}
	
	Which gives with our example:
	\setcounter{MaxMatrixCols}{20}
	
	In addition to the trivial solution (null vector), this system has an infinity of solutions because its determinant is zero (\SeeChapter{see section Linear Algebra}), which can easily be verified with the \texttt{MDETERM( )} function of the of Microsoft Excel 11.8346.

	Let us now write the previous system in the following equivalent form:
	\setcounter{MaxMatrixCols}{20}
	
	We notice two things before continuing:
	\begin{enumerate}
		\item In the above matrix, each column is such that the sum of its values equals unity. It is therefore the transpose of a stochastic matrix of a Markov chain (\SeeChapter{see section Probabilities}).

		\item The importance $\mu_i$ is positive or zero. So the vector $\vec{\mu}$ is strictly positive and it is the stochastic vector of the Markov chain (\SeeChapter{see section Probabilities}) whose sum of the components must be equal to unity.
	\end{enumerate}
	We will write this system in condensed form:
	
	Thus, $1$ is the eigenvalue and $\mu$ is the eigenvector of the application $P$. We know from our study of Markov chains that this relation is verified only for the invariant measure denoted traditionally $\pi$ in the domain of study of Markov chains:
	
	To determine the measure, it is necessary to first rewrite the network in the format of a matrix application of a Markov chain (\SeeChapter{see section Probabilities}):
	
	With Microsoft Excel 11.8346, the modeling is quite simple enough to reproduce to determine the invariant measure with any starting point (any distribution) in the graph:
	\begin{figure}[H]
		\centering
		\includegraphics{img/computing/google_pagerank_graph_matrix_importance_vector_excel.jpg}
		\caption[]{Determining the Invariant Measure with Microsoft Excel 14.0.7173}
	\end{figure}
	with the following explicit formulas:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/computing/google_pagerank_graph_matrix_importance_vector_excel_explicti_formulas.jpg}
		\caption[]{Explicit formulas for determining the Invariant measure with Microsoft Excel 14.0.7173}
	\end{figure}
	The system converges fairly quickly, after the $30$th step, we already have a convergence to the second decimal:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/google_pagerank_rank_vector.jpg}
		\caption[]{Convergence at the $35$th iteration}
	\end{figure}
	So finally the invariant measure is:
	
	This gives us the convergent distribution per page of a cohort of $17$ Internet users (sums of values of the stochastic vector).

	The pages where $\mu_i$ is large are the most popular in terms of equilibrium probabilities. In the quest to classify web pages, it is still an argument to use the measure $\mu$ as an empirical indicator.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader will be able to found in the R companion book how we plot the above network and calculate the invariant measure in a more efficient way that in Microsoft Excel...
	\end{tcolorbox}
	
	\subsubsection{Absorbing states}
	However, if our graph contains a page (or group of pages) without issue then this one absorbs all the probability, because our cohort will fall sooner or later on this page and will not be able to leave it ("absorbing state" as defined In the Probabilities section). We represent this in the following graphical form if, for example, we add a page $P_{13}$ that is an absorbing page:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/computing/graph_with_absorbing_state.jpg}
		\caption{Graph with an absorbent state}
	\end{figure}
	With the associated transition matrix:
	\setcounter{MaxMatrixCols}{20}
	
	Our model is not yet satisfactory. We could then construct a matrix that allows to escape the absorbent states by putting that any column that contains only one $1$ with only zeros teleport the visitor on all pages with an equal weight. Which would give us since we have $13$ pages:
	\setcounter{MaxMatrixCols}{20}
	
	But this is not very optimal, because it loads the matrix so it is very greedy in terms of memories and calculations.

	To escape the absorbing states, Google uses a more refined empirical model.

	We take the initial application which is a linear mapping:
	
	Where for reminder the sum of the components of $\mu$ is always $1$. We rewrite this in the form:
	
	where the $1$ represents the fact that there is a $100\%$ probability of having the state $P^T\mu$. Now the trick is to write:
	
	with:
	
	which means that the Internet user has at all times a probability $(1-c)$ of being in the state $P^T\mu$ and a probability $c$ of being in the state $\varepsilon$, that is to say to found ourselves with an equiprobable probability in any point of the graph. Finally, we sum up the two to obtain a stochastic vector (whose sum of the components is equal to $1$).

	Indeed, if we take for example $10\%$ for the value of $c$, we have the sum of the components of the vector:
	
	which will be equal to $0.9$ and respectively the sum of the components of the vector $\varepsilon c$ that will be equal to $0.1$ (since the sum of the components of $\varepsilon$ equals $1$ by construction).
	
	So to summarize, with a fixed probability $c$ the indexing engine (or the Internet user) abandons the current page $P_i$ and starts again on one of the $n$ pages of the web, this one being chosen, in order not to privilege anyone! Otherwise, with a probability $1-c$ the crawl engine follows one of the links of the page $P_j$, chosen equally. The only delicate point that remains is to calibrate the value of $c$ which is therefore between $0$ and $1$. If it is $0$ we fall back on the initial model with the problem of the absorbing states, if it is $1$ we have a model that gives an equal note to each page. It is therefore necessary to choose $c$ with low values close to $0$.

	This teleportation tip avoids being trapped by an absorbing page and guarantees to arrive anywhere in the graph, regardless of connectivity issues.

	We must put the whole back in the form of components, so we have the writing (whose second term converges):
	
	which has the enormous advantage of being able to calculate the value of any component $\mu_i$ of the stochastic vector of any step of a random walk on the graph and this without necessitating at any moment the manipulation of an immense matrix that would be too greedy in terms of memory capacity !!

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{20} & \pbox{20cm}{\score{2}{5} \\ {\tiny 7 votes,  42.86\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Industrial Engineering}
	\lettrine[lines=4]{\color{BrickRed}I}ndustrial Engineering involves the design, improvement and installation of systems. It uses the knowledge from the mathematical, physical and social sciences as well as the principles and methods specific to the art of engineering in order to specify, predict and evaluate the results arising from these systems.

We can summarize all areas affecting the industrial engineering (and not only... industrial this can apply with ad hoc adaptation to the administration) with the objective to optimize and monitor the overall performance of the business (costs , deadlines, quality) because:
\begin{center}
\textit{Only what is measured can be improved!}	
\end{center}
Note that some industrial engineering techniques have been discussed in other chapters as: quantitative management techniques, optimization (operations research), financial analysis, queues analysis, etc. and that this section also includes "\NewTerm{Quality Engineering}\index{Quality Engineering}".

In this chapter we will only deal with the minimum theoretical aspects of SQC (Statistical Quality Control) relating to statistical quality control (that's the job of the "quality controller") in the manufacturing environment and into production of goods or services and which is the minimum-minimorum knowledge of any quality engineer in any organization (industrial or administrative) under penalty of having no credibility! Also beware of companies - especially multinationals - who are looking for quality specialists mastering Microsoft Excel or Microsoft Access. Because it means that they use non-professional tools to do a job which should be done with the appropriate tools (and Microsoft Excel or Microsoft Access are not) !!! So in terms of internal organization, you can ensure that these companies organize and analyze anything, anyhow, with an unsuitable tools and therefore that internally the organization is a general mess.

Depending on the use we distinguish three main areas of application of Industrial Engineering that are in the conventional order:
\begin{enumerate}
	\item "\NewTerm{Statistical process control}\index{Statistical process control}", production monitoring and quality settings (Statistical Process Control, SPC). This is the monitoring of a manufacturing process for the mass production of products to discover the differences in quality and to be able to step in and drive directly corrective actions.
	The engineers must absolutely consult the norm ISO/TR 13425:2006 \textit{Guidelines for the selection of statistical methods in standardization and specification} as well as the norm ISO/TR 8258 \textit{Shewhart control cards} and finally the ISO/TR 18532 \textit{Guidelines for the application of statistical methods to quality and to industrial standardization} before implementing SPC tools within their company.
	\item "\NewTerm{Delivery check or receipt sample test (Acceptance Sampling)}\index{Delivery check or receipt sample test (Acceptance Sampling)}". This is an input control, a control during production and a final product inspection in a company (or factory) without direct influence on the production. Thus, the amount of product is measured. The initial inspection is also used to reject the incoming goods. It therefore influences the production only indirectly.
	
The engineers must absolutely consult the family of norms ISO 3591 \textit{Sampling procedures for inspection by variables and attributes} before setting up reception control tools within their company.
	\item Preventive maintenance and control of aging and failure and critical impacts best known as "\NewTerm{Failure Mode Analysis, Effects and Criticality Analysis FMEA"\index{Failure Mode Analysis, Effects and Criticality Analysis}}. This is mainly to calculate the lifespan of components or machines to provide replacements in advance and actions related thereto to be taken to avoid human or financial emergencies.
	
	The engineer must absolutely consult the norm IEC 61649 \textit{Weibull Analysis} and the norm NF EN 13306 \textit{Terminology for maintenance} before putting in place preventive maintenance tools within their company.
\end{enumerate}

These three areas using statistics in general, the engineer should always refer to the family of norms ISO 3534 \textit{Vocabulary and symbols}, ISO 3534-1 \textit{Probability and general statistical term}s, ISO 3534-2 \textit{Statistical Quality Control}, ISO 3534- 3 \textit{Design of experiments}.

For information, since the late 20th century, it is fashionable to combine the first two points in a working methodology named "Six Sigma" that we will study immediately. Finally, note that in practice, to get an interest of chiefs executives the engineer must always find a quantitative relations between non-quality and costs in order to make things change...

\subsection{Six Sigma}

Two objects are never perfectly identical. Whatever the techniques used to manufacture these objects or precise are the tools, there is variability in any production process. The aim of any industrial is that this natural variability remains within acceptable limits. This is a major concern in the improvement of industrial quality.

One of the tools used to work towards this quality is the "\NewTerm{Statistical Process Control S.P.C.}\index{Statistical Process Control}" .If you produce a certain type of object, and if you want to keep your customers to sustain your business, you must ensure that the lots that you deliver them are consistent with what has been agreed between you, usually by contract. Any serious  industrial performs checks on produced lots to check the quality, that he is the producer of the goods or that he receives them. Various statistical techniques related to sampling are then used to avoid, in most cases, to check one by one all the objects in a batch. The samples control  taken in batches is essential if the controls at destroying the artefact, as in an analysis of the active ingredient dose contained in a tablet. However, there are cases where it is preferred to check all objects (for example it is desirable that the brakes of a vehicle works and a control of the braking function on a sample of produced cars does not guarantee that all vehicles have good braking...).

When a batch is controlled, it is conform or it is not. If it is conform, we deliver it (we are the supplier) or we accept it (we are the customer). If it is not conform, we can destroy it, check one by one all the elements and destroy only those who do not comply, etc. All solutions to handle non-conforming lots are expensive. If the batch does not comply, the damage is done. The S.P.C. methods aim to avoid producing non-conforming goods by monitoring production and intervening whenever anomalies are found. A good SPC implementation eliminates a significant number of controls of conformity by setting up statistical tools for monitoring manufacturing processes.

To resume, the S.P.C. is therefore to control the process during manufacturing and to act on the process rather than on the product if problems are detected. This approach tries to go up at the highest possible level of the production line to prevent the occurrence of defective product and goods. We discuss in this particular case of "process control".

This is a useful working methodology to satisfy customers and the idea of this methodology is to deliver products/services of quality, knowing that quality is inversely proportional to the variability. Moreover, the introduction of quality should be optimized so as not to overly increase costs. The subtle interplay between these two parameters (quality/costs) and their joint optimization is often associated with the term "\NewTerm{lean management}\index{lean management}". If we integrate Six Sigma, then we speak of "\NewTerm{Lean Six Sigma}\index{Lean Six Sigma}".

Six Sigma integrates all aspects of the control of variability in business whether at the level of manufacturing, services, organization, marketing or management. Hence why it is such interesting! Moreover, in Six Sigma a defect must be paradoxically welcome, because it is a source of progress of an initially hidden problem. Then you have to ask several times the question "Why?" (traditionally 5 times) to better trace the source of it.

We distinguish two types of variability in practice:
\begin{enumerate}
	\item The "\NewTerm{inherent variability}\index{inherent variability}" in the process (and difficult to change) that induces the concept of measures distribution (usually accepted by businesses as a Normal distribution).
	\item The "\NewTerm{external variability}\index{external variability}" which induces most often a bias (deviation) in the distributions over time.
\end{enumerate}
Manufacturing processes in the high tech industry having a strong tendency to become terribly complex, it should be noted that the basic components used for each product are not always of equal quality or performance. And if in addition, production procedures are difficult to establish, the drifts will inevitably exists.

Whether for one reason or another, ultimately many products will be outside of the normality and will deviate of the range corresponding to the acceptable quality for the customer. This drift is very costly for the company. The management of waste, rework or customer returns for nonconformity generate substantial costs seriously amputating the expected benefits.

As we shall see in the texts, a pretty fair possible definition of Six Sigma is: solving problems based on the exploitation of data. Thus it is a scientific method of management!

\pagebreak
\subsubsection{Quality Control}

In the situation of quality studies in a business, we often give up a 100\% control because of the price it would generate. We then do a sampling. These must obviously be representative, that is to say, representatives and with equal probabilities (i.e. the mixture is good).

The purpose of sampling is obviously the real probability of failure of the whole lot on the basis of observed failures in the sampling.

Let us recall before going further that we have seen in the section of Statistics the hypergeometric distribution (and its interpretation) given by (\SeeChapter{see section Statistics}):
	
where the notation of the binomial coefficient is consistent with that defined and chosen in the section of Probabilities (therefore non-compliant with ISO 31-11).

During a sampling, we normally have a pack of n elements which we draw p of them.  Instead of taking m (remember it is an integer!) as the number of defective parts, we will implicitly defined it as being equal to:
	

where $p_d$ is the probability (assumed known or imposed...) that a part is defective. Thus, we have for probability to find $k$ defective parts in a sample of $p$ among $n$:
	
The cumulative probability of finding $k$ defective parts (between $0$ and $k$ defectives in other words) is then simply calculated with the cumulative hypergeometric distribution:
	

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Example:}\\\\
In a batch of 100 machines, we assume that a maximum of 3 are defective (i.e. that is to say $p_d=3\%$). We conduct a control sampling $p$ at each output order of 20 machines.\\

We want to know at first what is the probability that in this sample of size $p$, 3 machines are defective and secondly what is the maximum number of defective machines allowed in this sample of size $p$ that says us with 90\% confidence that the batch of $n$ machines contain only 3 defective.

	
	Thus, the probability of sampling in one series three defective machines in the sample of $20$ is $0.7\% (0.007)$ and the maximum number of defective machines allowed in this sample of $20$ that allows us with at least $90\%$ of confidence to have $3$ defectives is $1$ defective machine found in the sample (cumulative probability: $0.899$)!

	$H(x)$ values can be calculated easily with the English version of Microsoft Excel 11.8346. For example, the first value is obtained by the function:
	\begin{center}
	\texttt{=HYPGEOM.DIST(0,20,3,100,FALSE)\\=COMBIN(30,0)*COMBIN(97,20-0)/COMBIN(100,20)=0.508}
	\end{center}
	\end{tcolorbox}
	
\subsubsection{Defaults/Errors}

Let us now exhibit for the general culture and a practical and particular example of what is only a simple application of the theory of statistics and probabilities. To understand the importance of quality and the concept of "zero defects", let us consider the following general example of the inventor of the method:

For example, consider that the installation of a tourism car includes $2,500$ operations (pieces) and each operation is perfect $99$ times out of $100$. A priori, a having a successful operation in $99\%$ of cases is a sign of an almost perfect mastery of the quality. But in fact, the perfection of the whole assumes that $2,500$ times, the operations are fully perfectly realized! If the daily production is of $2,000$ units, one the $5$ million operations that are done daily in our factory, there is $100\%-99\%=1\%$ of defects assemblies that is to say $50,000$, and an average of $25$ defects per car, which is hardly acceptable. Also, let us imagine  this imaginary factory is equipped with a control service sampling systematically at the end of the assemblies line. This represents a considerable cost in working hours for control. If the defects can be corrected, this will require rework, replace perhaps some parts and work in unexpected conditions to correct the defects. If the defects are too important, these defects makes the final product unusable, and scrap are extremely expensive depending on the product. Even worse: if the control service sees $99\%$ of defects it remains about $500$  of them daily and this requires costly repairs and returns and a significant deterioration of the image depending on the performance of competitors.

This example serving as a school case study, imagine now a company manufacturing $3$ copies of a same product coming out of the same production line, each copy being composed of $8$ elements.

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
We can just as easily imagine a services company developing (manufacturing) 3 copies of a software (product) out of the same development team (production line), each composed of an equal number of modules (elements).
	\end{tcolorbox}

Suppose that the product $P1$ has $1$ defects in average, the product $P2$ has $0$ defects $P3$ has $2$ defects and that each product is composed of 8 items.

Here Six Sigma implicitly assumes that defects are independent variables, which is relatively rare in machinery production lines but more common in chains in which humans are involved. However, we can consider in the application of the S.P.C. on machines that a sampling in time in the measurement process is equivalent to having a random independent variable!!

	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
\textbf{R1.} As part of the example above with the software, independence is unlikely if we do not take an example in which the programming modules are customized according to customer needs.\\\\
\textbf{R2.} The inconstancy of production results of certain machines whose settings move during operation... (which is common!), and also where raw material quality changes during production (which is also common!) generates major issues in S.P.C. methods and is this inconstancy problem is the heart of Six Sigma.
	\end{tcolorbox}
The arithmetic average of the defects is named in the Six Sigma methodology "\NewTerm{Defects Per Unit (D.P.U.)}\index{defects per Unit}\index{defects per Unit}" and is defined by:
	
And this will give with our small example above:
	
which means that on average each product has a design or manufacturing defect. 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
Warning! This value is not a probability for simple reasons it may initially be higher than 1 and then it has the dimension of [defects]/[products].
	\end{tcolorbox}

Similarly, the analysis can be done with the total number of possible defective elements that make up the product so that we are led naturally to define following the Six Sigma methodology the "\NewTerm{Defects per Unit Opportunity D.P.O.}\index{Defects per Unit Opportunity}":
	
And this will give with our small example:
	
and this can be seen as the probability of a default product element since it is a dimensionless value and the value is always less than or equal $1$:
	
By extension, we can argue that $87.5\%$ of the components of a unit does not have defects and because Six Sigma enjoys working with examples of the order of a million (it's more awesome...) we then define the "\NewTerm{Defects per Million Opportunities D.P.M.O.}\index{Defects per Million Opportunities}" as:
	
And this will give wit our small example:
	
	As $D_i$ is the probability that an element $i$ is not defective is $87.5\%$ (that is to says $12.5\%$ of scrap) then, by the axiom of joint probabilities (\SeeChapter{see section Probabilities}), the probability that product as a whole is not defective if all elements are serial is:
	
	This is not good...

	In Six Sigma, the joint probabilities are also naturally used to calculate the joint probability of non-defective products in a production process connected in series or to calculate the defect probability of an administrative serial workflow. This joint probability is named in Six Sigma the "\NewTerm{Rolled throughput Yield R.T.Y.}\index{Rolled throughput Yield }" and is denoted by:
	
	\pagebreak

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the following serial process/workflow or production line with its given yields (non-default rates):

	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/process_rty.eps}
		\caption{Serial Process/Workflow}
	\end{figure}
	
	We find ourselves in final with a $65.6\%$ reliability that is to say a cumulative probability of default for the entire process/workflow of $34.4\%$ (you can also imagine that these are four tasks in sequence of a project!).
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
The reader that is attentive will have noticed that the serial system is always less reliable than its least reliable component!!
	\end{tcolorbox}
	
This type of calculation is widely used by supply chain manager and the result is named "\NewTerm{availability rate}\index{availability rate}" as well as by project managers for the duration of a phase of a project when they consider durations as independent tasks (for more complex structures, remember that we sometimes talk about "\NewTerm{weighted probability trees}\index{weighted probability trees}" or "\NewTerm{Topological Systems}\index{Topological Systems}").

Thus, in an industrial production line based on the previous example to have a well-defined amount $Q$ of products (supposed to use only one component of each stage) at the end of the production line, it will be necessary at the step $A$ to bring:

	
That is to say $52.42\%$ of components of type $A$ more than expected. At stage $B$:
	

Recall now that the probability density of having $k$ times the event $p$ and $N-k$ times the event $q$ in any arrangement (or order) is given by (\SeeChapter{see section Statistics}):
	
and is nothing less than the binomial law whose expected mean and standard deviation are (\SeeChapter{see section Statistics}):
	
Thus, in the Six Sigma methodology, we would apply the binomial distribution (to make things simple because in facts we should use the Hypergeometric law for small samples) to determine what is the probability of having zero defective items and $8$ others  working well on a product of the production line of our example (if all elements have the same probability to fail for sure...):
	
and we obviously fall back on the value obtained with the joint probabilities:
	
Or the probability to have one defective item and seven working others on a product of the production line:
	
we see that the binomial distribution gives us $39.26\%$ probability to have one defective item on $8$ in a product.	

Moreover, in the section of Statistics, we have shown that when the probability $p$ is very small and tends to zero but however the average value $n\cdot p$ tends to a fixed value when $n$ approaches infinity, the binomial law of average $\mu=np$ with $k$ trials was then given by a Poisson law:
	
with (\SeeChapter{see section Statistics}):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
In a practical context, it is made use of the maximum likelihood estimators of the exponential law to determine the mean and standard deviation above (\SeeChapter{see section Statistics}).
	\end{tcolorbox}
What in Six Sigma methodology is naturally written:
	
with:
	
Thus, in our example, it is interesting to see the value obtained (which will necessarily be different since we are far from having an infinity of individuals and $p$ is far from being small):
	
with:
	
that is a result even worst than with the binomial law for our products.

However, if $p$ is fixed and small, the average $\mu=np$ also tends to infinity in theory with the Binomial Law, moreover the standard deviation $\sigma=\sqrt{npq}$ also tends to infinity.

If we want to calculate the limit of the binomial distribution, we will therefore need to make a change of origin which stabilizes the average at $0$ for example and a change of unit that stabilizes the standard deviation for example to $1$. This calculation has already been done in the Statistics section, we know that the result is the Normal distribution:
	
For our example we know we have to take $k=0,\sigma=0,\mu=0$:
	
Thus, applying the Normal distribution, we have $24.19\%$ chance to sample the first time a defective product. This difference compared to other methods is simply explained by the assumptions (finite number of individuals, significant probability of defect, etc.).

	\subsubsection{Capability Indices}

The Six Sigma methodology (and also the series of norms ISO 22514) defines several indexes that permits to quantify during the manufacturing process the capability of control in the case of a large amount of defects measures distributed as Gaussian-Laplace law (Normal law).

In the case where the measures do not follow a Normal law we have therefore to transform the data using various empirical techniques (Johnson transformations typically).

Basically, if we imagine ourselves working in a company, in charge of the manufacturing quality of a new machine of a new series of pieces, we will be faced with two major situations:
	\begin{enumerate}
		\item At the beginning of production, there may be big quality differences due to defects in the machine or poorly initialization of important settings of the machine. These are defects that will often be rapidly corrected (on the short-term) thanks to a systematic control (destructive or not) of small samples. 
		
		\item Once the big anomalies corrected, we will have in theory only minimal issues that will be very difficult to control and even on the long term. Therefore systematic control is not necessary anymore and once this period of large corrections passed, we make checks per batch (between each correction) and each will be considered an independent and identically distributed random variable (according to a Normal law), but obviously with a different average and standard deviation.
	\end{enumerate}

These two scenarios show that we do not then logically perform the same  tests at the beginning of the production and then on the long term. This is why we define in S.P.C. several indices (whose symbols are unique to this book because they change according the used standards) whose two most important are:
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{short-term process potential capability}\index{short-term process potential capability}" or "\NewTerm{short-term dispersion index}\index{short-term dispersion index}" the ratio of the range of control $E$  of the distribution of values tolerated by the customer and the Six Sigma quality ($6 \sigma$) when the process is almost under control as:
			
		Where USL is the upper control limit or officially the "\NewTerm{Upper Specification Level}\index{Upper Specification Level}" of the distribution and LSL the lower control limit or officially the "\NewTerm{Lower Specification Level}\index{Lower Specification Level}" we often impose (but not always!) in the industry as at equal distances to the desired theoretical mean $\mu$. 
		
		Normally within manufactures, the range of control is fixed (the numerator) and so when the value of the standard deviation is large (more variations, not enough controls) the value of the index is low and when the standard deviation is low (less variation, lot of controls) the value of the index is high as shown by the two examples below:
		
		\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/engineering/capability.eps}
		\caption{Illustration of $C_p$ index}
		\end{figure}
		
		The above capability is therefore an indication that we will seek to maximize (since the standard deviation in the denominator should be minimized!).
		
		This ratio is useful in the industry in the sense that the range $E$ (which is important because it represents the tolerated dispersion/process variation) is considered as the "voice of the customer" (his request) and the 6 sigma denominator the actual behavior of the process/process meant to include virtually all possible outcomes. It is better to hope that this report is at worst equal to unity!
		
		Here is a typical example in project management where, when the customer does not pay for a fine tuning risk modeling (the customer therefore accepts by contract a change in time and costs that can exceed 50\%), we come across with this type of chart:
		
		\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/analysis/six_sigma.eps}
		\caption{Typical plot of a control histogram with tolerances and limits}
		\end{figure}
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
In S.P.C., the range $E$ is sometimes denoted TI, meaning "tolerance interval".
		\end{tcolorbox}
		
		The standard deviation in the denominator is simply defined as the average of the variances in the case of $k$ independent random variables:
			
		where it is useful perhaps to remember that ST is the abbreviation for "Short Term" (abbreviation often unspecified in the practice as assumed known in the context). The standard deviations $\sigma_{\text{ST}}$ is obviously the easiest to apply for the first scenario which we have mentioned above. Because between each big correction, batches are considered independent and can not be analyzed as a single lot (it would be an aberration!).
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		According to the type of industry some use the standard deviation based on the range for $\sigma_{\text{ST}}$ and that is given as we proved it in the section Statistics during our study of extreme values by the ratio of the average of ranges of all measurements made by different operators and divided by the Hartley's constant such that:
		
		\end{tcolorbox}
		
		
		Be careful! As is often the situation in the short term process (during the correction of the big sources of error) tested batches are small, even very small, and thus to reduce costs. Then the standard deviation below the root is for sure not really a correct value...This is why it is important to calculate a confidence interval! So let us see that it is possible to easily construct a hypothesis test for the $C_p$ indicator under the assumption that each sample is identically distributed following a Normal law. Indeed, remembering that we proved in the section on Statistics that:
		
		It is immediate that:
		
		Thus:
		
		Therefore:	
		
		or:	
		
		and we can apply the same reasoning for all types of indicators of the same kind that we will see later!
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
As part of our study later of control charts, we will see that it is possible to use special expressions for the standard deviation when working with samples of measures. These expressions will be based for one on the chi-square law and the other on order statistics.
		\end{tcolorbox}
				\item[D2.] We name "\NewTerm{Overall performance of long-term process}\index{Overall performance of long-term process}" the ratio of the range of control $E$ of the distribution of values tolerated by the customer and the Six Sigma quality ($6 \sigma$) when the process is centered that is to say under statistical control (product manufacturing parameters vary only a little bit) as:
			
			But where the standard deviation in the denominator is given this time by the fact we consider all the big issues corrected and the process as stable such that we can consider all manufactured pieces in the long term as a single controlled batch:
			
			where it is useful perhaps to remember that LT is the abbreviation for "Long Term" (abbreviation often unspecified in the practice as assumed known in the context). This standard deviation is obviously the simplest for the second scenario which we have mentioned above. Because the variations are now, by hypothesis, very small, the entire production can be assumed as a single lot of control over the long term (this does not avoid that sometimes it is necessary to clean the extreme values that may occur).
	\end{enumerate}
Tolerancing the characteristics is therefore important for obtaining the desired quality and reliability of assembled products. Traditionally, a tolerance is expressed in the form of a bi-point $[\text{Min}, \text{Max}]$. A characteristics is then accepted as conform if it is within tolerance.

The problem of the art of tolerancing is to try to conciliate the fixation of the acceptable variability limits to reduce production costs and ensure the highest level of quality in the finished assembled product.

Two approaches attempt to solve this problem:
	\begin{enumerate}
		\item "\NewTerm{Worst Tolerancing}\index{Worst Tolerancing}" ensures the assembly in all situations from the time the basic characteristics are within tolerance. This is the far the best method when the production is very small (and in the extreme case when there is only one unit...).
		\item "\NewTerm{Statistical Tolerancing}\index{Statistical Tolerancing}" takes into account the low probability of extreme assemblies and enables to significantly expand the tolerances to reduce costs and so it is to this that we will look further below. For sure this can be apply only for large production sets.
	\end{enumerate}

A process is said to "\NewTerm{limit capable}\index{limit capable process}" if the ratios $C_p,P_p$ given above (choosing six times the standard deviation) is almost equal to $1$. But in the industry, we actually prefer to take  the value $1.33$  in the case of a Normal distribution that is not perfectly centered (we will see later where this last value comes from).

Obviously, the value of the standard deviation  $\sigma$  can be calculated by using the maximum likelihood estimators with and without bias seen in the Statistics and in practice you must never forget that this is just an estimator and not the real theoretical value! Furthermore, we will see later that depending on the standard deviation used, the notations change!

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
In manufacture, one must be careful because the measuring instrument adds its own standard deviation (error) on these of the production process.
	\end{tcolorbox}

As we have proved it in the section on Statistics, the standard error (standard deviation of the mean) is:
	
	In the Six Sigma methodology, we then often for long term process and under statistical control:
	\begin{equation}
  \addtolength{\fboxsep}{5pt}
   \boxed{
   \begin{gathered}
		\begin{aligned}
			\text{LCL}&=\mu-3\dfrac{\sigma}{\sqrt{n}}\\
			\text{UCL}&=\mu+3\dfrac{\sigma}{\sqrt{n}}
		\end{aligned}
   \end{gathered}
   }
	\end{equation}
when we analyze control charts (see further below) whose random variables is a sample of $n$ independent and identically Normally distributed random variables and that the limits were not imposed by a customer or an internal policy or technical constraints! Obviously, we must be aware that UCL and LCL are not the same expression in more complex cases and therefore for distributions other than the Normal distribution!

Furthermore, the above expression differs for short-term process because the example given above is for a case of measures over the long term for reminder!

As we know $C_p$ requires that the average (target) is centered between USL and LSL. Therefore, the average is confounded with what we name the "\NewTerm{target $T$}\index{process target}" of the process.

But the average $\mu$ in reality can be offset from the original target T which must always (in common use) be equidistant between USL and LSL as shown in the figure below in the particular case a Normal distribution:

		\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/engineering/target.eps}
		\caption{Measures under statistical control offset from the target (typical with index $C_p$)}
		\end{figure}

But this is not necessarily the case in reality where engineers (whatever their application field) can choose asymmetrical a LSL and USL with respect to the average and this only because the distribution is not always following a Normal law (typically the case in project management ...)! Hence the following definition:

\textbf{Definition (\#\mydef):} We name "\NewTerm{short term non-centered potential Capability  short for the process}\index{short term non-centered potential Capability  short for the process}" (in the biased case) or more frequently "\NewTerm{Process Capability Index (Within)}\index{Process Capability Index (Within)}" the relation:
	
	with:
	
	where $k\geq 0$ is named the "\NewTerm{degree of bias}\index{degree of bias}" or "\NewTerm{index position}\index{index position}" and $T$ is as we know the "target" naturally given by:
	
	which gives the middle of the distribution relative to the bi-point $[\text{LSL}, \text{USL}]$ requested by the customer (remember that the standard deviation in the denominator of the prior-previous relation is the standard deviation short term!).
	
	In fact this control capability indicator $C_{pk}$ may seem very artificial, but it is not completely... Indeed, there are some outstanding values (those that interest the engineer) that can help to get a good idea of what happens with it:
	\begin{enumerate}
		\item If the average and the target are confused, then we have:
			
			We find ourselves with $k=0$ and therefore $C_{pk}=C_p$ and judgment criterion of the value of the index will then be based on the short-term capability index.
			\item If because of a poor control of the process we have:
			
			Then the mean $\mu$ is either above USL or below LSL which has the consequence of having $k>1$ and therefore $C_{pk}<0$.	
			\item If we have:
			
			Then the average $\mu$ is between the values USL and LSL which has the consequence of having $0<k<1$ and therefore $0<C_{pk}<C_p$.	
			 \item If we have:
			 
			Then it simply means that the average is aligned with USL or LSL and we therefore have $k=1$ and $C_{pk}=0$.
			As the interpretation remains sometimes delicate and difficult, we build unilateral capability indices "Upper Capability Index CPU" and "Lower Capability Index CPL" given by:
			
			That we will obviously also seek to maximize. Let's see where these two definitions comes from and how to use them:
			\begin{dem}
			First, we need two specific formulations of the degree of bias $k$.
			\begin{enumerate}
				\item If:
				
				then we can get rid of the absolute value:
				
				\item If:
				
				then we can get rid of the absolute value:
				
			\end{enumerate}
	We have therefore when $T>\mu$:
		
		and when when $T<\mu$:
		
		\begin{flushright}
			$\square$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
		In the long term, in some companies (firms), it is interesting to know what are the worst values taken by the CPU indices and CPL (this is the case in the field of production but not necessarily in project management).
		
		The worst values being by construction the smallest one, we often write (with a few different notations that can be found in the literature ...):
		

Below an example of an analysis diagram of the capability generated by the software Minitab 15.1.1 Software with the various aforementioned factors on a sample of 68 measures that can not be rejected as not following a Normal distribution (a normality test was done before):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/engineering/minitab_cp_cpk_cpu_cpl.jpg}
\caption{Serial/Process Workflow}
\end{figure}

Two typical readings of this chart are possible (we will explain the lower left part of the chart later):
	\begin{enumerate}
		\item In manufacturing: The process is capabable (value $>1.33$) but with a (too) strong deviation to the left in relation to the defined target, which is not good (CPL having the smallest value) and must be corrected.
		\item In Project Management: Redundant tasks are under control (value$>1.33$) but with a strong deviation to the left, which can be good if our goal is to get ahead compared to planning (nothing to correct).
	\end{enumerate}
	You really have to take care of the fact that in reality it is not always possible to take the Normal distribution (we recall this because all the examples given above and below are based on this simplifying assumption!).
	
	Always in the context of quality management in manufacturing, the figure below is represents well the reality in the context of a short or long-term process:
	
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/engineering/variation_process_control.jpg}
\caption{Short/Long term process (source: Maurice Pillet ISBN13: 978-2708133495)}
\end{figure}

	Every little Gaussian in light gray, represents a batch analysis assimilated to the concept of "\NewTerm{instantaneous dispersion}\index{instantaneous dispersion}". We see that their average don't keep moving during the measurement period (that this variation is large or very small) and that's what we name the "\NewTerm{global dispersion}\index{global dispersion}". The goal in organizations (industries or administrations) is to ensure that the instantaneous or global variability is limited as much as possible.
	
	But the relation defining $C_p$ assumed, as we mentioned, that the process is under control and also centered (so all Gaussian are aligned) and on a short term.
	
	Similarly, the relation defining $C_{pk}$ assumed, as we mentioned it before, the that the process is under control, on a short-term perspective and non-centered by choice (or because of the fact that the law is not Normal).
	
	If the process is not centered because it is not under control when it should be, the random variable measured could be the sum of the small random behavior of the machine $X$ and uncontrollable small random variations of constraint measurement of the pieces $Y$.
	
	The total standard deviation is then, if the two random variables follow a Normal distribution (and that they are independent!), the square root of the sum of deviations (\SeeChapter{see section Statistics}):
	
	Now if we only have one measure, we get taking the unbiased estimator (it's a little stupid to use it in this case but...):
	
	But in the case of study that interests $Y$ represents the experimental mean (measured) of the process we are trying to bring under control (this is also why we can put $n=1$). This average is traditionally denoted $m$ in the field of industrial engineering.
	
	Then $m_Y$ being not known we take what it should be: that is the target $T$ in the process. Thus, we introduce a new index named "\NewTerm{short term non-centered biased process potential capability}\index{short term non-centered biased process potential capability}":
	
	where once again it must be remembered that the standard deviation in the denominator inside the root of the standard deviation short term of the machine!
	We immediately see that most $C_{pm}$ is close to $C_{p}$ better it is (at least in manufacturing field).
	
	We finally have three common centered and non-centered short term capabilities indices (we deliberately chose to standardize the notations and put as much information inside the relations below):
	
	Similarly we also have three common long-term capabilities indicators centered or non-centered (we deliberately chose to standardize the notations and put as much information inside the relations below):
	
	Finally, it is good to know that although if this is not very relevant, sometimes some engineers do both analyzes (short term + long-term) at the same time on the same measurements.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us indicate the capabilities of an industrial process methods when applied to machines are denoted respectively by $C_m,C_{mk},C_{mm}$. Anyway ... for more information (but without proofs) refer to the norm ISO 22514-2:2013 or the old version ISO 21747:2006.
	\end{tcolorbox}
	
	However, to make an objective analysis on the indexes of capability seen so far, it would first have to check measuring instruments are themselves capable... what we often name the "\NewTerm{R\&R methods}\index{R\&R methods}" (Repeatability, Reproducibility).
	
	The basic principle (a little bit more advanced principle is to make use of the two-factor ANOVA with repetition as studied in details in the section of Statistics) is then to evaluate the short term or respectively  long term dispersion of the measuring instrument to calculate a "\NewTerm{process control capability}\index{process control capability}" defined by:
	
	or according to the type of industry some use the standard deviation based on the range for $\sigma_{\text{instrument}}$ and as we proved it in the section of Statistics during our study of extreme values and therefore named "\NewTerm{Equipment Variation EV}\index{Equipment Variation}" consisting in the calculation of the average of ranges of all measurements made by different operators and divide by the Hartley's constant:
	
	In the classical cases, we declare the control method as capable for an SPC tracking when its capability is greater than $4$ and we will immediately see why. 
	
	Recall for this first that:
	
	But the variance observed is in fact the sum of the "true" variance and this of the instrument such that:
	
	But we have:
	
	Putting it all squared, we deduce:
	
	And now under the strong assumptions that all $E$ are equals we have:
	
	Therefore:
	
	That give us with some rearrangements:
	
	Finally:
	
	This can be translated in graph in the figure below which shows the interest of a $C_{pc}$ of at least equal to 4!
	
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{img/engineering/observed_cp.jpg}
\caption{Relations between $C_{p,\text{observed}}$ and $C_{p,\text{true}}$ for fixed $C_{pc}$}
\end{figure}

	In practice, note that to determine $C_{pc}$ we use a standard prototype measured by laser interferometry and then we ensure that all repeated measurement take place on the same two measuring points.
	
	Once this done, we performs several measures of the standard prototype and we take the standard deviation of these measures. This will give the $\sigma_{\text{instrument}}$.
	
	The range $E$ is imposed by the customer or by internal engineers to the company. It will often be taken as maximum to the tenth of the tolerance unit of the standard prototype.
	
	For example, if we have a piece with an internal diameter of $36\pm 1 [\mu m]$ (tolerance range of $2 [\mu m]$ which is already googd level precision because in our time the standard is closer to $3 [\mu m]$!), our device will then need to have according the rule cited above range of $0.2[\mu m]$...
	
	\pagebreak
	\subsubsection{Quality Levels}
	
	Some engineers as we already mention it like to know how many elements by million units produced (parts per million: PPM) will be considered as defective.
	
	The calculation is then easy since the engineer has at his disposal at least the following information:
	
	and under the strong assumption that the data follow a Normal distribution and that the target is centerd on the mean it is immediate (\SeeChapter{see section Statistics}) in this simple case the for the defective below the LSL we get (for long term or short term):
	
	and:
	
	values very easy to get with any spreadsheet like Microsoft Excel for example.
	We for sure:
	
	If $\mu$ and $T$ are still aligned and that USL and LSL are symetric to this two values we have the special case:
	
Note now an important point relatively to Six Sigma! In fact, objectively, the idea of this method is, of course, to make SPC (among others, but that's nothing really new) but especially to: guarantee to the customer according to tradition with a commonly accepted standard deviation having an upper limit of $\sigma=1$ with an absolute deviation to the mean (in absolute value) of $1.5\sigma$ relative to the target, which guarantees at maximum the famous 3.4 PPM (that is to say 3.4 defectives per million).	

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	This choice seems to come from the empirical practice of Six Sigma by one of its main creator (Bill Smith). It is said that he observed in his business (Motorola) that under statistical control, it was almost always a deviation ranging between $1.2$ and $1.8\sigma$ of the average for almost all its industrial processes.
	\end{tcolorbox}
	Let's see where this value comes from with the following two tables:
	\begin{enumerate}
		\item First we construct an ideal type of table presenting data from a short-term process (but the calculations are identical for a long-term process) centered on the target (target will be zero here, which is a typical case), with a zero mean (i.e. on target and therefore $C_p=C_{pk}$) and unit standard deviation with USL and LSL symmetrical (which against restricted by the scope of application):
	
		where all the values are obtained using the following relations from the potential capability index only. First (remember the assumption that $T=\mu$ and $\sigma=1$:
	
	if the standard deviation is reduced (that can always be done and does not change the accuracy of the results!). And since in the above table LSL and USL are symmetrical with respect to the target:
	
	and PPM are conforming with what we saw just before given by:
	
	and therefore as in the example above LSL and USL are symmetric with respect to the target it simplifies to:
	
where, for example, the value of PPM given at the line "Limit" is obtained with Maple 4.00b using the following command:

	\texttt{>evalf((1-1/sqrt(2*Pi)*int(exp(-x\string^ 2/2),x=-infinity..3.9))*2)*1E6;}
	
	or with Microsoft Excel 11.8346:
\begin{center}
	\texttt{=(1-NORMDIST(3.9,0,1,1))*1E6}
\end{center}
		Let us recall that the "sigma quality level" denoted by $\sigma_q$ is given in fact with the following table we had built in the section Statistics under the assumption of a Normal law (thanks to John Cannin for the \LaTeX{} figure):
		\begin{center}
		\pgfplotsset{compat=1.7}
		\pgfmathdeclarefunction{gauss}{2}{\pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
		}
		\begin{tikzpicture}
		\begin{axis}[no markers, domain=0:10, samples=100,
		axis lines*=left, xlabel=Standard deviations, ylabel=Frequency,,
		height=6cm, width=10cm,
		xtick={-3, -2, -1, 0, 1, 2, 3}, ytick=\empty,
		enlargelimits=false, clip=false, axis on top,
		grid = major]
		\addplot [fill=cyan!20, draw=none, domain=-3:3] {gauss(0,1)} \closedcycle;
		\addplot [fill=orange!20, draw=none, domain=-3:-2] {gauss(0,1)} \closedcycle;
		\addplot [fill=orange!20, draw=none, domain=2:3] {gauss(0,1)} \closedcycle;
		\addplot [fill=blue!20, draw=none, domain=-2:-1] {gauss(0,1)} \closedcycle;
		\addplot [fill=blue!20, draw=none, domain=1:2] {gauss(0,1)} \closedcycle;
		\addplot[] coordinates {(-1,0.4) (1,0.4)};
		\addplot[] coordinates {(-2,0.3) (2,0.3)};
		\addplot[] coordinates {(-3,0.2) (3,0.2)};
		\node[coordinate, pin={68.2\%}] at (axis cs: 0, 0.4){};
		\node[coordinate, pin={95\%}] at (axis cs: 0, 0.3){};
		\node[coordinate, pin={99.7\%}] at (axis cs: 0, 0.2){};
		\node[coordinate, pin={34.1\%}] at (axis cs: -0.5, 0){};
		\node[coordinate, pin={34.1\%}] at (axis cs: 0.5, 0){};
		\node[coordinate, pin={13.6\%}] at (axis cs: 1.5, 0){};
		\node[coordinate, pin={13.6\%}] at (axis cs: -1.5, 0){};
		\node[coordinate, pin={2.1\%}] at (axis cs: 2.5, 0){};
		\node[coordinate, pin={2.1\%}] at (axis cs: -2.5, 0){};
		\end{axis}
		\end{tikzpicture}
		\end{center}
	
	and for which we had given the Maple 4.00b command to obtain the values that are valid for all standard deviations and all means!

	\item Now let us build the table at worst as considered by Six Sigma, that is to say a table for non-centered process (that is to say where $C_p=C_{pk}$ is not satisfied) with a deviation of the average of $+1.5\sigma$ (so on the right but could be taken as shifted on left and the results would be exactly the same!) against the target and unit standard deviation with symmetrical USL and LSL (which still restricts the scope of application):
	
	where all the values are obtained using the following relations from the potential capability index only. First (remember the assumption that $T=\mu$ and $\sigma=1$:
	
	if the standard deviation is reduced (that can always be done and does not change the accuracy of the results!). And since in the above table LSL and USL are symmetrical with respect to the target:
	
	Therefore:
	
	and PPM are conforming with what we saw just before given by:
	
where, for example, the value of PPM given at the line "Limit" is obtained with Maple 4.00b using the following command:

	\texttt{>evalf((1-1/sqrt(2*Pi)*int(exp(-(x-1.5)\string^ 2/2),x=-infinity..(1.3*3))))*1E6}\\
	\texttt{+evalf((1/sqrt(2*Pi)*int(exp(-(x-1.5)\string^ 2/2),x=-infinity..-(3*(1.3+1)))))*1E6;}\\
	
	or with Microsoft Excel 11.8346:
\begin{center}
	\texttt{=(((1-NORMDIST(3*1.3,1.5,1,1))+NORMDIST(-3*(1.3+1),1.5,1,1)))*1E6}
\end{center}
	\end{enumerate}	
	
	We finally understand seeing this famous "limit" row, why a process under control is said to be "\NewTerm{limit capable}\index{limit capable}" with a potential capability index of 1.33 potential given the number of PPM!
	
	So the goal in practice is obviously to be in the situation of the first table with the corresponding values and with in the first table at a quality level of $\sim4.7\sigma$ for the equivalent of 3.4 PPM of the second table (because it is easier to center a process thant to control its variations).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us now make a summary of all this considering a new small production of $n=50$ pieces in packs of $k=n_i=10$ (in order to adjust the machines during production). Measuring the specifications of 5 pieces per hour during 10 hours with a tolerance of $10\pm0.07$ that is to say in terms of a range of hundredths:
	
	and a target of $T=0$. We have the data:
	
	We immediately see that the manufacturing process was not stationary during this first production, we will therefore need to make corrections in the future:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/application_non_stationnary_process.jpg}
		\caption[]{Visual evidence by a plot that the process seems not stationary}
	\end{figure}
	or as a very simple control average-standard deviation chart with $3\sigma$ (as I like do it for beginners):
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/application_non_stationnary_process_control_chart.jpg}
		\caption[]{Visual evidence by a plot that the process seems not stationary}
	\end{figure}
	So we easily guess that the process is limit capable...
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	An interesting thing to know is that this type of chart can be analyzed using the mathematical tools of the time series analysis as we will see later.
	\end{tcolorbox}
	First, if we want to make a relevant statistical study of the different data above we can calculate the average of deviations that under the assumption of a Normal distribution is the arithmetic mean (\SeeChapter{see section Statistics}):
	
	Then the standard deviation of all pieces taken as a single group is using the maximum likelihood estimator of the variance of the Normal distribution (long term standard deviation) and taking the some notation for this variance as the one used in the section of Statistics:
	
	So the standard error  (the estimator of the standard error of the mean)  is as proved in the section Statistics:
	
	So the confidence interval to 95\% of the average is (\SeeChapter{see section Statistics}):
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	In our case this gives:
	
	And the statistical inference with our long-term standard deviation using the chi-square $\chi^2$ bilateral hypothesis test gives (\SeeChapter{see section Statistics}):
	
	What gives us in our case:
	
	Therefore:
	
	We notice therefore that on a long-term analysis we have the intervals:
	
	The variations can therefore be huge with a cumulative probability of 95\% and we must then take care in practical cases to make adjustments as quickly as possible to reduce as possible the values of those moments!\\
	
	Let us now calculate the potential long-term process capability (if assumed centered!). We have:
	
	But with an instrument having $P_{pc}$ of $4$, it really corresponds to:
	
	Furthermore, let us indicate that if we can do a a calculation of the confidence interval for $\sigma_{\text{LT}}$ (see calculations done previously), it is then easy to have one too for $P_p$!\\
	If the analysis of the overall performance of the long-term process is not centered (which is the case here)  we use:
		
	and we know once again that because of the instrument, this value is somewhat undervalued!
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We have of course:
	
	So the process is not centered (as we suspected...). Then, we must calculate the potential long-term average non-centered capability $P_{pm}$ of the process using the relations defined above:
	
	To resume, either the value of $P_{pk}$, $P_p$ or $P_{pm}$, we see that all the values are limit capable (that is to say that the value is greater than $1$ - see definition above for a reminder of what means "limit capable").\\
	
	If we do then our calculations of PPM according to the relations obtained above with the value of $S_{\text{LT}}$ and $\bar{X}$ previously obtained then we have:
	
	Then say this number  is good or bad is difficult because we lack the information to know what is the cost of production, cost of repair, the cost of a product and the whole is itself dependent of the total amount produced! But we can also use the model of Taguchi (see further below) to know the value of the parameters (moments) calculated that it would be preferable not to exceed!\\
	
	Let us now calculate the short term capability indices! For this, we need the estimator of the average of the whole considering each individual as a random variable. We know (\SeeChapter{see section Statistics}) that this average is the arithmetic mean in the case of a Normal distribution and is strictly equal to that which is calculated by considering the set of individuals as a single random variable. So it follows that:
	
	Regarding the standard deviation this is not the same! But we know (\SeeChapter{see section Statistics}) that the Normal law that is stable by the sum. For example, we have proved that given two independent random variables distributed according to a Normal distribution (by imagining that each variable represents two of our ten samples), we hat for their sum:
	
	But we have also proved in the section of Statistics using the property of linearity of expected mean, that we have:
		
	\end{tcolorbox}

	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	and for the variance:
		
	Therefore:
	
	and in our particular case:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A software like Minitab used the pooled standard deviation (\SeeChapter{see section Statistics}) that for recall is given by:
	
	But it's still not accurate enough. As we will prove it further below during our study of control charts, by default (but there is an option to deactivate this) Minitab use unbiasing constants such that finally:
	
	\end{tcolorbox}
	So the standard error (the estimator of the standard error of the mean) is:
	
	So the confidence interval to 95\% of the average is (\SeeChapter{see section Statistics}):
	
	In our case this gives:
	
	We note that in the short term, the range is much wider in the long term, which is normal given the low value of $k$ (which is only $5$ in our example).
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	And the statistical inference with our short-term standard deviation using the chi-square $\chi^2$ bilateral hypothesis test gives (\SeeChapter{see section Statistics}):
	
	What gives us in our case:
	
	Therefore:
	
	We notice therefore that on a short-term analysis we have the intervals:
	 
	The variations can therefore be huge with a cumulative probability of 95\% and we must then take care in practical cases to make adjustments as quickly as possible to reduce as possible the values of those moments!\\
		
	Let us now calculate the potential long-term process capability (if assumed centered!).
	We have:
	
	So we have logically (the opposite would be problematic):
	
	But with an instrument having $C_{pc}$ of $4$, it really corresponds to:
	
	Furthermore, let us indicate that if we can do a calculation of the confidence interval for $\sigma_{\text{ST}}$ (see calculations done previously), it is then easy to have one for $C_p$ too!\\
	
	If the analysis of the overall performance of the long-term process is not centered (which is the case here)  we use:
		
	and we know once again that because of the instrument, this value is somewhat undervalued! We have of course:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	So the process is not centered (as we suspected...). Then, we must calculate the potential long-term average non-centered capability $C_{pm}$ of the process using the relations defined above:
	
	To resume, either the value of $C_{pk}$, $C_p$ or $C_{pm}$, we see that all the values are limit capable (that is to say that the value is greater than $1$ - see definition above for a reminder of what means "limit capable").\\
	
	If we do then our calculations of PPM according to the relations obtained above with the value of $S_{\text{ST}}$ and $\bar{X}$ previously obtained then we have:
	
	With same remarks as for $\text{PPM}_{\text{tot}}^{\text{LT}}$.\\
	
	To close this long detailed example, here is the output of a program like Minitab 17.3.1 in which we find all the calculations above plus the control charts which we will further detailed the proofs (see the section of Statistics for the details about AD Anderson-Darling test):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/six_pack_analysis.jpg}
		\caption[]{Minitab 17.3.1 Six Pack Capability Analysis}
	\end{figure}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	and with a screenshot highlighting the fact that they are multiple ways to calculate $\sigma_{\text{ST}}$ with or without unbiasing constants:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/capability_short_term_standard_deviation_options.jpg}
	\end{figure}
	\end{tcolorbox}
	
	\pagebreak
	\subsection{Taguchi Model}
	As part of the SPC (Statistical Process Control), it is interesting for a manufacturer to estimate the financial losses generated by the differences in the target (caution! can also apply this approach in other areas that the industry!).
	
	We can have a relatively simple and satisfactory estimate of its (financial) losses under the following assumptions/hypothesis:
	\begin{enumerate}
		\item[H1.] The process is under control (constant standard deviation) and follows a symmetric decreasing left and right mass function relative to the target (which can be a measurement, a number of errors per periods, etc.).
		\item[H2.] The cost of error is zero when production (or work) is centered on the target (minimum).
		
		 \item[H3.] The cost increases in the same way as production off center on the left and on the right (which is not the case in the field of administration for example). The cost function therefore pass, following assumptions/hypothesis H2 and H3, through a minimum on the target.
	\end{enumerate}
	Therefore, if we note  $Y$ the off-centering with respect to the target $T$ and $L$ financial loss. We have:
	
	Although we do not know the form of this function, we can write it as a Taylor expansion (\SeeChapter{see section Sequence and Series}) around $T$ as:
	
	If we develop to the third order:
		
	But by the assumption/hypothesis H2, we have $L (T)$ is zero. It then remains:
	
	and as by H3, the derivative of the function $L (Y)$ is zero in $T$ as it is a minimum then:
	
	That is traditionally noted in SPC:
	
	and is named "\NewTerm{Taguchi (centered) loss function}\index{Taguchi (centered) loss function}" or just "\NewTerm{quality loss function (centered)}\index{quality loss function (centered)}".
	
	Well it's nice to have this relation, but how should we use it?
	
	In fact, it's relatively simple. Under the assumptions mentioned above, if we have production defective measurement (ribs, delays, breakdowns, bug, etc.) then just calculate their arithmetical mean $\mu$ (estimator of the expected mean of a Normal distribution) and then to know the financial or hourly cost $L$ that the defect generates for the company, institution or manufacture (sometimes this average is calculated on the basis of a single sample...).
	
	Therefore, the above relation becomes:
	
	with known $L$ and $\mu$.
	
	And since $T$ is given by the customer or depending on the context it is therefore easy to get the $k$ factor:
	
	which is the fact mathematically the inflection point of the mathematical function $L$ (\SeeChapter{see section Differential And Integral Calculus}).
	
	The latter relationship is sometimes written:
	
	
	Once we have $k$ with a good estimation, it is possible to know $L$ for any value of $Y$ and so we can calculate in production the cost of any deviation from the target.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider a power supply for a stereo for which voltage $T$ is $110 [V]$. If the voltage is out of the range $110\pm 20 [V]$ then the stereo fails and must be repaired. Suppose the repair cost (including all direct and indirect costs!) is of $100.-$. Then, the associated cost for a given value of the voltage is:
	
	\end{tcolorbox}
	
	Let us now see an elegant way to calculate the average Taguchi cost (average unit loss). We obviously have for a production line on several pieces of the same family:
	
	where the $X_i$ are supposed to be normal random variables (Gaussian by  assumption because the manufacturing process is under statistical control). But we have proved in the section of Statistics in our study of the confidence interval for the variance with known empirical mean that:
	
	Therefore:
	
	And finally:
	
	This last expression has the advantage of showing very clearly that to minimize loss, we must act on the dispersion and the adjustment of the average on the nominal value.
	
	Now remember that we have proved in the section Statistics during our study of likelihood estimator that (it is important in these developments that we use the notation which distinguish different estimators!):
	
	Therefore
	
	If $n$ is large, then we have for a lot of products:
	
	where the first term in brackets represents the variance of $Y$ around its own mean and the second term of the deviation of $Y$ with respect to the target $T$.
	
	We also find this last expression in the literature and in softwares often denoted as follows:
	
	where the index $N$ means "Nominal". Obviously, when the target $T$ (or nominal value) is taken as zero, the relations simplify even more.
	
	Now, remember the following properties of the expected mean and variance (\SeeChapter{see section Statistics}):
	
	Therefore, if the process is non-centered and that we must correct the random variable to recenter it, it comes intuitively that the correction factor will then be (if linear):
	
	Therefore, it comes immediately:
	
	So to minimize $L$, knowing that the other terms are imposed, it is necessary to minimize the ratio:
	
	or, equivalently, maximize the inverse ratio (which remains without dimensions):
	
	This number is often very large, it is customary in the literature and in statistical software to take the base-10 logarithm and multiply the result by 10 (it is an idea inspired by the acoustic physics). We then have what we name the "\NewTerm{signal/noise ratio}\index{signal/noise ratio}", given in decibels:
	
	for what engineers call "\NewTerm{nominal is the best}\index{nominal is the best}".
	
	Relation that will meet in a software like Minitab during the analysis of Taguchi design of experiment (see below what are experimental designs). Some very rare software also offer the following special case:
	
	For what follow let us return to the following relation:
	
	Engineers talk about minimizing the quality loss function when $T$ (the target) must be zero and therefore it is necessary to minimize all $y_i$. So the above relation is finally reduced to:
	
	What engineers sometimes like to summarize as follows (whereas I personally think that taking the logarithm in base $10$ as well as putting a "-" in this case is really not justified...):
	
	and even sometimes they say it is still a signal/noise ratio... while obviously there is no noise in this specific case that is named "\NewTerm{smaller is better SIB}\index{smaller is better }". This is exactly in this form in which you can find this target in the software Minitab.
	
	In the context of a parameter to maximize it is customary to say that we seeks to minimize:
	
	because given that $T$ tends to infinity it would be difficult to do anything mathematically speaking.
	
	So because $T$ tends to infinity, the preceding relation is reduced to:
	
	What engineers also likes sometimes summarized as follows (here the base-10 logarithm is justified in my point of view!):
	
	and even sometimes they say also it is still a signal/noise ratio... while obviously there is no noise in this specific case that is named "\NewTerm{larger is better}\index{larger is better}". This is exactly  under this form in which you can find this target written in a software like Minitab.
	
	It is, however, being clever, possible to introduce noise in the relation:
	
	For this let us write:
	
	We put:
	
	So in this case, using the Taylor series as proved in the section of Sequences And Series:
	
	Then we have:
	
	and under statistical control we have:
	
	and assuming that we work on all parts of the population, we have:
	
	It comes then:
	
	So in the end by replacing a little bit abusively by the estimators, we get:
	
	
	\pagebreak
	\subsection{Preventive Maintenance}
	The evolution of production techniques towards greater automation of complex technical systems has increased the importance of the reliability of production machines or servers. Also, an unexpected shutdown is costly or even deadly to a business. Similarly, in the aerospace industry, problems of reliability, maintainability, availability are crucial. Maintenance has the purpose to quantify the reliability for all the components (mechanical, electromechanical, IT, etc.).
	
	The existence of a maintenance service has for purpose to maintain the equipment (systems) and also the reduction in breakdowns. Indeed, these are expensive, they generate:
	
	\begin{enumerate}
		\item Intervention, repair costs		
		\item Cost of product non-quality		
		\item Indirect costs such as fixed costs, production losses, lost profit...
	\end{enumerate}
	Therefore, every effort should be made to avoid the blackout/failure, act quickly when it occurs to increase equipment availability. To do this, one must model the life of equipment and train people to react. The set of methods and techniques for its problems are usually classified under the name of "\NewTerm{Failure Mode and Effects Analysis FMEA}\index{Failure Mode and Effects Analysis}".
	
	"\NewTerm{Quantitative Reliability Engineering}\index{Quantitative Reliability Engineering}" analysis involves more than just reliability predictions and reliability demonstration that are performed against a given program or project requirements. • Quantitative Reliability Engineering analysis can play a key role in supporting a broad range of applications. It is critical
in addressing design and manufacturing deficiencies.
	
	We distinguish two main classes of systems:
	\begin{enumerate}		
		\item Non-repairable systems (satellites, although low cost goods, etc.)
		\item Repairable systems (production machinery, means of transport, etc.)
	\end{enumerate}
	where the theoretical approaches are different. For the second category, you can also use Markov chains (where the states represent the number of functional components or failure of a system in accordance with the standard ISO 31010), Petri nets or Monte-Carlo simulation.
		
	The idea is in the texts that follow to make the point on one of these methods, to seek their efficiency and enable engineers practitioners and technicians to better understand these problems. An emphasis will be placed on the Weibull model, which has a significant importance in this field.
		
	\pagebreak
	\subsubsection{Planned Obsolescence}
	The planned obsolescence goes against preventive maintenance philosophy whose primary purpose is to provide a quality product and respectful of its environment with precise knowledge of the product's lifetime and quantifiable risk of obsolescence.
		
	At the opposite, the purpose of planned obsolescence is to reduce the lifetime (or "lifespan") of a product in order to increase the rate of replacement by the buyer (sometimes motivated by political strategy or short term  money return...). Therefore the induced demand would have for aim (not proved in the very long term ... as this is quite bad for the reputation of the brand) to benefit the producer, or its competitors. The sector then has a greater production, stimulating productivity growth (economies of scale) and technical progress (which accelerates the obsolescence of previous products) and maintains the use of an ever growing population which is therefore problematic by definition.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/capacitor_life_expectancy.jpg}
		\caption[]{Example of a reliability plot of capacitor (source: Cash Investigation about Samsung TV issue) }
	\end{figure}
	The reader will therefore have to understand that I am strongly against these methods and that every engineer and every scientist working for a company must refuse by deontology this type of request coming from the hierarchy even denounce these methods anonymously on the Internet or to consumer associations.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The consumer must be very careful with TV shows or newspapers about the subject of planned obsolescence! Indeed as they don't have any statisticians consultants, these shows or newspapers communicate numbers in a very wrong and bad way (a journalist being not a scientist...).
	\end{tcolorbox}
		
	It must be also notice that lifetime (lifespan) will be a major subject of concern for future generations as Earth's resources are not unlimited and that in this 21st century people still don't recycle everything. So it will be a challenge for these future generations (at least if we will not be able to bring material from other planets, satellites or asteroids) to found solutions to get the longest lifespan. Having the longest lifespan is however also in the 21st century sometimes a requirement as it is an important factor in aeronautics, trains, boats or all other areas of cutting edge technologies and where human life are in concerns (or very rarely in luxury goods also).

	But let us also show a small chart of the status of the known reserves - in years - of row material resources (so not including unknows reserves!) if humans continue to consume as they do in the beginning of this 21st century (sorry did not find the french version and also the original paper of this study...):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.38]{img/engineering/raw_materials_reserves.jpg}
	\end{figure}
		
	\pagebreak
	\subsubsection{Reliability Empirical Estimators}
	As part of the non-accelerated reliability study (accelerated aging will be treated in a near future), we are led to define some variables which are listed below:
	\begin{itemize}
		\item $N_0$ will the number of good elements at time $t_0$ (initial time)
		\item $N_i$ the number of good elements at time $t_i$
		\item $n_i$ the number of defective elements between $t_i$ and $t_{i-1}$ also denoted  by $\Delta N_i$
		\item $\Delta t_i$ the time interval between $t_i$ and $t_{i-1}$
	\end{itemize}
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] We define the "\NewTerm{failure rate per time-slot}\index{failure rate per time-slot}" $\Delta t$ by the relation:
		
		which therefore is interpreted as being the number of defective elements relative to the number of surviving elements on a given timeslot (this is therefore a percentage of non-conformities related to a time slot).
		
		The latter relation is also sometimes named "\NewTerm{hazard function}\index{hazard function}" (or "\NewTerm{hazard ratio HR}\index{hazard ratio}") or "\NewTerm{relative survival}\index{relative survival}" and often denoted by $\bar{h}(t_i)$.
		
		\item[D2.] We define the "\NewTerm{failure function}\index{failure function}" by the relation (probability density of failure at time $t_i$):
		
		noting that the denominator is not the same as that which defines the default rate per timeslot!
		
		This function is therefore interpreted as the \% of defective elements in the studied  time frame relative to the total number of elements initially tested. This is the indicator that interests most often the engineer because can be assimilate to a probability and having the properties of a probability!
		
		\item We naturally define the "\NewTerm{cumulative failure function}\index{cumulative failure function}" by:
		
		which tends to $1$ when time tends to infinity.
		
		This function is therefore interpreted as the \% of cumulative defective components based on the total number of elements initially tested. It is therefore the function of empirical distribution of probabilities of failures.
		
		\item[D4.] We define verbatim the "\NewTerm{reliability function}\index{reliability function}" by (this is the second term of the previous relation):
		
		Its name comes from the interpretation of the ratio in the context of the definition of the cumulative failure function.
		
		It must be remembered that the letter $R$ is derived from the word "\NewTerm{reliability}\index{reliability}" and $F$ is derived from the word "\NewTerm{failure}\index{failure}" which means "breakdown".
	\end{enumerate}
	It follows from these definitions that:
	
	The latter relation is used for the calculation of laws reliability as we will see later! We deduce a notation which sometimes helps to better understand the failure rate per time slot. Effectively:
	
	Because:
	
	the failure function can be seen as a probability as we have already mentioned it, which naturally led us to define its expected mean:
	
	Very useful relation in practice which in theory gives the average percentage of failed elements at the moment $t_i$.
	\pagebreak
	So to summarize and with different notations that we can find in the literature, this gives (we will see later the same summary in the continuous case):
	\setlength\extrarowheight{12pt}
	
	\setlength\extrarowheight{0pt}
	Note that we will see later below a statistical mathematical model based on the Kaplan-Meier estimator that gives in fact a non-parametric estimation of the survival function. The following example will perhaps help you better understand the concept!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have identified on a batch of 37 engines of a given type the following listed failures in slices (given by customers or internally measured on test benches):
	
	We must consider the value of the reliability function $\hat{R}(t_i)$, of the failure function $\hat{f}(t_i)$ and the failure by time slice $\hat{\lambda}(t_i)$. The calculations are elementary and we get the following table:
	
	We see above  that the failure rate $\hat{\lambda}(t_i)$ is of course not constant! The failure rate $\hat{f}(t_i)$ would therefore be about $2.5\%$ to $3\%$ the first year, about $10\%$ to $11\%$ the second year,etc.
	\end{tcolorbox}
	Regarding default rates, engineers often recognize three analysis slices following that the studied objects are: 1) new, 2) in normal operation or 3) considered aging.
	
	In practice we considered quite intuitively (and sometimes roughly) that for very common goods the failure rate follows a bathtub curve as shown below (in the technical tables it is often the normal operation failure rate that is given):
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/reliability_bathtube.jpg}
		\caption[]{Reliability bathtube}
	\end{figure}
	whereas if you look at the table of the example above, the failure rate does not follow at all a bathtub curve (so it is a cons-example).
	
	Reliability engineers often cut the bath in the three visible parts above, but under the following technical names:
	\begin{enumerate}
		\item D.F.R. for "\NewTerm{Decreasing Failure Rate}\index{Decreasing Failure Rate}": young unidentified components with manufacturing problems during the process are eliminated from the batch which has the effect of reducing the failure rate. The Weibull distribution is relatively well suited to model this phase (see later below the details of this model).
		
		\item C.F.R. for "\NewTerm{Constant Failure Rate}\index{Constant Failure Rate}": the components are in stable conditions.
		
		\item I.F.R. for "\NewTerm{Increasing Failure Rate}\index{Increasing Failure Rate}" components are in end of life and their failure rate increases. The Weibull distribution is again relatively well adapted to model this phase.
	\end{enumerate}
	
	The Poisson distribution (\SeeChapter{see section Statistics}) is relatively well adapted to model the failure arrival time, and the exponential distribution (\SeeChapter{see section Statistics}) to model the time between successive failures. This fact follows mathematical proofs available in the section of Quantitative Management (queues theory). So, this means that failures are often considered independent. Therefore, the law on the number of failures in a given period is then a law "without memory" and can be modeled by a Poisson distribution (proof also made in the section of Quantitative Management).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Contrary to what a lot of theorists think... many general public softwares have also sometimes a failure rate following a bathtub curve. Indeed, at the beginning there undetected bugs that cause the failure will decrease following the identification and correction. Then, because of frequent updates of the environment that tend to add other issues (service pack), the failure rate remains fairly constant. Finally, over time, the evolution of surrounding technologies (frameworks) make the application obsolete and some functions no longer properly work which is again an increase the failure rate.
	\end{tcolorbox}
	
	Vis-à-vis the effectiveness of renovation, let us indicate (by simplifying) that they can be frequently divided into three categories:
	
	\begin{enumerate}
		\item "\NewTerm{As good as new}\index{as good as new}": This is preventive maintenance in the sense that we change a piece when his life times leads it to a failure rate that we consider too high and that the unanticipated failure will cost more that its non-anticipation.
		
		\item "\NewTerm{As bad as old}\index{as bad as old}": It is the poor maintenance in the sense that we change a piece only when it comes ot break, which mainly generates higher costs of maintenance that the preventative maintenance that involves in anticipating issues.
		
		\item "\NewTerm{Partial Restore}\index{partial restore}": This is the minimum preventive maintenance in the sense that we repair the defective part rather than replace it with a new one. Again the cost of the problem must be calculated by an audit of requirements and of the deadlines of the business.
	\end{enumerate}
	
	Let us return to other definitions in the passage to the limit of the continuum!
	
	So we know that the "\NewTerm{instantaneous failure rate}\index{instantaneous failure rate}" will have for unit the inverse of time as $[\lambda]=s^{-1}$. This rate is, within the framework of our study, not necessarily constant over time as we have seen above!
	
	Given $R(t)$ the cumulative percentage of objects analyzed (tracked) always good working of a sample tested at time $t$. The number of objects falling down during the infinitesimal time $\mathrm{d}t$ is therefore equal to:
	
	this corresponds to the decrease of the initial stock in functioning at time $t$.
	
	We can then write the relation:
	
	therefore:
	
	Which can often be found in the literature under the following equivalent forms:
	

	\textbf{Definition (\#\mydef):} The "\NewTerm{conditional probability of failure}\index{conditional probability of failure}" between $t$ and $t + \mathrm{d}t$ is defined as the temporal conditional probability of experiencing the event at a given time $t$, knowing that we have not experienced before and so we survived up to a time $t$ (i.e. it only increase with time...):
	
	where $F(t)$ and $R(t)$ are, for reminder, respectively the cumulative probability fonction of failure (cumulative probability to fail at time $t$) and the cumulative probability function of reliability also named "\NewTerm{survival function}\index{survival function}". $R (t)$ is $1$ at time $0$ and ... $0$ after an infinite time as we have already seen before!
	
	We will come back on this conditional probability further below with a more pedagogical approach during our study of Cox's proportional hazard model.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	By the same intellectual approach, rather than defining a function of failure $F(t)$ and survival $R(t)$ with its associated hazard function, we can define a repairability function with its function $M(t)$ which would then be a "\NewTerm{maintainability function}\index{maintainability function}".
	\end{tcolorbox}
	If we integrate (caution! $u$ now represent the time!):
	
	As $F(t_0=0)=0$ we have:
	
	Therefore:
	\begin{equation}
  \addtolength{\fboxsep}{5pt}
   \boxed{
   \begin{gathered}
   		\begin{aligned}
		R(t)&=e^{-\int\limits_0^t\lambda(u)\mathrm{d}u}\\
		F(t)&= 1 - e^{-\int\limits_0^t\lambda(u)\mathrm{d}u}
   		\end{aligned}
   \end{gathered}
   }
	\end{equation}
		Moreover, since we have seen that $\hat{f}(t_i)=\hat{\lambda}(t_i)\hat{R}(t_i)$, then we have the "\NewTerm{density/repartition function of instantaneous failure}\index{density/repartition function of instantaneous failure}":
	
	We can get this relation and interpretation in another way:
	
	So where we find back therefore $F (t)$ the function of cumulative probability of failure. Obviously to determine the law $f (t)$, we use the usual statistical tools for test adjustments (\SeeChapter{see section Statistics}).
	
	We then get the very important equation (see the most important one!) in practice, which connects the density function of the instantaneous failure and the reliability's law:
	
	We have above the three most general expressions binding laws of reliability and the instantaneous failure rates. Let us add that we have also the following instantaneous hazard function that follows:
	
	It then follows immediately that:
	
	Since $f (t)$ is the failure density function, the expected value of the failure is given by:
	
	Thus, if the distribution of failures is equally likely (uniform density function), which is rare..., half of the equipment will be off at $E(t)$.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Observing $100,000$ hard drives, Google engineers have observed an average of $8\%$ loss per year! So a higher loss rate than would have announced that manufacturers that was about $300,000$ hours! The loss rate is higher the first $3$ years! But perhaps the disks that survive live longer!?
	\end{tcolorbox}
	We also have use the integration by parts and the above relation:
	
	Hence another way to express it:
	
	So to summarize and with different notations that we can find in the literature, this gives:
	\setlength\extrarowheight{12pt}
	
	\setlength\extrarowheight{0pt}
	Let us notice again (!) that we will see further statistical mathematical model based on the Kaplan-Meier estimator giving a non-parametric estimation of the survival function!
	
	Before continuing let us give some other definitions of maintenance indicators, the most important are the European Maintenance Standard EN 13306:2010 (I precise it because on the web we can find the best and the worst on the definitions of these reliability indicators...):
	
	\begin{itemize}
		\item The "\NewTerm{Time Between Maintenance T.B.M.}\index{Time Between Maintenance}" is the contractual time between two visits or maintenance control (not defined in the EN 13306:2010 standard).
		
		\item The "\NewTerm{Mean Operating Time Between Failures M.O.T.B.F.}\index{Mean Operating Time Between Failures }" applicable only for repairable items is the expected mean of the operational time of a system between the end of the first failure and the beginning of the next (defined in the EN 13306:2010 standard). In the industry M.O.T.B.F. is also named "\NewTerm{Mean Up Time M.U.T.}\index{Mean Up Time}".
		
		\item The "\NewTerm{Mean Time Between Failures M.T.B.F.}\index{Mean Time Between Failures}" applicable only for repairable items is the expected mean of operational time of a system between the start of two failures (defined in the standard EN 13306:2010). This indicator includes the time of non-operation. In the area of services, the M.T.B.F. is sometimes named "\NewTerm{Mean Time Between System Accidents M.T.B.A.}\index{Mean Time Between System Accidents}". In the field of the production lines MTBF is named "\NewTerm{Mean Time Between Defects M.T.B.D.}\index{Mean Time Between Defects}" (we do not stop the production because only one defect was detected as we will see during our study of control charts further below). In the field of computing science the M.T.B.F. is named "\NewTerm{Mean Time Between Errors M.T.B.E.}\index{Mean Time Between Errors}" (we also rarely stops a software or a website because one bug or error was detected!).
		
		\item The "\NewTerm{Meantime To Failure M.T.T.F.}\index{Meantime To Failure}" applicable for repairable or not repairable items is the expected mean of operational time of a system to its first failure (not defined in the standard EN 13306:2010). In fact it is the latter which is calculated most often in tests laboratories.
		
		\item The "\NewTerm{Mean Repair Time M.R.T.}\index{Mean Repair Time}" applicable only for repairable items is the expected mean of repair time (defined in the standard EN 13306:2010).
		
		\item The "\NewTerm{Mean Time To Restore M.T.T.R.}\index{Mean Time To Restore}" applicable only for repairable items is the expected mean restart time (defined in the standard EN 13306:2010). In the area of services, M.T.T.R. is sometimes named "\NewTerm{Mean Time To Restore Service M.T.T.R.S.}\index{Mean Time To Restore Service}".
		
		\item The "\NewTerm{Mean Down Time M.D.T.}\index{Mean Down Time}" is equal to the mathematical expected mean of the time that the system is not operational. This indicator (not defined in the standard EN 13306:2010) therefore includes the M.R.T. It follows that the M.T.B.F. is equal to the sum of the M.O.T.B.F. (M.U.T.) and of the M.D.T.
		
		\item The "\NewTerm{Mean Time To Detection M.T.T.D.}\index{Mean Time To Detection}" is equal to the mathematical expected mean of the time that the system is not detected as being non operational. This indicator (not defined in the standard EN 13306:2010), as well as the M.R.T. is included in the M.D.T.
		
		\item The "\NewTerm{Mean Time to Answer M.T.T.A.}\index{Mean Time to Answer}" is equal to the mathematical expected mean of the time during which the user expected an answer or response of manufacturer after reporting a failure. This indicator (not defined in the standard EN 13306:2010), as well as the M.R.T. and M.T.T.D. are included in the M.T.T.A.
		
		\item The "\NewTerm{Overall Equipment Effectiveness O.E.E.}\index{Overall Equipment Effectiveness}" in the field of maintenance is traditionally defined as being simply the ratio of the operational running time and time expected operational performance (not defined in the standard EN 13306:2010).
		
		\item The "\NewTerm{Service Ability S.A.}\index{Service Ability}" which in the field of maintenance is often given by the following two relations and whose goal is to make it as close as possible to $1$ (that is to say to make the M.D.T. approaching zero):
		
	\end{itemize}
	and we can continue to define many others indicators outside the standard...
	
	The below diagram summarizes maybe a little better all these definitions:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/reliability_indicators.jpg}
		\caption{Common Reliability Indicators}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} For each of these indicators it will be indicated if external causes have been taken into account or not.\\
	
	\textbf{R2.} If the tested sample size is very small or just unitary (only one machine), it is customary to use the arithmetic average as an estimator of the expected mean.... Obviously it can lead to huge errors...\\
	
	\textbf{R3.} It is of course possible from the defined indicators to calculate whether a machine (element) is able to ensure the desired service (such as a number of pieces per year to produce for example!). So not only do these indicators can be measured, but they can therefore also be used to verify the achievement of a desired goal!\\
	
	\textbf{R4.} In the beginning of the 21st century countries should strictly speaking legislate to require all manufacturers to communicate the MTTF of their products so that consumers can make the best choice to buy and compare the values in relation to the guarantee provided! Unfortunately, this is not the case and it would highlight a current bad tradition in the consumer products industry that is to manufacture components whose life revolves around 200,000 hours to ensure industrial renewal of their market.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A machine is supposed to work 24/24 hours 7/7 days. It turned up today for $5,020$ hours with $2$ stops of a total of $20$ hours (thus included in the $5,020 [\text{h}]$). Give the conventional indicators relative to the limited information provided:
	

	\end{tcolorbox}
	The classification of systems in terms of availability commonly leads to 7 classes from "not available" (system available $90$\% of the time, and therefore unavailable over a month per year) to "ultra available" (available $99.99999$\% of the time and therefore unavailable only $3$ seconds per year): these classes corresponds arbitrarily to the numbers of "$9$" in the percentage of time that the class systems are available (one year includes $525,600$ minutes for refresh):
	
		The use of these parameters in the context reliability framework make me us told that we have an "approach through average values".
		
		Note however one thing! This is not because an event has a smaller or infinitely less probability than another one that it is less important!! Indeed we must obviously take into account (certainly a little in a empirical way) the gravity of the failure. Thus, a nuclear power plant may well be $1,000$ times more secure than a thermal central in terms of probabilities ... nothings is comparable in terms of severity when a major failure occurs in a nuclear plant! Indeed in the case of a thermal central there may be in the worst case $500$ persons affected while with a nuclear power plant that is another story (see Tchernobyl or Fukushima...)!
		
		Let us also note a simple case: Some components (typically electronic) have in their period of a maturity constant failure rate. The cumulative probability distribution of the failure that results is then immediately deducted as the instantaneous hazard function is given by $\lambda(t)=\lambda$ and therefore:
	
	Let us indicate that we find very often the latter relation in the following form in the literature:
	
	and then we have well:
	
	To summarize a bit for the exponential law:
	
	And as we have:
	
	And we proved in the section of Differential and Integral Calculus that:
	
	Therefore:
	
	The previous hazard and survival are then graphically of the following form:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/exponential_law_reliability.jpg}
		\caption{Plot of the hazard function and Reliability of the exponential law}
	\end{figure}	
	The density function of faulty components at time $t$ is obviously:
	
	It therefore follows an exponential law! This law and its moments are known to us (\SeeChapter{see section Statistics}). It then becomes easier to determine the M.T.T.F. and its standard deviation (inverse of failure rate) and also a confidence interval for it.
	
	Furthermore, if we calculate the reliability $R(t)$ at the time corresponding to the M.T.T.F. (inverse of the failure rate in the case of the exponential law) we will always obtain a cumulative probability of $F(t)=1-e^{-1}=36.8\%$ (so basically almost one chance on three chance to run at this time and $2$ chances on $3$ to be down) and not $50\%$ as sometimes we can intuitively think (this is the case only if the probability distribution is symmetric).
	
	Since the technical tables of reliability in the industry almost always assume the reliability rate to be constant then we better understand the importance of it (we will have an example in our presentation further below of topological systems).
	
	Also remember that we saw in the section of Probability that if we have a set of independent events (independent mechanisms) with a given probability, then the probabilities calculated on the whole as a globality involves the multiplication of probabilities!
	
	Therefore in a mechanism having independent but essential parts (system said to be "\NewTerm{serial system}\index{serial system}") the overall reliability density function will be equal to the multiplication of cumulative probability distributions $R (t)$ and which is therefore equivalent in the case of a exponential having a single exponential law whose overall failure rate is equal to the sum of the various parts failure rate!
	
	Another example... in mechanics, where the phenomenon of wear is the cause of the failure, the failure rate is often linear (so it increases steadily over time and is not zero at the initial starting of the device):
	
	Then:
	
	Therefore:
	
	As (for an equipment whose repair time is negligible):
	
	this integral can be calculated by numerical method. Therefore we prefer to take close law thanks to the chi-square adequation test (\SeeChapter{see section Statistics}), such as the Weibull distribution (see just below for detailed developments), which is a little bit the trash to put everything in the field of reliability engineering...
	
	You should know that in common and free public reliability data banks reliable on the market are normally given: the name of the material or component, the M.T.B.F., the average default rate, the statistical heritage (...), a multiplicative factor of failure rates depending on the environment or usage constraints. 
	
	\pagebreak
	\paragraph{Average Failure Rate}\mbox{}\\\\
	As we have seen, the hazard function usually varies with time. What is sometimes of interest to the practitioner is the average rate of failures within a given time interval, which is clearly specified by the functional specification. We then define the average failure rate or "\NewTerm{average failure rate}\index{average failure rate}":
	
	Therefore:
	
	This rate, therefore denoted by $\text{AFR}(t_1,t_2)$, is a single number that can be used as a specification or target for the population failure rate over that interval. If $t_1$ is $0$, it is dropped from the expression. Thus, for example, $\text{AFR}(40,000)$ would be the average failure rate for the population over the first $40,000$ hours of operation.  

	An interval often taken in practice is $0$ to $\tau$. Then ic comes:
	
 	Sometimes in the literature considering the following variant:
	
	and considering that $F(\tau)$ is small, by by a Taylor development to the second order, we have:
	
 	We can transform the antecedent relations into a failure distribution function:
	
 	and therefore:
	
 	And finally:	
	
	
	\pagebreak
	\subsubsection{Weibull Distribution}
	Once again maintenance techniques use probability and statistics so we refer the reader to the section of the same name in this book. However, there exists in the field of maintenance (and not only!) a probability density function widely used named "\NewTerm{Weibull law}\index{Weibull law}".
	
	It seemed to be completely empirical and defined by:
	
	where $f(x)\geq 0,x\geq 0$ with $\beta >0,\eta>0,-\infty<\gamma<+\infty$ that are named respectively "\NewTerm{scale parameter}\index{scale parameter}" $\eta$, "\NewTerm{shape parameter}\index{shape parameter}" $\beta$ and "\NewTerm{location parameter}\index{location parameter}"  $\gamma$.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The engineers in corporations have to to refer to \textit{IEC 61649 Weibull analysis} to make a standardized usage of this law!
	\end{tcolorbox}	
	The Weibull distribution is often written as following by putting $x:=x-\gamma$, $\eta:=\beta$, $\beta:=\alpha$ (in this case the location parameter is implicitly posed as equal to zero):
	
	It can be calculated in the English version of Microsoft Excel 11.8346 in this latter form using \texttt{WEIBULL( )} function.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The engineers in corporations have to to refer to I\textit{EC 61649 Weibull analysis} to make a standardized usage of this law!
	\end{tcolorbox}
	The Weibull law can be seen as a generalization of the exponential distribution function (we'll see why later!) with the advantage that it is possible to play with the three parameters to obtain almost anything.
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/engineering/weibull_law.jpg}
		\end{center}	
		\caption{Weibull Density and Distribution functions adifferent $\alpha$ values (source: Wikipedia)}
	\end{figure}
	By canceling the location parameter $\gamma$, we get the "\NewTerm{two-parameters Weibull distribution}\index{two-parameters Weibull distribution}":
	
	which has an important practical application and for which we calculated the estimators of the parameters in the Statistics section.
	
	By assuming again $\gamma=0$ and assuming also that we have $\beta=C=c^{te}$ we get "\NewTerm{one-parameter Weibull distribution}\index{one-parameter Weibull distribution}" (it's a little stupid as a definition but never-mind...):
	
	where the only unknown parameter is the scale parameter $\eta$. We assume that the parameter $\beta$ is known a priori from past experiences on similar samples. Notice that the two-parameter Weibull distribution function a is the derivative of the following repartition function (cumulative probability function or "unreliability" as say practitioner in the field of maintenance) where there are three successive interiors derivatives to fall back on the Weibull distribution function ... which is a good application of this we saw in the section of Differential and Integral Calculus:
	
	It follows therefore in reliability engineering that in this particular case, we have then:
	
	We also find sometimes this relation in the literature in the form:
	
	Note that if $\beta=1$, then we fall back on an exponential of mean $\eta$:
	
	The MTBF is then given by the mean of the Weibull distribution with nonzero $\gamma$:
	
	Let us put:
	
	with:
	
	What gives:
	
	The first integral above is already known to us, we have already study it in the section of Integral and Differential Calculus. This is the gamma Euler function:
	
	Therefore we have finally:
	
	By canceling the location parameter $\gamma$ we get the following current case in reliability:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If then by chance $\beta^{-1} \in \mathbb{N}$, then as we have proved during our study of the gamma Euler function:
	
	In case where $\beta^{-1} \in \mathbb{R}$ we must make use of the numerical tables obtained by the algorithms seen in the section of Theoretical Computing.
	\end{tcolorbox}	
	Similarly:
	
	Finally when the location parameter is equal to zero, we get:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some specialists, when it is made usage of two moments of order two in the probabilistic analysis of reliability, sometimes talk about "\NewTerm{mean square approach method}\index{mean square approach method}"...
	\end{tcolorbox}	
	To conclude on the Weibull Law, note that it is customary to consider five important and relevant situations of the Weibull distribution based on the values of its parameters:
	
	\begin{enumerate}
		\item Survival density function looks like a rapid exponential decay and therefore the hazard function (failure rate) is high initially and decreases over time (first portion of the "bathtube" hazard function/failure rate). This is typically the situation where the failures are early because  failures occur in the initial period of the life of the product.
		
		\item Survival density looks like an exponential decay and therefore the hazard function (failure rate) is constant over time (second portion of the "bathtube" hazard function/failure rate). This is typically a situation where failures are random and sources of multiple causes. In this situation, the Weibull models the phase of "useful life" of the product.
		
		\item The survival density function rises to a peak and then decreases with a fairly pronounced asymmetry (high density on the left) and therefore the hasard function (failure rate) increases rapidly at first and eventually stabilize. This is typically a situation where the initial wear is very fast.
		
		\item The survival density function looks like a half large Gaussian Law and the hasard function (failure rate) is growing rapidly (exponential type of growth). This is typically the situation where we reach the end of life of a set of parts (third portion of the "bathtube" hazard function/failure rate).
	\end{enumerate}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.85]{img/engineering/weibull_reliability_examples.jpg}
		\end{center}	
		\caption{Plot of four 4 Weibull survival functions for different parameter values}
	\end{figure}
	And with the corresponding hasard functions:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.85]{img/engineering/weibull_hasard_examples.jpg}
		\end{center}	
		\caption{Plot of four 4 Weibull hasard functions for different parameter values}
	\end{figure}
	To summarize  we have therefore for the Weibull distribution with two parameters:
	
	So we see that if well that if we pub $\beta=1$ we fall back on all the reliability functions based on the exponential law.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A car manufacturer knows the distribution of time until the first failure of one of its vehicles under average driving conditions. The Weibull distribution function obtained by the tests is a two-parameter Weibull function given by:
	
	where $x$ is the distance in kilometers of the tested vehicles.	
	If we want to find the warranty in kilometers encompassing $5\%$ of the vehicles, then we must solve the equation:
	
	That is to say with some elementary algebra:
	
	\end{tcolorbox}
	
	\paragraph{Two-parameter Weibull distribution linearization}\mbox{}\\\\
	In the case of the 2-parameter Weibull, the CDF (also the "unreliability" for recall) is given as we have just seen by (we use the temporal variables $t$ instead of $x$ now):
	
 	This function can then be linearized (i.e., put in the common form of $y=ax+b$ format) as follows for practial regression (adequation) analysis:
 	We start from the previous relation and rearrange the whole:
	
 	and we take the natural logarithm:
	
 	Therefore:
	
	Rearranging and taking the natural logarithm againa we get:
	
	Using the properties of the logarithm (\SeeChapter{see section Functional Analysis}):
	
	Then by setting:
	
	and:
	
	the equation can then be rewritten as:
 	
	which is now a linear equation with a slope of:
	
	and an intercept of:
	
	
	\pagebreak
	\subsubsection{Topology of Systems}
	When working with non-repairable real systems (mechanical, electronic or other), we face different constraints depending on the type of installation that we have. The study of such systems is also named "\NewTerm{Reliability Block Diagramm}\index{Reliability Block Diagramm}".
	
	The two main assumptions of the study of these systems are:
	\begin{enumerate}
		\item The failure of a component is independent of the  others.
		\item No breakdowns arrive at the same time.	
	\end{enumerate}
	We recognize five main topologies in which each component is represented by a block:
	\begin{enumerate}
		\item Serial topology:
		
		If only one component fails the whole system does not work anymore (the examples are so numerous and easy to find that we did not mention any of them).
		
		So in the hypothesis that the failure of a component is independent of the other, the cumulative probability of failure is the product of cumulative probabilities (\SeeChapter{see Section Probabilities}).
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_serial.jpg}
			\end{center}	
			\caption{Serial Topology}
		\end{figure}
		In the study of probabilistic fault tree (see further below), this topology corresponds to an OR gate with $n$ inputs, as it is sufficient that onE blocks may be down so that the output no longer works:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/gate_or.jpg}
			\end{center}	
			\caption{OR gate}
		\end{figure}
		As often made use of the exponential distribution. The multiplication of several terms of the accumulated probabilities are relatively long to write, we prefer the use of the cumulative probability of reliability $R$.
		
		Thus, the reliability (probability of operation) of a series system is given by:
		
		Which brings us well to zero reliability if at least one block has zero reliability.
		
		What is traditionally noted in the field:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The attentive reader will have noticed that the serial system is always less reliable than its least reliable component!
		\end{tcolorbox}	
		Caution!! In the case of electronic components, the failure rate is often regarded as constant for simplicity and density function is that of an exponential law:
		
		However, we have proved that in the case of a non-repairable system:
		
		And as we have proved it in the section Statistics the expected mean of the exponential law:
		
		We have also proved earlier that:
		
		Therefore:
		
		So for a series constant failure rate system:
		
		Thus, in this particular case:
		
		or written differently:
		
		and... the problem is that the a lot of Internet pages that talk about series topology systems (or parallel) implicitly use an exponential law, this is why sometimes beginner practitioners use this last relation without having studied mathematics underlying details and this brings to huge errors in the reliability of their product.
		\item Parallel topology:
		
		Unlike the previous system, parallel system continues to operate if at least one component still works (typically redundancy systems in airplanes, rockets, nuclear power plants or critical computer systems). Therefore is is considered as the most important system to study.
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_parallel.jpg}
			\end{center}	
			\caption{Parallel Topology}
		\end{figure}
		In the study of probabilistic fault tree (see further below), this topology corresponds to an AND gate with $n$ inputs, as it is necessary that all blocks are down so that the output no longer works:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/gate_and.jpg}
			\end{center}	
			\caption{AND gate}
		\end{figure}
		In other words, it stops working if and only if the cumulated probability failure is equal to:		
		
		So it comes immediately for a purely parallel system:
		
		In the very common case where the blocks are redundant with equal reliability $R_i=R$ and that $R_P$ is fixed by the customer we must found a component such that is reliability is therefore equal to:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		In Project management this type of parallel structure is typically used when one wishes to know the total reliability of several supplier of identical parts (in this case if any of them has delays or other problems, the others must afford to mitigate the negative impact) knowing or believing obviously the reliability of each...
		\end{tcolorbox}	
		We therefore have the parallel system whose components have a constant failure rate and all identical (to simplify the study):
		
		Therefore we have:
		
		If we put:
		
		and replacing in the prior-previous expression we get:
		
		As far as we know is not possible to integrate this latter relation, but if we compared for different values of $n$ fixed then we see quickly that:	
		
		or written in another way:
		
		Let us still do the detailed development for a parallel system with two components whose failure rate is constant and not identical. For this, we start from the relation proved earlier above:
		
		Either in the case where $n$ is equal to $2$, we have:
		
		and therefore:
		
		So if $\lambda_1=\lambda_2$, we have:
		
		Therefore:
		
		Before continuing with other topologies, here is an example of a small school assignment (with solution!) mixing parallel/serial topology that shows that this does not apply only to machines.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		A laborant must perform $3$ different tests on a sample set: Tests $A$, $B$ and $C$ in the given order! We know that the laborant miss the test $A$ ne out of four times but may start it again $2$ times in case of failure (so they can do it $3$ times in total), they succeed the terst $B$ in $96\%$ of times and may also start it again only once on failure (so they can do it $2$ times in total), they fail the test $C$ half of time and can not start it again. What is the probability that a laborant having successfully made the $3$ lab tests in only one time (without failed)?\\
		
		To solve this exercise you have to see each of reproducible tests as a parallel process and all the tests as a serial process. We then the have total probability that is given by:
		
		\end{tcolorbox}
				
		\item Topology $k/n$:
		This system works if $k$ out of $n$ components of same reliability law $R$ still work. This is typically the case of RAID hard disks computer where it is necessary to more than one to always works in the end and this number is determined by the volume of data. It is also operating principle of redundancy voting system on airplanes where we have a  $2/3$ system named "triplex".
		
		We then have the following schematic representation:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_kn.jpg}
			\end{center}	
			\caption{$k/n$ Topology}
		\end{figure}
		In the study of probabilistic fault tree (see further below), this topology corresponds to a $k$-OR (Voting OR), as it is necessary that $k$ blocks are down so that the output no longer works:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/gate_or_kn.jpg}
			\end{center}	
			\caption{Gate $k$-OR (Voting OR)}
		\end{figure}
		Therefore seek the cumulative probability law of reliability returns to the question of the cumulative probability of having $k$ elements that work among $n$ that are indistinguishable.
		
		This therefore is equivalent to use the binomial distribution (\SeeChapter{see section Statistics}) for which we have proved that the cumulative probability was given by:
		
		This relationship is extremely important in the practice of reliability engineering !!! We notice as well as for $k=n$ we find the reliability of a series system whose elements all have the same reliability:
		
		And for $k=1$, we fall back on the reliability of a parallel system whose elements all have the same reliability (remember the properties of the binomial coefficient in the section Calculus if necessary):
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		In the case of the triplex, we have therefore:
		
		So, if the reliability is given by an exponential law where we have already proved that:
		
		Then we have:
		
		So the overall MTBF of the $2/3$ system is larger than a simple system which obviously the goal.
		\end{tcolorbox}
		
		\pagebreak
		\item[T4.] Serial/Parallel and Parallel/Serial topology with symmetrical configuration:
		
		These are just simple compositions of the first two previously studied systems.
		
		We first have the series/parallel system:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_serial_parallel.jpg}
			\end{center}	
			\caption{Serial/Parallel symmetric Topology}
		\end{figure}
		However, as the series systems are given by:
		
		and the parallel by:
		
		the composition of the two gives trivially in the case above:
		
		And we also have in the family of parallel/series systems
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_parallel_serial.jpg}
			\end{center}	
			\caption{Parallel/Serial symmetric Topology}
		\end{figure}
		when by using exactly the same approach as previously we have:
		
		
		\item[T5.] Complex Topology:
		
		In fact, it is not really about complex systems but they just require a little knowledge of probability axioms. The particular example which will interest us is the following (typical cascading RLC filter):
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_bridge.jpg}
			\end{center}	
			\caption{Complex topology (bridge type)}
		\end{figure}
		named "\NewTerm{bridged network}\index{bridged network}".
		
		And we guess that what makes the complex system robust is the component number $5$. We can then consider a first approach that is to break down the problem.
		
		The system relatively to component $5$ will be in one of the state:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_bridge_first_decomposition.jpg}
			\end{center}	
			\caption[]{Bridged Network system decomposition (first stage)}
		\end{figure}
		if defective with a probability density function:
		
		and having itself a reliability:
		
		according to our previous results of complex series/parallel system.
		
		Or in the following state if $5$ is functional with a probability density law $R_5$:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_bridge_second_decomposition.jpg}
			\end{center}	
			\caption[]{Bridged Network system decomposition (second stage)}
		\end{figure}
		and having itself a reliability of:
		
		according to our previous results on the parallel/serial complex system.
		
		Since the system can not be in both states at the same time (at this time it is not a quantum system...), we are dealing with a disjoint probability (\SeeChapter{see section Probabilities}) that is to say the sum of the densities to which we must join the other components. Therefore we have:
		
	\end{enumerate}
		We can then using all 5 previous topologies evaluate the reliability of any system!
	
	Another strategy for complex systems is to decompose them into simple series or parallel systems. If this is not possible, we can calculate the reliability of each system configuration that is considered to operate and then sum the probabilities of operation of each configuration.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Let's make an example by considering the following case:
		\begin{figure}[H]
			\begin{center}
				\includegraphics{img/engineering/topology_complex_example.jpg}
			\end{center}	
			\caption[]{Special case assumed as a complex system}
		\end{figure}
		And consider the following truth table with the $2^n$ possible configurations:
	\end{tcolorbox}
	
	The principle (not always easy to guess...) is to say that an UP state (thus worth equal $1$) is assigned to the value $R_1$ of of the concerned block $i$ and the DOWN state (thus worth equal $0$) is assigned to the value $1-R_i$ of the concerned block $i$. Each value will be multiplied with others to get the total reliability of the system in a given state.
	
	Thus, in the previous example, we have $3$ states that allow the system to operate. Let us calculate their respective reliability. The states No $4$ and No $5$ thus provide by definition:
	
	The state No $6$ gives him by definition:
	
	and we sum all as previously indicated:
	
	And we can check that this approach is actually correct by taking the general relation of such a system proved above:
	
	this shows that we have the same result and that the decomposition approach also works.
	
	Finally, let us notice in conclusion that when we include in the systems elements that can tolerate or accept some errors, we then speak of "\NewTerm{fault tolerance}\index{fault tolerance}" and we distinguish mainly three types:
	\begin{enumerate}
		\item \NewTerm{Active redundancy}\index{active redundancy} In this case all redundant components operate simultaneously.
		
		\item \NewTerm{Passive redundancy}\index{passive redundancy} One redundant element is working, the others are waiting, which has for advantage to reduce or remove the aging of redundant elements, but in return requires the insertion of a component of failure detection and switching.
		
		\item \NewTerm{Majority Redundancy}\index{majority redundancy} This redundancy mainly concerns signal processing. The output signal will be that of the majority of redundant components.
	\end{enumerate}
		
	\pagebreak
	\paragraph{Fault Tree Analysis}\mbox{}\\\\
	A "\NewTerm{fault tree}\index{fault tree}" is a widely used technique in engineering safety studies and reliability of systems consisting of represent the possible combinations of events that allow the realization of a predefined undesirable event. Such a representation therefore highlights the relationships of cause and effect:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
		% Gates and symbols style
		    and/.style={and gate US,thick,draw,fill=red!60,rotate=90,
				anchor=east,xshift=-1mm},
		    or/.style={or gate US,thick,draw,fill=blue!60,rotate=90,
				anchor=east,xshift=-1mm},
		    be/.style={circle,thick,draw,fill=green!60,anchor=north,
				minimum width=0.7cm},
		    tr/.style={buffer gate US,thick,draw,fill=purple!60,rotate=90,
				anchor=east,minimum width=0.8cm},
		% Label style
		    label distance=3mm,
		    every label/.style={blue},
		% Event style
		    event/.style={rectangle,thick,draw,fill=yellow!20,text width=2cm,
				text centered,font=\sffamily,anchor=north},
		% Children and edges style
		    edge from parent/.style={very thick,draw=black!70},
		    edge from parent path={(\tikzparentnode.south) -- ++(0,-1.05cm)
					-| (\tikzchildnode.north)},
		    level 1/.style={sibling distance=7cm,level distance=1.4cm,
					growth parent anchor=south,nodes=event},
		    level 2/.style={sibling distance=7cm},
		    level 3/.style={sibling distance=6cm},
		    level 4/.style={sibling distance=3cm}
		%%  For compatability with PGF CVS add the absolute option:
		%   absolute
		    ]
		%% Draw events and edges
		    \node (g1) [event] {No flow to receiver}
			     child{node (g2) {No flow from Component B}   
			     	child {node (g3) {No flow into Component B}
			     	   child {node (g4) {No flow from Component A1}
			     	      child {node (t1) {No flow from source1}}
			     	      child {node (b2) {Component A1 blocks flow}}
					}
			     	   child {node (g5) {No flow from Component A2}
			     	      child {node (t2) {No flow from source2}}
			     	      child {node (b3) {Component A2 blocks flow}}
					}
				   }
			     	child {node (b1) {Component B blocks flow}}
				};
		%% Place gates and other symbols
		%% In the CVS version of PGF labels are placed differently than in PGF 2.0
		%% To render them correctly replace '-20' with 'right' and add the 'absolute'
		%% option to the tikzpicture environment. The absolute option makes the 
		%% node labels ignore the rotation of the parent node. 
		   \node [or]	at (g2.south)	[label=-20:G02]	{};
		   \node [and]	at (g3.south)	[label=-20:G03]	{};
		   \node [or]	at (g4.south)	[label=-20:G04]	{};
		   \node [or]	at (g5.south)	[label=-20:G05]	{};
		   \node [be]	at (b1.south)	[label=below:B01]	{};
		   \node [be]	at (b2.south)	[label=below:B02]	{};
		   \node [be]	at (b3.south)	[label=below:B03]	{};
		   \node [tr]	at (t1.south)	[label=below:T01]	{};
		   \node [tr]	at (t2.south)	[label=below:T02]	{};
		%% Draw system flow diagram
		   \begin{scope}[xshift=-7.5cm,yshift=-5cm,very thick,
				node distance=1.6cm,on grid,>=stealth',
				block/.style={rectangle,draw,fill=cyan!20},
				comp/.style={circle,draw,fill=orange!40}]
		   \node [block] (re)					{Receiver};
		   \node [comp]	 (cb)	[above=of re]			{B}  edge [->] (re);
		   \node [comp]	 (ca1)	[above=of cb,xshift=-0.8cm]	{A1} edge [->] (cb);
		   \node [comp]	 (ca2)	[right=of ca1]			{A2} edge [->] (cb);
		   \node [block] (s1)	[above=of ca1]		{Source1} edge [->] (ca1);
		   \node [block] (s2)	[right=of s1]		{Source2} edge [->] (ca2);
		   \end{scope}
		\end{tikzpicture}	
		\caption{Example of Simple Fault Tree (author: Zhang Long)}
	\end{figure}
	This technique is complemented by mathematical processing that allows the combination of single failures and their probability of occurrence. It therefore allows to quantify the probability of occurrence of an adverse event.
	
	The idea is very simple as long as we use only binary probabilities, by cons when it is necessary to use continuous distributions functions (the most common case in practice), we must go through Monte Carlo simulations (\SeeChapter{section Theoretical Computing}) with typical software like Weibull++.
	
	Consider the following fault tree that has $5$ primitive events and $3$ combined events and where the probabilities of the primitive events are assumed to be known:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/fault_tree.jpg}
		\end{center}	
		\caption{Example of Simple Fault Tree with binary probabilities and AND / OR gates only}
	\end{figure}
	We then for the next event $E6$ the following probability since it is an AND gate (equivalent to a parallel system):
	
	and for the event $E7$ since it is an OR gate (similar to a series system), we have:
	
	And so in the end:
	
	Thus, the probability of having the room without light is (a posteriori) of $14.24\%$.
	
	\paragraph{Markov Chain Reliability Model}\mbox{}\\\\
	The notable strengths of Markov chain (\SeeChapter{see section Statistics}) models for reliability analysis is that they can account for repairs as well as failures. This makes the technique particularly useful for assessing the long-term average reliability of one or more devices with established maintenance and repair strategies.  With the advent of high-integrity "fault-tolerant" systems, the ability to account for repairs of partially failed (but still operational) systems has become increasingly important. Markov modeling is well-suited to the task of determining inspection and repair intervals needed to achieve a desired level of safety.

	For any given system, a "\NewTerm{Markov chain reliability model}\index{Markov chain reliability model}" consists of a list of the possible states of that system, the possible transition paths between those states, and the rate parameters of those transitions. 
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We should speak rigorously about "\NewTerm{semi-Markov chain reliability model}\index{Markov chain reliability model}" as mathematicians do the difference between a Markov chain where the jumps from one state to another are at constant probability an a semi-Markov chain where the jumps are random variables with any distribution, whose distribution function may depend on the two states between which the jump is made. For example semi-Markov process where all the holding times are exponentially distributed is named a "continuous time Markov chain/process".
	\end{tcolorbox}	
	In reliability analysis the transitions usually consist of failures and repairs. When representing a Markov model graphically, each state is usually depicted as a "bubble", with arrows denoting the transition paths between states, as depicted in the figure below for a single component that has just two states: healthy and failed.
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/markov_reliability_chain.jpg}	
		\caption{Markov Chain Reliability Model example}
	\end{figure}
	This system is then formalized in the following form (not obvious to guess the very first time):
	
	We can rewrite system in matrix form as well:
	
	With:
	
	A possible way to solve this system consists in a first time in diagonalizing (if possible) the matrix, that is to say to find an invertible matrix and a diagonal matrix such that:
	
	We thus fall back an application of the spectral theorem proved in the section of Linear Algebra and applied to the resolution of differential equations (it is sometimes terrifying and admirable the extent of the striking power of pure mathematics...).
	
	From here we want to be able to write:
	
	or also (it remains the same):
	
	By doing thechange of variable:
	
	the system to be solved becomes obviously:
	
 	Let us denote by $\gamma_2$,$\gamma_1$, $\gamma_0$ the diagonal elements of $D$ and $\vec{X}=(X_2,X_1,X_0)^T$. We then want to rewrite the previous system as:
	
	But the solutions of this system are easy to determine, they are given by:
	
	for $i=0,1,2$ where $c_i^{te}\in \mathbb{R}$.

	Finally, we will determine the solutions of the system of departure with the help of the equality:
	
	Well... now let us put all this into practice!

	To diagonalize $M$ (if it is possible) we must determine its eigenvalues and its eigenspaces. Calculations made (see chapter of Linear Algebra for the general method) we find that the values of $M$ are:
	
	The corresponding eigenspaces are given by the vectors (see chapter of Linear Algebra for the general method):
	
	Hence, if we write:
	
	We can then indeed write:
	
	The diagonal elements of $D$ are:
	
	The components of the vector $\vec{X}$ being given earlier above by:
	
	we therefore have:
 	
	To finish the solutions of the original system are for recall given by:
	
	So finally:
	
 	If we impose the initial condition:
	
	we then have the system of three equations with three unknowns:
	
 	Its resolution should be quite trivial for the reader (if it is not the case do not hesitate to contact us we will add the details) and we get:
	
	So at the end:
	
	We can also verify that for any $t$ we have:
	
 	This is the reason why we often find the above system in the following form:
	
 	From the above formulas it follows that the availability on the long-term system is given by:
	
 	Because $\lambda+\mu>0$ and because we also have:
	
	
	\pagebreak
	\subsubsection{Maximum Likelihood for failure rate determination of samples}
	We studied in the section Statistics and Theoretical Computing that the technics using the maximum likelihood allowed us to estimate the maximum or minimum value of the parameters of a statistical law under the assumption that events are independent.
	
	This technique, even empirical and sometimes quite questionable, is also used in the field of the reliability. As the theory has already been developed in the section Statistics let us see directly some practical cases.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider a system whose time intervals between failures is distributed according to an exponential law:
	
	of unknown parameter $\lambda$ and where we consider (as always ...) that every failure is independent.\\
	
	Observations gave us the following number of months between each failure: $10, 5, 11, 12, 15$.\\
	
	Then using maximum likelihood which idea is for refresh is to maximize (minimize) the total probability of independent events (therefore we must multiply the probabilities), we have:
	
	Therefore determine the failure rate that maximizes the likelihood is equivalent as we know to calculate when the derivative (\SeeChapter{see section Differential and Integral Calculus}) is zero:
	
	We then have two roots that are trivial:
	
	and of course we will keep the second as root failure rate of our system.
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. We consider now see the same data as before but this time with a Weibull distribution with the following simplified notation:
	
	and therefore:
	
	To maximize this kind of function, as far as we know, we can use only numerical methods. By cons, a simple spreadsheet software like Microsoft Excel with its solver allows to find the two parameters of the Weibull distribution in three clicks!\\
	
	E3. We imaginer that we put under observation $10$ similar manufacturedelements. On the $10$, we observed a stop of $4$ of them after respectively $700, 800, 900$ and $950$ hours. The remaining $6$ items will be considered as being over the $1,000$ work hours and we stop observing from this value. Assuming that the failure distribution is exponentially again, then we have (the idea is subtle but simple):
	
	It comes then:
	
	We then have two roots that are trivial:
	
	and of course we will keep the second root as failure rate of our system.
	\end{tcolorbox}

	\pagebreak
	\subsubsection{Kaplan-Meier Survival Rate}
	In areas such as high-level industry, high-level medicine or high-level biology, we often interested in the:
	\begin{enumerate}
		\item Survival duration after a serious event
		\item Duration of remission after treatment or surgery
		\item Duration of symptoms after an abnormality
		\item Duration of a symptomless infection
	\end{enumerate}
	We seek very often to distinguish at least "the event of interest":
	\begin{enumerate}
		\item System shutdown after serious event
		\item End of remission 
		\item End of a symptom after anomaly
		\item Beginning of a symptom during an infection
	\end{enumerate}
	of the variable to explain "duration before the event of interest":
	\begin{enumerate}
		\item Elapsed duration before shutdown
		\item Elapsed duration before the end of remission
		\item Elapsed duration before the end of the symptom
		\item Elapsed duration without symptoms
	\end{enumerate}
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] We name "\NewTerm{remission}\index{remission}", the reduction of a disease or a temporarily malfunction.
		
		\item[D2.] The "\NewTerm{survival time}\index{survival time}" or "\NewTerm{lifetime}\index{lifetime}" $T$ means the time which elapses from an initial time (start of treatment, diagnosis or failure, etc.) until the occurrence of a final event of interest (patient death, relapse, remission, cure, repair, etc.). We say that the object of the study "survives at time $t$" if at this moment the final interest event has not yet occurred.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Although this type of study can be associated with preventive maintenance (\SeeChapter{see section Industrial Engineering}), statisticians associate this type of study rather to the domain of "\NewTerm{survival analysis}\index{survival analysis}".
	\end{tcolorbox}
	
	We will focus in this book on a particular context but that can be easily generalized:
	\begin{itemize}
		\item Cohort/Clinical trial where we study the survival time of each patient (machine).
		
		\item All patients (machines) do not have the same observation time (different instants of entry into the study).
		
		\item We have information on the survival time of each patient (machine) but we do not know exactly when it happens.
	\end{itemize}
	From the last two points, we conclude that the survival time can be censored. So the usual statistical techniques does not apply directly as censored data require special treatment (of course, if we remove the censored data we lose information). It goes without saying that this situation is extremely common and therefore the study of the Kaplan-Meier estimate is very important (for other survival models the reader should refer to the Statistics section in part concerning Survival Models).
	
	\textbf{Definition (\#\mydef):} The duration $T$ is said to be censored if the duration is not observed completely. The different types of censoring are:
	\begin{itemize}
		\item Type I censoring: fixed right. In this situation, the time is not observable beyond a maximum, fixed, named "\NewTerm{fixed-censoring}\index{fixed-censoring}" and imposed. So either we have the opportunity to observe the real duration of the event of interest for the item if it occurs before the fixed-censorsing, or we limit ourselves to the fixed-censoring time if the vent of interest has not occurred before.
		
		\item Type II censoring: wait. In this situation, we observe the lives of $n$ individuals until $m\leq n$ individuals have seen the event to occur (deceased). The duration considered is therefore that of the beginning of the experiment until the event of interest for the $m$-th.
		
		\item Type III censoring: random left. In this situation, we do not know when the event of interest has occurred (because we started to observe the subject of study too late). We can not then deal with "durations" in the measurable sense and we have to limit ourselves to a simple count.
		
		\item Type III censoring: random right. In this situation, we do not know when the event will take place (because we stopped to observe the subject before it occurs  for any reason). We can not then also not deal with "times" in the measurable sense and we must limit ourselves to a simple count.
		
		\item Type IV censoring: random intervals. In this situation, we have a mixture of random left and right censoring. That is to say that for some study subjects, we do not know when the event of interest has begun, and for others we do not know when it will occur (if any. ...).
	\end{itemize}
	In the machinery industry, we often deal with the type I censoring: fixed right. In the medical field, in clinical trials, we often deal with a type III censoring: random right. In the case of pandemics, we are dealing with type III censoring: random left.
	
	To introduce this subject, rather than doing obscure theory, as always in this book we prefer at pragmatic approach. Suppose that the study is a clinical trial involving two groups of patients receiving two types of treatments. Two important questions raised to the physicians in a Phase II clinical trial (phase I is for non-toxicity approval to human and phase 0 for animals):
	\begin{enumerate}
		\item Is one of the two treatments more effective than the other in terms of improving patient survival?
		
		\item Can we highlight prognostic factors, that is to say that improve / deteriorate survival?
	\end{enumerate}
	
	To answer the first question we can develop statistical methods that will allow us to compare the two groups of patients who receive both types of treatment.
	
	To answer the second question we propose a model that links patient survival time for explanatory variables and highlight the prognostic factors.
	
	Let us as always assist the theory with an example. For this consider the following table where two cohorts of patients (we move from mechanical engineering to human engineering...) of same initial size with acute leukemia drug test (6-MP ) against a placebo (of course blindly).
	
	We have the following remission times for $21$ patients (the table of $21$ lines therefore indicates the number of weeks for where patient is considered cured after treatment before falling again ill):
	
	The $+$ sign corresponds to patients who left the study for that week. They are therefore censored. For example, the fourth patient was lost of view for any reason after $6$ weeks of treatment with 6-MP: it has therefore has a duration of remission greater than $6$ weeks. So in the study 6-MP, there are $21$ patients and $12$ with censored data.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The theoretical model assumes that censorsing is independent of the survival time (not informative censoring). But if censoring is due to the discontinuation of treatment, the independence assumption is not valid anymore!
	\end{tcolorbox}
	For the placebo group it is simple to make a survival curve. It is sufficient to produce the following table (for the omitted weeks, obviously we impose the number of remission as constants):
	
	So if the data are not censored, the survival $S(t)$ can be estimated by the proportion of individuals surviving at time $t$, that is customary to write under the following mathematical form:
	
	The idea is therefore to estimate:
	
	by the proportion of patients who survived until time $t$.
	
	If the data are censored, the estimated survival function requires specific tools. Kaplan and Meier have proposed in this particular case the following calculation:
	
	Let's see it in a slightly more mathematical form:
	
	With of course:
	
	If we denote by $X(1)\leq X(2)\leq ...\leq X(n)$ the moments (sorted) where an event occurred (death or censored), then we have:
	We estimate:
	
	where $d_k$ is the number of deaths (failures) observed in the time corresponding to the event $X(k)$ and $R_k$ is the number of individuals at risk (at risk of death/failure) just before $X(k)$.
	
	We define the Kaplan-Meier estimator for any $X(0)\leq t <X(k)$ by:
	
	Therefore we get doing now for 6-MP group (not the placebo group!!!!!) the following :
	
	We thus find the same values as those given for example by Minitab Statistical Software 15.1.1 (for the details see the Minitab companion book).
	
	\pagebreak
	\subsubsection{ABC Method}
	In a company, the tasks are various and the maintenance teams are systematically reduced to a minimum. In addition, the most advanced technologies in maintenance can be very expensive, and should not be applied indiscriminately.
	
	It is therefore appropriate to be organized effectively and efficiently. The ABC analysis\footnote{Not to be confused with Activity-based costing (ABC) that is a costing methodology that identifies activities in an organization and assigns the cost of each activity with resources to all products and services according to the actual consumption by each.} (one should say in all rigor "ABC objective"), using implicitly use the cumulated Pareto law (\SeeChapter{see section Statistics}), allows remedy this relatively well with an empirical approach. Thus, a classification of costs in relation to the types of failure gives priority interventions to carry on (this empirical method is also used in many other areas including one that is very well known: supply chain management).
	
	The idea is at first like Pareto analysis (\SeeChapter{see section Quantitative Management Techniques}) to classify faults in order of increasing maintenance costs (or cost impact in case of failure) every failure relating a simple or complex system and to establish a chart matching the percentage of cumulative cost  to the cumulative percentages type of failure.
	
	Then we distinguish traditionally in the industry three zones:
	\begin{enumerate}
		\item Zone A: In most cases, about $20\%$ of total failures represent $80\%$ of costs and so this is the priority area.
		
		\item Zone B: In this zone, representing $30\%$ of total failures we have the following $15\%$ additional costs.
		
		\item Zone C: In this zone $50\%$ of the remaining failures generates only $5\%$ of costs.
	\end{enumerate}
	The field of participatory web (web 2.0) considers areas (targets) of respectively $90-9-1$ percents so we found very everything and anything in this field as it is empirical...
	
	Let us see an example assuming that the following data were collected and we would like to do an analysis of the percentage of machines that should be that we focus on to reduce the cost off failures hours of approximately $80\%$ (in reality we will focus more on the percentage of the financial cost!).
	
	Then, in a spreadsheet like Microsoft Excel or other ... we can easily establish the following table:
	
	We then have graphically:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/abc.jpg}
		\end{center}	
		\caption{Graph of the ABC method}
	\end{figure}
	where the areas $A$, $B$ and $C$ are rounded to existing points. Thus, the critical area $A$ contains the machines $11$, $10$, $1$, $8$, $9$ and $3$. improving the reliability of these machines can therefore get up to $78.8\%$ time saving on downtime.
	
	Now let us determine the parameters of the Pareto distribution law (\SeeChapter{see section Statistics}):
	
We have to determine $k$ and $x_m$ and the other parameters are given us by our measurements (the table).

	We can play in the following way:
	
	Therefore:
	
	So thanks to:
	
	we should be able to determine the two seeked parameters by considering the expression as the equation of a linear function whose $k$ is the slope and $k\log(x_m)$ the intercept:
	
	A simple linear regression (\SeeChapter{see section Theoretical Computing}) gives us:
	
	Therefore:
	
	So we have finally:
	
	What then gives the following table (the $x_i$ of the Pareto distribution are the Cumulative\% failure):
	
	Then we can get the real corresponding Pareto curve easily in Microsoft Excel 11.8346:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/abc_measurement_vs_theory.jpg}
		\end{center}	
		\caption{ABC method associated with a Pareto curve}
	\end{figure}
	The difference between the experimental curve and the theoretical is significant... As $k$ is less than $1$, then as we have seen proved it the section of Statistics, the Pareto law has neither expected mean nor variance.
	
	The practitioner must be careful to the fact that in the field of management, the Pareto law is used wrongly for a little bit everything when an another probability distribution could be much more appropriate.
	
	Furthermore, any GoF test (\SeeChapter{see section Statistics}) shows us that we must reject the approximation by a Pareto Distribution. Moreover, specialized software such @RISK reject the Pareto approximation beyond the $20$ best adjustments, the best fit with this same software is the Log-Normal law.
	
	\pagebreak
	\subsection{Design of Experiments (DoE)}
	The behavior or the subjective assessment of industrial or manufactured products is generally a function of many phenomena, often dependent on each other. To predict the behavior / assessment, the products and phenomena are modeled and simulations are performed. The relevance of simulation results obviously depends on the quality of the models.
	
	In particular, as part of the design or redesign of a product, the models generally involve a number of physical quantities (parameters) which we are authorized or not to modify. The behavior of industrial products generally depends on many phenomena, often dependent on each other. To predict this behavior, product and phenomena are modeled and simulations are performed.
	
	However, these tests (simulations) are many times expensive, and this even more if the number of parameters to vary is important. Indeed, changing a parameter may for example require disassembly and reassembly of the product, a destructives analysis, or the manufacture of several different prototypes (case of a piece produced in series), or the interruption of production to change tool (for a manufacturing process) ... The cost of an experimental study depends on the number and order of the tests carried out.

	The idea then is to select and order tests in the purpose to identify, at lower costs, the effects of parameters on the response of the product. These are statistical methods using most often simple mathematical concepts. The implementation of these methods has three stages (but the practitioner should better refer to the ISO 3534-3: 1999 norm on the subject to be more rigorous!):
	\begin{enumerate}
		\item Postulate a behavior model of the system (with coefficients being unknown)
		
		\item Do data transformation to adjust the model to the assumptions of DoE tools and techniques (typically log-transformation for rates, etc.)
		
		\item Check that the assumptions of the DoE models are satisfied for your experiment after data transformation
		
		\item Define an experiment protocol, i.e. a series of tests ("\NewTerm{runs}\index{runs}") for identifying the model coefficients
		
		\item Do the tests, identify the coefficients ("\NewTerm{runs}") and influential variables
		
		\item Determine the values of the variables that allow you to get as closed as desired to the target result or with minimal variability and conclude.
	\end{enumerate}
	The experimental designs, or "\NewTerm{Design of Experiment (DoE)}\index{Design of Experiment}" allow to better organize the tests that accompany scientific research or industrial research. They are applicable to many disciplines and to all industries from the moment the practitioner is seeking for the relation between a variable of interest, $y$ (amount of scrap, defects detection, amplitude, vacations, satisfaction, etc.) and control or environmental variables $x_i$ in the purpose of:
	\begin{itemize}
		\item Determining which variables $x_i$ are most influential on the response $y$
		
		\item Determining where to set the influential variables $x_i$ so that $y$ is almost always near the desired nominal value
		
		\item Determining where to set the influential $x_i$ so that variability in $y$ is small
		
		\item Dtermining where to set the influential variables $x_i$ so that the effects of uncontrollable variables (denote $z_i$ most of times) are minimized
	\end{itemize}
	This is why they are softwares to treat them such as R, Minitab, QI Macros or JMP mainly to name the most known one.
	
	Let us also indicate that the experimental designs are a pillar of chemometrics (mathematical tools, especially statistics, to get the maximum information from chemical data) but are also used in supply chain optimization, marketing product design and and computer network dimensioning. But designed experiments were first used by agronomists during the 20th. This method seemed highly theoretical at first, and was initially restricted to agronomy. Genichi Taguchi made the designed experiment approach more accessible to practitioners in the manufacturing industry.

	In the last years of the 20th century, the application of design of experiments has developed, particularly given the acknowledged fact that they are essential to improving the quality of goods and services. Even if statistical quality control, managerial solutions, inspections, and other quality tools also perform this function, the design of experiment is the methodology of choice in the case of an environment with complex parameters, variables and interactions and when we need to quantify the facts and benefits. From a historical point of view, the design of experiment have been developed and evolved in the agriculture sector. Medicine/Pharma has also a long history of design of experiments developed with caution. Currently, industrial environments prove the significant benefits of this methodology, because of the ease of initiation of efforts (of user-friendly application software), better training, influential trainers, and many successes achieved by DoE. Furthermore, today the process of analysis and manipulations is automated by robots in most high level industries.
	
	There are three main types of recognized DoE actually:
	\begin{enumerate}
		\item "\NewTerm{Screenings designs}\index{Screenings designs}"  or "\NewTerm{factor screening experiments}\index{factor screening experiments}" whose purpose is to discover the most influential factors on a given response with a minimal number of experiences. This is the simplest family as considered close to the experimental intuition (it is sometimes considered a subfamily of the second family below because it disregards the interactions of factors and is thus reduced to a purely additive model).
		
		\item "\NewTerm{Modelization plans}\index{Modelization plans}" whose purpose is to find the mathematical relations between the measured responses to the variables associated with factors through an analytical approach or purely matrix one. Complete factorial and fractional designs (2 levels by factors with linear models) and response surfaces designs (at least 3 levels in models with factors of the second degree) are part of this family.
		
		\item "\NewTerm{Mixture designs}\index{Mixture designs}" whose purpose is the same as the second family, but where the factors are not independent and are forced by a dependence relation (e.g. the sum of their ratio must be typically equal to a given constant).
	\end{enumerate}
	The technique of DoE has for purpose to be scientifically more advantageous than techniques consisting in only change one factor at a time (thus still let fixed the other factors) because the latter does not allow a deep statistical study of errors and (especially!) hides completely the interactions.
	
	The general principle of the strategy of DoE is based on the fact that the study of a phenomenon can, usually, be summarized as follows: we are interested in a quantity $y$ that depends on many variables, $x_1,x_2,…,x_n$ (and their order has no influence ... which is a problem in some chemistry experimentation...).
	
	The mathematical modeling consists to find a function $f$ such that:
	
	that takes into account the influence of each single factor or combination of factors (interactions). 
	
	A conventional study method consists of measuring the response $y$ for several values of the variables $x_1$ o while leaving the other $n-1$ variables fixed. We then iterate this method for each variable.
	
	Therefore, for example, if we have $8$ variables and if we decides to give $2$ experimental values to each of them, we are led to make $2^8=256$ experiences.
	
	The use of an experimental design then gives a strategy in choosing the test methods. The success of experiments in research and industry is related to the need for business competitiveness: they allow quality improvement and cost reduction!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The method of of DoE was developed early in the century, in the 1920s, by Ronald A. Fisher (the same for man as for the Fisher Test!). It has grown considerably with the development of information technology and computing power that accompanies it.
	\end{tcolorbox}
	The treatment of results is made most of time using univariate or multivariate linear regression techniques (\SeeChapter{see section Numerical Method}) and that of analysis of variance (\SeeChapter{see section Statistics}).
	
	With the DoE, the goal is therefore to obtain the maximum information (but not all information!) with minimum of experiences (and therefore a minimum of cost) in order to model and optimize the studied phenomena.
	
	An experimenter who starts a study is interested in a quantity that he measure at each test. This quantity is named obviously the "\NewTerm{response}" or "\NewTerm{exogenous variable}\index{exogenous variable}" and is the quantity of interest. The value of this quantity depends on several variables. Instead of the term "variable" or "endogenous variable" we will use the word "\NewTerm{factor}\index{factor}". The value given to a factor to achieve a test is named a "\NewTerm{level}\index{factor level}". When studying the influence of a factor, in general we limit is variations between two bounds (yes we must have to stop one day and be reasonable...) respectively named "\NewTerm{lower level}\index{factor lower level}" and "\NewTerm{upper level}\index{factor upper level}".
	
	Obviously, when we have several factors $x_1,x_2,…,x_n$ they represent a point in $\mathbb{R}^n$ named "\NewTerm{experimental space}\index{experimental space}".
	
	The set of all possible values that a factor can take between the lower level and the upper level is named the "\NewTerm{domain of variation of the factor}\index{domain of variation of the factor}" or simply the "\NewTerm{factor domain}\index{factor domain}". The usage is to denote the lower level by $-1$ and the upper level by $+1$ when all the factors have only two levels, otherwise is customary to note the levels from $1$ to $n$ (number of levels). Of course, depending on the chosen notation, the expressions of the mathematical models must be adapted accordingly.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If the factors don't have all the same number of level we say that we have an "irregular experimental region" and the most simple corresponding design is then a complete (general) factorial design.
	\end{tcolorbox}
	So for example for a factor having an domain of variation between an upper level of $20\; [^{\circ}\text{C}]$ corresponds to $+1$ and a lower level of $5\; [^{\circ}\text{C}]$ corresponding to $-1$ we will have at the end of our study to transform all the experimental values in "\NewTerm{centered reduced units}\index{centered reduced units}" in which the $x_i$ must be used.

	Thus, we have two entry points $(20,5)$ and two outputs $(+1, -1)$. Any intermediate value is simply given by the equation of the line:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_units_normalization.jpg}
		\end{center}	
		\caption{Principle of construction of centered reduced units}
	\end{figure}
	The slope is obvious to obtain, to get $b$, we just have a simple linear equation with one unknown to solve:
	
	or (what remains the same):
	
	Therefore the transition form the non-normalized variables, denoted by $x$, to the normalized one, denoted by $X$, is then written:
	
	Therefore in the case of an $(+1,-1)$ output:
	
	and vice-versa:
	
	that is to say in the case of an $(+1,-1)$ output:
	
	The set of domains of all factors defines the "\NewTerm{study domain}\index{study domain}". This study domain is the experimental space chosen by the  to make his tests. A study, that is to say multiple well defined experiments, is represented by points distributed in the study domain.

	For example, for two factors, the study domain is an experimental plane surface in $\mathbb{R}^2$:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_plane_study_domain.jpg}
		\end{center}	
		\caption{Two normalized factors study domain example}
	\end{figure}
	The levels of the reduced centered $x_i$  represents the coordinates of an experimental point and $y$ is the answer of this point. The graphical geometric representation of a DoE and the answer need always one dimension more than the dimension of the experimental space. A DoE with two factors need therefore a three dimension space to be represented: one dimension for the answer, two dimensions for the factors.

	When we have determined the mathematical model, to each point of the study domain, corresponds then a set of answers that are located on a surface (or hypersurface) named "\NewTerm{response surface}\index{response surface}" (this is why typically DoE are named in the general case: "DoE for response surfaces") that look like for example with two factors:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_response_surface.jpg}
		\end{center}	
		\caption{Two centered reduce factors quadratic surface response}
	\end{figure}
	The number and the position of the experimental points is one of the fundamental problems of DoE. Indeed, we are looking for the best precision on the response surface by minimizing at the same time the number of experimentations. The practitioner will therefore look for a mathematical function that links the response variable to the factors.
	
	For this, we simplify the problem first by remembering (\SeeChapter{see section Sequences and Series}) that any continuous function, whatever is its number of variables, can be approximated in a sum of power series on a given point.

	We then take a limited Taylor development of a bivariate function (for example):
	
	Therefore at the neighborhood of $x_0=0,y_0=0$ (what we can do as we take the centered reduced variables... and this is also one of the reason we centered and reduced them!), we have the following second order Maclaurin serie by change notation accordingly to the traditions in DoE for $i=1,2$:
	
	where $y$ is the response and the $x_i$ the factors and the $a_0,a_i,a_{ij},a_{ii}$ the coefficients of the a priori adopted mathematical model. They are not known and must be calculated from the results of experiments.
	
	The benefit (pros) of modeling the response by a polynomial is to calculate afterwards all the answers of the study area without having to do the experiments thanks to a model named "\NewTerm{postulated model}\index{postulated model}" or "\NewTerm{prior model}\index{prior model}".
	
	The disadvantage (cons) of a polynomial model it is that it is not bounded (diverge or: non-lipschitz function) and have therefore not asymptotic answer. In some situation NURBS (or splines in the case of a univariate function) could be more accurate but sadly we do not know actually how to make statistical inference with NURBS  to determine which factors are significant or not...
	
	Two complements need to be made to the model described above:
	\begin{enumerate}
		\item The first complement is the "\NewTerm{lack of fit}\index{lack of fit}". This expression reflects the fact that prior model is likely almost surely different (only by the approximation of the approach) of the real model that governs the studied phenomenon. There is a difference between these two models. This gap is the lack of fit!

		\item The second complement is to take into account the random nature of the response (without the latter to be stochastic otherwise we have to use other tools!). Indeed, if we measure a response several times in the same experimental point, we do not get exactly the same result. The results are scattered. The dispersions of this kind are named "\NewTerm{experimental error}\index{experimental error}" or "\NewTerm{residuals}\index{residuals}" and are supposed to follow a supposed to follow a Normal law.
	\end{enumerate}
	These two differences, lack of fit and experimental error, are most of time merged into only one error denoted $e$ (or sometimes as in linear regression by $\varepsilon$).
	
	The model used by the experimenter will then be written at the second order term and first degree:
	
	and is sometimes named "\NewTerm{synergic model}\index{synergic model}" and in the particular case of two variables we speak of a " \NewTerm{hyperbolic paraboloid model}\index{hyperbolic paraboloid model}" typically represented by:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/doe_hyperbolic_paraboloid.jpg}	
		\caption{Generic example of interaction for two factors (hyperbolic paraboloid)}
	\end{figure}
	and as resqueste by a reader, an example of univariate functions (to differentiate the linear a non-linear case when variables transformations could be necessary):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/generice_univariate_functions.jpg}	
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If we stop the Taylor developement to the first order term and first degree (without interactions), we then speak of "\NewTerm{affine model}\index{affine model}".
	\end{tcolorbox}
	
	In practice we write the latter relation as following (we remove the error term for readiness purposes):
	
	where we have the abusive notations for what it is common to name in this field the "\NewTerm{rectangle term}\index{rectangle term}":
	
	Therefore it comes that for $2$ factors, the expression contains $4$ terms, for $3$ factors it contains $8$ terms, for $4$ factors it contains $16$ terms, and so on... There are always power of $2$!
	
	This model without error is many times named "\NewTerm{controlled (linear) model (or order $2$) with interactions}\index{controlled (linear) model with interactions}".
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For example, a response surface associated with a relation like the previous one looks like with Maple 4.00b:\\
	
	\texttt{>plot3d(5+3*x1+2*x2+4*x1*x2,x1=-10..10,x2=-10..10,\\
view=[-10..10,-10..10,-10..10],contours=10,style=PATCHCONTOUR,\\
		axes=frame,numpoints=10000);
		}
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/engineering/doe_hyperbolic_paraboloid.jpg}
		\end{center}	
		\caption{Generic example of interaction for two factors (hyperbolic paraboloid)}
	\end{figure}
	where we have represented the isolines (\SeeChapter{see section Functional Analysis}). This permits the practitioner to search the combination of explanatory variables that for a same result is the less expensive!\\

	Obviously if we remove the interaction terms we simply get a plane:\\
	
	\texttt{>plot3d(5+3*x1+2*x2,x1=-10..10,x2=-10..10,\\
	view=[-10..10,-10..10,-10..10],contours=10,\\
	style=PATCHCONTOUR,axes=frame,numpoints=10000);
	}
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.65]{img/engineering/doe_plane.jpg}
		\end{center}	
		\caption{Generic example of interaction for two factors (hyperbolic paraboloid)}
	\end{figure}
	\end{tcolorbox}
	Obviously, the higher the degree of the polynomial is, the higher the theoretic model will be close to reality. But (!!!) the high degree polynomials require a lot of data points and their validity can quickly diverge outside the experimental area (\SeeChapter{see section Calculus}). If the study requires it, we prefer to use specific mathematical functions to better fit the model to the experimental results.
	
	However, in practice, the higher order interactions often have very little influence on the response (in fact this assumption depends strongly on the industrial/economical field ...!). It is therefore possible to not to include them in the model, which leads to less trials. This principle is used in the construction of many experimental designs, as we shall see in the next subsection. In many applications, we obtain results quite satisfactory results by limiting ourselves to double interactions.
	
	Why do we satisfy us of this approximate relation with four terms? For the simple reasons that:
	
	\begin{enumerate}
		\item The answer may be non-zero when all factors are zero (thanks to the first coefficient $a_0$).

		\item The answer depends trivially (intuitively) of the sum of the effects of the first and second factors $x_1,x_2$ in an independent way (coefficients $a_1,a_2$).

		\item The answer also depends on the interaction between the two factors $x_1,x_2$ (coefficient $a_{12}$).
	\end{enumerate}
	Each experimental point for which the $x_i$ are given then provides a value of the response $y$. This response is modeled by a polynomial whose coefficients are the unknowns an must be determined and also categorized as significant or not.
	
	\subsubsection{Two levels factorial Designs}
	So in an experimental design of $2$ factors with $2$ levels, we need at least (and at most for cost reasons!) $4$ measurements (trials) to have a system of $4$ equations with $4$ unknowns which are the coefficients $a_0,a_1,a_2,a_{12}$ and such that the system has a unique solution (\SeeChapter{see section Linear Algebra})
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For a study of $2$ factors with $3$ levels, we can no longer take a linear model. Then we must take the quadratic terms of the Taylor expansion!
	\end{tcolorbox}	
	Since for each of the factors we need to set a low and a high level in order to work reasonably... so if we have two factors, we have an experimental space defined by $4$ points {(up, up), (down, down ) (up, down), (down, up)}, corresponding to the $2$ times $2$ levels ($2^2$), which is enough for us then to get our system of $4$ equations with $4$ unknowns and then determine the $4$ coefficients (uniquely).
	
	Thus, the points to be taken in our experience naturally correspond to the vertices (geometrically speaking) of the experimental space!
	
	We then have the following system of equations (recall that this approach works only for $2$ level factors, the case with $3$ levels will be study much further below with an example):
	
	or explicitly:
	
	As we are interested about the coefficients (and that the values of the variables are known!), the problem simply consist in solving a linear system (\SeeChapter{see section Linear Algebra}).
	
	If the variables were not coded, we would have for example for a (two-level) complete factorial design with one variable (velocity of car) having for lower value $40 \;[\text{km}\cdot\text{h}^{-1}]$ and for upper value  $50 \;[\text{km}\cdot\text{h}^{-1}]$, and for a second variable (wheel pressure) having for lower value $1.5\;[\text{Pa}]$ and for upper value $3\;[\text{Pa}]$ the following system to solve knowing that we have drive respectively for each combination the distances $32700, 32680, 31710, 33222\;[\text{km}]$:
	
	Hence in matrix form:
	
	By solving this system by hand (\SeeChapter{see section Linear Algebra}), or with a calculation software (spreadsheet or statistical software), we get:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_full_factorial_excel_values.jpg}
		\end{center}	
		\caption[]{Putting in equation and implicit resolution of the FF DoE in Microsoft Excel 14.0.6123}
	\end{figure}
	thus explicitly:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.7]{img/engineering/doe_full_factorial_excel_formulas.jpg}
		\end{center}	
		\caption[]{Putting in equation and explicit resolution of the FF DoE in Microsoft Excel 14.0.6123}
	\end{figure}
	Therefore the solution is finally:
	
	thus exactly the same coefficients as those given by a specialized software like Minitab 15.1.1 (see the companion Minitab book).
	
	Going back to our system with the coded variables, it then comes when solving the system algebraically (relations valid if and only if the variables are coded!):
	
	which can be written in the following matrix form:
	
	What is written in a more general way for linear models for the second order under the following form (\SeeChapter{see section Linear Algebra}):
	
	The matrix $X$ containing $2^n$ rows is named "\NewTerm{full factorial design (FFDoE) with $2^n$ interactions}\index{full factorial design with $2^n$ interactions}" (the term "\NewTerm{factorial}" coming from the fact that all factors vary simultaneously).
	
	The matrix $X$ in practice is named the "\NewTerm{experimental matrix}\index{experimental matrix}" or "\NewTerm{effect matrix}\index{effect matrix}" and is often represented as follows in the previous case:
	
	But we see immediately that in practice, the second column ("Rest") is unnecessary because it is always $+1$ and it is also only implicitly implemented in statistical softwares.
	
	It is the same for the fifth column (Factor 12), because it is automatically deductible from the third and fourth columns (it is equal to the multiplication of row elements... what some practitioners name the "\NewTerm{Box multiplication}\index{Box multiplication}").
	
	The reader will also notice that by passing from one column to another or from one row to another, there are always two factors that have the level that change! By cons, if we focus only on the columns Factor 1 and Factor 2, we see that from one row to another, there is one factor that changes at a time.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Once again notice that the first column has only $+1$ and the there is also always at least on row with only $+1$ values!
	\end{tcolorbox}
	Thus, in practice (softwares) and in many books, we represent rightly only the following reduce form table (which hides the fact that we are dealing in reality with a square matrix):
	
	for a $2$ factors experimental design with $2$ levels with interactions in linear model (no errors) we have even more extreme ("\NewTerm{Yates notation}\index{Yates notation}") ... in terms of writing:
	
	We see better in this form of writing that in addition to the fact that the two columns Factor $1$ and Factor $2$ are orthogonal (the norm ISO 3534-3: 1999 speaks of "\NewTerm{orthogonal contrast}\index{orthogonal contrast}"), they are also "\NewTerm{balanced}\index{balanced design}" in the sense that there are so many $+$ and $-$ in each column.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	By default, most software randomize the order of testing of the design (whether it is complete, fractional, factorial or not). Generally, it is recommended to randomize the order of testing to mitigate the effects of factors that are not included in the study and that parasitize it ("\NewTerm{nuisance factor}\index{nuisance factor}"), including effects related to the time. This is, we don't know that the factor exists and it may even be changing levels while we are conducting the experiment. However, in some cases, randomization does not produce a sequence of interesting essays and perhaps even dangerous because it can mask certain sources of systematic errors which were not identified before the experiment. For example, in industrial applications, modifying factor levels may be difficult or expensive. It is also possible that after the change done, the return to a steady state of the system takes a lot of time. In such cases, it may be desirable not to randomize the design to minimize the level changes. Also they are some cases where the nuisance factor is know but uncontrollable, but then we can compensate it by using the analysis of covariance (\SeeChapter{see section Statistics}).
	\end{tcolorbox}
	Practitioners appreciate to calculate the average of the answers and the average effect of a given factor since the system is linear. Thus, in at the level $+1$ for the factor $x_1$, by remaining on the linear system:
	
	then we can build and define the "average answers" given by:
	
	and we have the same with the level $-1$ for the same factor:
	
	We then the "\NewTerm{overall effect}\index{overall effect}" of the factor $x_1$ that will be given by:
	
	and the "\NewTerm{mean effect}\index{mean effect}" of the factor $x_1$ that is therefore define by the semi-difference between the average of the answers at the upper level of the factor $x_1$ and the average answers at the lower level of the same factor:
	
	but after some simplification of elementary algebra, it comes quickly that:
	
	It is obvious that if the overall effect (and verbatim the mean effect) is not zero, we can doubt that there is an interaction between the factors as the variation in amplitude of the response is not the same depending on the value of the level of the other factor (see the study of the analysis of variance of two factors in the Statistics section for more details). Obviously, in practice, the study of interactions will be made, as often for ANOVA, also with graphics (interaction diagrams).
	
	The average of all responses thus gives the value of the response at the center of the experimental domain:
	
	By doing some elementary algebra, that the plane is written in normalized and centered form or not, this expression reduces to:
	
	Obviously, we can summarize this simple case graphically if this can help the reader:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_mean_effect.jpg}
		\end{center}	
		\caption[]{Two factors with the response surface}
	\end{figure}
	and with the figure above, the changes being not very parallel when a factor is set, we can assume that there is interaction between the factors and thus this will require the use of an ANOVA to have a more in-deep numerical analysis.
	
	When the number of factors is large, it is not always easy for everyone to put the $+,-$ without the help of a software. So there is a small procedure named "\NewTerm{Yates algorithm}\index{Yates algorithm}" or "\NewTerm{Yates and Hunter algorithm}\index{Yates and Hunter algorithm}" which enables to reach very fast the desired result for factorial designs (whose factors have two levels) whose number of factors is a power of $2$: first, we start all columns by $-1$ and after we alternate the $-1$ and $+1$ all the $2^{j-1}$ rows for the $j$th row.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} If the type of table above contains encoded values, we speak of "\NewTerm{experimental design}\index{experimental design}" if not with the usual physical units we speak of "\NewTerm{experimental table}\index{experimental table}".\\
	
	\textbf{R2.} In the case of coded tables, it is customary to indicate under the table a second table with the correspondences between the coded units and physical units.
	\end{tcolorbox}
	Let us insist on one important thing: If we had $3$ factors with $2$ levels each, then we have $2^3$ possible experiences opportunities (therefore $8$). But the number $8$ matches exactly with the number of coefficients that we also have in the linear model with interactions of a response with three variables:
	
	which also corresponds also only to the linear terms and condensed form of the Maclaurin series expansion of a function $f$ of three variables for recall.

	And so on ... for $n$ factors with two levels each. This is why the linear full factorial designs $2^n$ are traditionally the most used because they are mathematically intuitive and simple to prove and develop as many results simplifies at the opposite of the general factorial designs!

	\pagebreak
	\paragraph{Replicated full factorial designs}\mbox{}\\\\
	Replicated full factorial designs are important as the give (among others that we will see during a companion example for the study of general factorial designs\footnote{especially the calculation of an ANOVA}) is the ability to calculate the confidence intervals of the effects.
	
	As a companion example let us consider the $2^3$ factorial design below replicated twice:
	
	Or in more complete form:
		
	We will assume to follower underlying model:
	
	The global average is:
	
	The average of all responses at high level of $x_1$ is equal to:
	
	the average of all responses at bottom level of $x_1$ is equal to:
	
	So the overall effect of the $x_1$ is equal to:
	
	Or equivalently using in one shot only all the $y_{i1}$ and $y_{i2}$:
	
	or using only the averages $\bar{y}_i$:
	
	And so on:
	
	Finally we get:
	
	Now the question that interest us here are:
	\begin{itemize}
		\item Which of these effects are important?
		\item Which of these effects are distinguishable from the noise in the experimental environment?
	\end{itemize}
	Let's say we run experiment numerous times. Sample effects, $E_{Gx_i}$ will be supposed to be Normally distributed. We can obviously do a Normal probability plot (Henry plot) of the measurements but here we will focus rather on the calculation of a confidence interval as it is more easy to communicate in corporations.

	So our experiment is replicated, we calculate a sample variance for each run:
	
	Then we get the following table:
		
	We now assume that the standard deviation is constant for all runs (very strong assumption!) and given by the pooled sample variance:
	
	So obviously we can generalize:
	
	The reader must keep in mind that the null hypothesis we would like to reject is:
	
	and under that null hypothesis they would follow a Normal distribution (so if we reject that one we will be quite happy as the effects are significantly non-zero).
	
	We may, based on each calculated effect, develop a confidence interval for the true mean effect:
	
	and we know (\SeeChapter{see section Statistics}) that this is written explicitly:
	
	Therefore in our example we have for the average:
	
	Numerically:
	
	That is to say:
	
	shortly:
	
	Since $0$ is not in the confidence interval, we are happy to reject $H_0$ that $\mu_\text{Average}=0$.
	
	Now let us calculate the confidence interval for the other effects. We know that we calculate effects generally by:
	
	Each bases on $N/2$ observations ($N/2=8$ for our example). We have obviously:
	
	with obviously:
	
	where $r$ is the number of replications. Therefore:
	
	Bus as we assume for all runs the standard deviation:
	
	Therefore:
	
	Hence:
	
	So finally:
	
	sometimes named the "\NewTerm{}\index{standard error of an effect}".
	
	For our example:
	
	Therefore:
	
	Now we get easily for each effect the following confidence interval:
	
	So in our example:
	
	Numerically:
	
	Hence:
	
	So finally
	
	So $0$ does not line on the confidence interval for $E_{Gx_1}$, $E_{Gx_2}$ and $E_{Gx_{13}}$. We now know that only $a_0$, $a_1$, $a_2$ and $a_{13}$ are not noise and therefore the fitted model becomes:
	
	That is to say:
	
	For comparison we can see the special way on how Minitab 17.1.3 show these same results:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=1]{img/engineering/replicated_design_minitab_analysis.jpg}
		\end{center}	
		\caption[]{Replicated factorial design effects C.I. with Minitab 17.1.3}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	What Minitab denote by the $T$-value is the ratio:
	
	That is to say the value that should take the $T$ distribution for not rejecting the null hypothesis.\\
	
	So for example for the factor $A$ we get:
	
	and the column $p$-value is as always (\SeeChapter{see section Statistics}) the corresponding percentile of the $T$-value.
	\end{tcolorbox}
	And finally for those who are interested to see the Henry plot of the effect:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=1]{img/engineering/replicated_design_henry_plot_minitab.jpg}
		\end{center}	
		\caption{Henry plot of the replicated factorial design with Minitab 17.1.3}
	\end{figure}
	
	\pagebreak
	\paragraph{Plackett-Burman Designs}\mbox{}\\\\
	It is important to notice that all these full linear designs approximated to the second order are in matrix form orthogonal square matrices $M_{n}$ and therefore of course can be inversed (\SeeChapter{see section Linear Algebra})!
	
	Also the previous matrices do not satisfy the following relation as seen in the section of Linear Algebra (scalar product of basis-columns with themselves):
	
	where for recall $\mathds{1}$ is a matrix with only $1$ in the diagonal and $0$ everywhere else, but have the particularity for all full experimental design to satisfy the relation:
	
	So unlike orthogonal matrices, which by definition have all the columns (or rows) that form an orthonormal base (unit norm), the experimental designs matrices have the particularity of not having the norm orthogonal basis vectors equal to the unit.
	
	\textbf{Definitions (\#\mydef):}
	Thus, we define the matrix whose components are all $+1$ or $-1$ and satisfying the previous relation:
		
	as a "\NewTerm{Hadamard matrix}\index{Hadamard matrix}" denoted sometimes $H$. 
	
	Put in another way, a $(+1,-1)$-matrix is Hadamard if the inner
product of two distinct rows is 0 and the inner product of a
row with itself is $n$.

	A few examples of Hadamard matrices are:
	
	It is quite apparent that if the rows and columns of an Hadamard matrix are permuted, the matrix remains Hadamard. It is also true that if any row or column is multiplied by $-1$, the Hadamard property is retained.

	\begin{theorem} 
	Hadamard matrices have the property to exist only for orders $1$, $2$, $4$, $8$, $12$, $16$, $20$, $24$, ... That is to say only for the multiple for the orders that are multiple of $4$ or written differently:
	
	 (if we omit the case $1$ and $2$ that are obvious).
	\end{theorem}
	\begin{dem}
	Knowing that the case of $1$ and $2$ are trivial and that the odd case must immediately intuitively be removed (try and you will see almost very quickly...), we will do the proof for $n\geq 4$.
	
	Since all the columns are mandatory orthogonal (so that the matrix is invertible and therefore the system uniquely solvable) and from the fact that the rows and columns of an Hadamard matrix are permuted or that if any row or column is multiplied by $-1$ the Hadamard property is retained, it is therefore always possible to arrange to have the first row and first column of an Hadamard matrix contain only $+1$ entries. An Hadamard matrix in this form is said to be "normalized":
	
	The reader must obviously consider that in the above system we can consider two case in facts:
	\begin{enumerate}
		\item It is a system that represents indeed an order $4$ matrix, so $x$, $y$, $z$ and $w$ have to be replaced in its mind by the value $1$

		\item It is a system that can represents only the $4$ first rows of a matrix of order $n$ greater $4$ (as all the other rows are equal to zero base on the same principle we have the habit to not represent them), so $x$ must therefore be in the reader mind replace by a given number of $1$ ($x=1\ldots 1$), same for $y$, $z$ and $w$. More explicitly (for example for $n=8$):
		
		as we can always normalize a Hadamard matrix to get such as system and that the Hadamard properties still remain valids.
	\end{enumerate}
	Solving the system gives:
	
	therefore $n$ must be divisible by $4$ for $n \geq 4$ if we want the number of terms in $x$, $y$, $z$, $w$ to be an integer!
	
	The result can seems not obvious but if the reader has the time... he can try for example to construct a Hadamard matrix of order $n=6$ and he will quickly see why it is not possible.
	\begin{flushright}
		$\square$  Q.E.D.
	\end{flushright}
	\end{dem}
	It then follows trivially the following relation (with a very abusive notation because it omits the notation of the unit matrix):
	
	However, we will see a little further below in our study of fractional factorial designs that Hadamard matrices of order $1$, $4$, $8$, $16$, $32$, ..., $2^n$ are almost never used under the expression "Hadamard matrices" as they are confused with the fractional factorial designs.
	
	By cons, it is interesting to observe that we have the potential existing design of experiments with $12$, $20$, $24$, $28$, ... trials (in other words:  the number of runs is a multiple of $4$ but not a power of $2$) that have some interest when the number of factors is greater than or equal to $4$ (some software like JMP - in 2012 - however don't propose these designs if the number of factors is less than $5$). These designs are named "\NewTerm{Plackett-Burman Design of Experiments}\index{Plackett-Burman design}" and sometimes "\NewTerm{Irregular Plackett-Burman Design of Experiments}\index{Irregular Plackett-Burman design}" or more rarely "\NewTerm{Non-geometric Plackett-Burman Design of Experiments}\index{Non-geometric Plackett-Burman Design}" (a software like Minitab propose however arbitrarily - and indicate it explicitly in the shoftware Help - a Plackett-Burman design of order $32$ and Plackett-Burman designs for $2$ or $3$ factors).

	 Robin L. Plackett and J. P. Burman have try with algorithms and by trial and error the expression of matrices corresponding  experimental designs that carry their name and which therefore contain $12$, $20$, $24$, $28$, ... trials. They have proposed a very useful tip so that practitioners can create these experimental designs without software. Let us do to start an example  with a Hadamard matrix of order $12$ (without focus for the moment on the number of factors that we use). The Plackett-Burman tables for the special $12$ order give only the first row (in fact... it is a column in the matrix):
	 
		Then we build the following table of experiment by following the algorithm proposed by Plackett and Burman (software use the same as far as we know):
	\begin{enumerate}
		\item First step, we put the first line as a column in a $12\times 12$ table:
		 
			
		
		\item Second step: we deduce the second column from the first column by shifting the signs a step to the bottom, the last "$-$" sign being ascended to the top of the second column (it is therefore equivalent to a circular permutation):
			
		and so on until the $11$th column:
		
		
		\item Last step, we add a row and a column of signs "$-$":
		
	\end{enumerate}
	The reader can easily verify that each row or column taken in pairs are orthogonal.
	
	Now arises the question of whether we choose to associate for example this design of experience to a $4$ factors design (meaning reduced centered design of course!):
	
	to reduce the number of tests from $16$ to $12$, which column should we associate to what? Or going further: for how many $2$-level factors can we associated this type of design having $12$ trials???
	\begin{enumerate}
		\item The first one is to say that the Plackett-Burman designs of order $n$ should only be used for the study of main effects (so no interaction is taken into account) of $n-1$ factors. Thus, a Plackett-Burmann design of order $12$ will be reserved for a study of $11$ factors and only of their main effects (purely additive linear model).

		\item The second one is to say that the Plackett- Burman design contain a concept that we will see further and who's named "\NewTerm{aliases}\index{aliases}" (Plackett-Burman designs are all DoE of resolution III of the second order). Therefore, we have to use this type of design when we are ready to consider initially that the interactions with two factors are very significant.
	\end{enumerate}
	The complexity of the alias with the Plackett- Burmann design makes that in practice they are rather used in the first religion release... by beginners and in the second by consultants.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A software like Minitab 15.1.1 does not display all the aliases used in the implementation of Plackett-Burmann designs.
	\end{tcolorbox}
	To conclude this section, let us summarize with simple observation:
	
	So using full factorial designs, the user is sure to have the optimal experimental procedure since these plans are based on Hadamard matrices and we have proved that we could not do better.
	
	\pagebreak
	\paragraph{Fractional Factorial Designs}\mbox{}\\\\
	In practice, full factorial designs can only be used on systems with very few factors, or when each test takes very little time. When $n$ is greater than or equal $3$ then experiences costs can quickly become expensive.

	The smallest case where it is interesting to optimize the number of tests is the one consisting of $3$ factors with $2$ levels each. We then have the following equation and experience table:
	
	
	Either as full experiment table:
	
	or in matrix form:
	
	where once again it is easy to control that all columns are orthogonales and balanced (same number of $+$ or $-$ in each column, or in other words the sum of each column - excepted the first one - is equal to zero) and that the matrix is a Hadamard matrix.	 Therefore the full factorial design for $3$ factors needs $8$ trials.

	We can also represent his in the following form:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.9]{img/engineering/doe_full_factorial_design_scheme_three_factor.jpg}
		\end{center}	
		\caption{Traditional representation of a full factorial design with $3$ factors}
	\end{figure}
	and the corresponding $2$-way interactions in the following form (typical of ANOVA studies):
	\newcommand\drawplane[2]
	{%
	    \draw
	    [
	        thick,
	        opacity=.6,
	        draw=#2,
	        fill=#2!60,
	    ] #1 -- cycle;%
	}
	
	\newcommand\drawonecase[4]
	{
	    \begin{tikzpicture}[scale=2]
	
	        \tikzset
	        {
	            edgevis/.style={black},
	            edgehid/.style={dashed,black},
	        }
	
	        \def\vertexradius{.7pt}
	
	        \coordinate (OOO) at (0,0);
	        \coordinate (OOI) at (xyz cs:z=1);
	        \coordinate (OIO) at (xyz cs:y=1);
	        \coordinate (OII) at (xyz cs:y=1,z=1);
	        \coordinate (IOO) at (xyz cs:x=1);
	        \coordinate (IOI) at (xyz cs:x=1,z=1);
	        \coordinate (IIO) at (xyz cs:x=1,y=1);
	        \coordinate (III) at (xyz cs:x=1,y=1,z=1);
	
	        \drawplane{#1}{#2}
	        \drawplane{#3}{#4}
	
	        \draw[edgevis] (OOI) -- (OII) -- (OIO) -- (IIO) -- (IOO) -- (IOI) -- cycle;
	        \draw[edgevis] (III) -- (IIO);
	        \draw[edgevis] (III) -- (IOI);
	        \draw[edgevis] (III) -- (OII);
	        \draw[edgehid] (OOO) -- (OOI);
	        \draw[edgehid] (OOO) -- (OIO);
	        \draw[edgehid] (OOO) -- (IOO);
	
	        \draw (OOO) circle (\vertexradius);
	        \draw (OOI) circle (\vertexradius);
	        \draw (OIO) circle (\vertexradius);
	        \draw (OII) circle (\vertexradius);
	        \draw (IOO) circle (\vertexradius);
	        \draw (IOI) circle (\vertexradius);
	        \draw (IIO) circle (\vertexradius);
	        \draw (III) circle (\vertexradius);
	    \end{tikzpicture}
	}
	\begin{figure}[H]
	\centering
    \begin{subfigure}[b]{\textwidth}
    	 \centering
        \begin{tabular}{ccc}
            \drawonecase
                {(OOO) -- (OOI) -- (OII) -- (OIO)}{red}
                {(IOO) -- (IOI) -- (III) -- (IIO)}{blue}
            &
            \drawonecase
                {(OOO) -- (IOO) -- (IIO) -- (OIO)}{blue}
                {(OOI) -- (IOI) -- (III) -- (OII)}{red}
            &
            \drawonecase
                {(OOO) -- (IOO) -- (IOI) -- (OOI)}{red}
                {(OIO) -- (IIO) -- (III) -- (OII)}{blue}
        \end{tabular}
        \caption[]{Main effects $A$, $B$ and $C$}
    \end{subfigure}
    \par
    \vspace{1em}
    \begin{subfigure}[b]{\textwidth}
    	 \centering
        \begin{tabular}{ccc}
            \drawonecase
                {(OOI) -- (OII) -- (IIO) -- (IOO)}{blue}
                {(OOO) -- (OIO) -- (III) -- (IOI)}{red}
            &
            \drawonecase
                {(OII) -- (OIO) -- (IOO) -- (IOI)}{red}
                {(OOI) -- (OOO) -- (IIO) -- (III)}{blue}
            &
            \drawonecase
                {(OOI) -- (IOI) -- (IIO) -- (OIO)}{blue}
                {(OII) -- (III) -- (IOO) -- (OOO)}{red}
        \end{tabular}
        \caption[]{Two-factor interactions $AB$, $AC$, $BC$}
    \end{subfigure}
    \caption[]{Geometric presentation of contrast (in each case, high levels are highlighted in blue, low levels in red)}
	\end{figure}
	A "\NewTerm{reduced design}\index{reduced design}", more commonly named "\NewTerm{fractional factorial designs  (FFD)}\index{fractional factorial designs}" or  "\NewTerm{screening plans}\index{screening plans}" (following the norm ISO 3534-3:1999), consist in selecting some combinations have therefore been proposed. They naturally gives the opportunity to reduce the experimental costs, but also decrease the information available on the system! We must therefore be sure of the adequate choice relatively to the model to identify!
	
	A first elementary method is to do the hypothesis that there is no interaction. Therefore our function is reduce to a purely additive mode:
	
	and to resolve thy system, $4$ trials are enough. We therefore go from a $8$ trial design to a $4$ trial design just by supposing that there are no interactions and therefore we fall back on a simple target design.
	
	To reduce the experimental costs by keeping the interactions implicitly, we can play with mathematics stuff. First let us take the actual problem with a full factorial design of $3$ factors into explicit form:
	
	Can we reduce the writing of this system to reduce the trials to do? The answer is: Yes! But in counterpart we will lost the measurement of pure effects (we therefore sometimes speak of "\NewTerm{confusion}\index{confusion}" or of "\NewTerm{confounding}\index{confounding}").
		
	The inferior nearest writting is the Hadamard matrix of order $4$. This means obviously that we need to conserve $4$ rows on the $8$ and that these $4$ rows must remain orthogonal, balanced and must satisfy the relation:
	
	The idea, named the "\NewTerm{Box and Hunter method}\index{Box and Hunter method}" and that works only for design with two level factors, is in a first time to merge together the influential factors (in indices) such that (the developments are similar for any $n$):
	
	The grouping choice is also made so that the interaction terms that are supposed to be negligible by the centered reduce normalisation are merged ("confused") with a coefficient of a main factor supposed as not negligible!! Therefore it is also logic that in each grouping we never found indices with a number of a main factor (as merging two main factors that are not negligible will be very bad!).
	
	We then say that we have an "\NewTerm{alias structure}\index{alias structure}" (the norm ISO 3534-3:1999 name this a "\NewTerm{concomitance}\index{concomitance}" when it is the practitioner that force the choice of the regroupement and "\NewTerm{alias}\index{alias}" if the regroupement choice is due to the nature of the experience) of the type:
	 \begin{center}
	 \texttt{0+123;1+23;2+13;3+12}
	 \end{center}
	 and if we write this in the same way as many statistical software, this give (this is exactly the alias given by a software like Minitab 15.1.1):
	\begin{center}
	\texttt{I+ABC;A+BC;B+AC;C+AB}
	\end{center}
	Also some practitionner writhe this (as \texttt{I} must always have a positive sign):
	\begin{center}
	\texttt{I=ABC;A+BC;B+AC;C+AB}
	\end{center}
	Let us write the previous system as following:	
	
	Let us change our notations:
	
	Naturally, if we consider this new notation as new variables, this unique system can now be splitted into two sub-systems (the grouping being named "\NewTerm{contrasts}\index{contrasts}" in this field but this is not corresponding to the vocabulary of the norm ISO 3534-3) to be solvable:
	
	which gives the possibility to divide the number of trials by $2$ relatively to the initial full factorial design!!!! 

	By solving of these two systems, we we say that the interactions are "\NewTerm{aliased}" (also said in "\NewTerm{confusion}\index{confusion}") with the pure effects in negative or in positive (in the present case, some main effects are aliased with two factors interactions).
	
	It is afterwards the tradition to keep only the positive aliased system:
	
	because if the interactions are zero, we fall back on the same experience of the matrix of a full $2^2$ factorial design! The approach therefore leads to select only the runs $2$, $3$, $5$ and $8$, which gives the possibility to divide the number of tests by two compared to a full factorial design. Thus, the fractional factorial design of an experiment with three factors may be reduced to $4$ runs with this method. The fractional factorial design above will naturally be represented by the following experimental matrix:
	
	
	Or more visually:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/full_factorial_design_vs_fractional_factorial_design.jpg}
		\end{center}	
		\caption{Visual $2^2$ factorial design and fraction factorial design}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We have seen that the $8$ runs $2^3$ experiment above is splitted into $2\times 4$ fractional experiments. In a software like Minitab (see companion book) the practitioner can choose if he wants to use the first fraction or the second one in the options of the design builder.\\ 
	
	For a fraction DoE splitted for example into $4$ parts the practitioner can therefore choose in Minitab $4$ possible fractions (but they are obviously equivalent!!!).
	\end{tcolorbox}
	There is however a problem: Even if the triple interaction is really equal to zero, it can remain up to $7$ other coefficients in the model, while we have only $4$ trial results to identify them. In other words, unless we know a priori that at least $3$ of these coefficients are equal to zero (to reduce the problem to $4$ equations with $4$ unknowns), we will get at best only relations between the coefficients and the rigorous identification will therefore be impossible. Thus, it is not possible to indefinitely reduce the cost of an experimental design without degrading its robustess (information it provides).
	
	It is important to notice that in the fractional factorial design above, the third factor is confused (is aliased) with the interaction $12$ of factors $1$ and $2$. We name this the "\NewTerm{initial alias}\index{initial alias}" or "\NewTerm{alias generator}\index{alias generator}" and we can notice by reiterating the calculations for factorial designs with $4$, $5$, $6$... factors that the generators helps us to immediately identify the trials (runs) to preserve (you can control when using softwares like JMP or Minitab that they don't let you choose the alias generator, these softwares take by default the theoretical one). For example, the generator ("alias" or "confusion") of the above experimental table will be written by the tradition:
	
	\begin{center}
	\texttt{C=AB}
	\end{center}
	
	This does not mean that the coefficient of the model are equal but simply that the design is unable to separate the analysis of these two entities. We must therefore pay a particular attention to the interpretation of the respective coefficients!
	
	We have just seen that a $2^{(3-1)}$ design (resolution \texttt{III}) has the serious drawback of confusing a main factor with an interaction of order $2$. A $2^{(4-1)}$ (resolution \texttt{IV}) design has the only disadvantage of confusing (aliazing) a main factor with an interaction of order $3$ and two interactions of order $2$. A $2^{(5-1)}$ design has even lower drawbacks. It is for this reason that the factorial theory uses the concept of "resolution". The higher the resolution, the greater the design is accurate.
	
	\textbf{Definitions (\#\mydef):}\index{interactions}
	\begin{enumerate}
		\item[D1.] When no main effect is aliased with another main effect, but the main effects have aliases with interactions of $2$ factors, we speak then of ""\NewTerm{resolution III designs}\index{resolution III designs}". From a practical point of view, resolution III designs are mainly intended to allow exploratory research because they allow to explore a number of factors with a certain economy. Even if the fail to obtain a sufficiently accurate model they possibly can eliminate a lot of factors in first time and then reduce the number of trials (runs).

		\item[D2.] When no main effect has alias with another main effect or another $2$ factors interaction, but some of the $2$ factors interactions have aliases with other $2$ factor interactions  and some main effects have aliases wither $3$ factor interactions, we speak of "\NewTerm{resolution IV designs}\index{resolution IV designs}".

		\item[D3.] When no main effect or interaction of $2$ factors have alias with another different main effect or another $2$ factors interaction, but $2$ factors interactions have alias with $3$ factors interactions and main effects have alias interactions with $4$ factors interactions we speak of "\NewTerm{resolution V designs}\index{resolution V designs}".
	\end{enumerate}
	and so on...
	
	This concept of resolution is important. We find it in programs like Minitab 15.1.1 in the selection of fractional factorial designs with the most common known alias known under the name "\NewTerm{minimum aberration designs}\index{minimum aberration designs}":
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_fractional_design_selection_minitab.jpg}
		\end{center}	
		\caption{Display of fractional factorial designs resolutions selector in Minitab 15.1.1}
	\end{figure}
	or with the Design Expert software (part of the picture but is more explicit than Minitab 15.1.1):
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_fractional_design_selection_designexpert.jpg}
		\end{center}	
		\caption{Display of fractional factorial designs resolutions selector in Design Expert}
	\end{figure}
	Thus, the reader will observe that a full factorial design of $5$ factors with 32 trials (runs) can be reduced to $16$ trials by bringing together the influential factors in pairs or $2$-tuples (hence the division by $2$ of the number of tests) , or $8$ tests by bringing together the influential factors by $4$-tuples.
	
	Then it's the job of the experimenter to know well its analysis and whether:
	\begin{enumerate}
		\item Among the aliased factors if there are interactions or not!

		\item In the aliased factor, the strong influence on the answer comes from the interaction or from the pure effect alone!
	\end{enumerate}
	Once determined coefficients, assuming that each of the factors or interactions is independent (acceptable limit hypothesis ...) some engineers are analyzing the variance of the regression line obtained final or determine the correlation coefficient to whether the linear approximation of the model is acceptable in the field of study and application.
	
	Regarding the generators of fractional factorial designs here is a non-exhaustive summary table:
	
	Of course, use of fractional design is an economic (and temporal) bet. If the conclusions are clear, then we have saved time and reduced our effort. But sometimes we lose the bet. We can therefore use a "\NewTerm{complementary design}\index{complementary design}" the consists of adding in a pertinent way enough lines to the initial design to unaliased some desired coefficients based on the choice of the alias generator.
	
	Before moving on to another type of design, let us come back on to the fractional factorial design with $3$ factors and therefore with $4$ trials for educational reasons. Consider that we have done these $4$ trials, and for each, we got a measurement as given in the figure below:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/doe_real_three_factors_factorial_design.jpg}
		\end{center}	
		\caption{Representation of a real fractional factorial design with $3$ factors}
	\end{figure}
	We want to show to the reader how not determine the coefficients by calculations (its all about solving a simple linear system and then there will be an example about its it a little further below) but how to calculate the effects in the context of this particular case?

	Indeed, the effect of $x_1$ is:
	
	and this of $x_2$ is:
	
	and this of $x_3$ is:
	
	The effect of interaction is somewhat subtle in the case of $3$ factors using the figure above (we will see in the example further below with a picture that with a table it is much more intuitive). Thus, we have for the interaction $x_1x_2$:
	
	and for $x_1x_3$:
	
	and for $x_2x_3$:
	
	
	For the triple interaction $x_1x_2x_3$ with the figure above it is also not obvious (it is a little more intuitive with a table as we will see in the example further below). We have:
	
	
	\pagebreak
	\subsubsection{General factorial Designs}
	In this book we name "\NewTerm{general factorial design}\index{general factorial designs}" designs that have a given quantity of factors but with more than $2$ levels and that are not reduced. 
	
	Given $n$ the number of factor, $l_i$ the number of levels of each factor. The number of runs $N$ of a general factorial design is given obviously:
	
	As we will see below, many of the techniques relatives to complete factorial design (designs with only $2$ levels for recall) applies identically to general factorial design. The main differences that surprise people starting to work in the field of DoE are:
	\begin{itemize}
		\item The final regression model don't have a unique coefficient for each factor therefore we cannot do a statistical regression analysis.
		
		\item The calculation methods of the standard error of the coefficients we have seen for the $2$ factorial design cannot be applied (we have to use another methods of calculations)
	\end{itemize}

	Rather than doing a long theoretical introduction to general factorial designs, let us use a companion example that will show us that in fact most concepts that we have learned so far also applies!
	
	As part of the study of tires, the criterion retain by a company is the longevity (number of kilometers traveled before the tire blow up) and the chosen factors are the type of use (city or highway), average speed ($40$ [km/h] or $50$ [km/h]) and the inflation pressure ($1.5$ or $2$ or $2.5$ [kg]).\\
	
	Therefore we have a general factorial design with the following number of runs:
	
 	Furthermore, it is more healthy to repeat the experiments (to analyze the dispersion of the values in order to run an ANOVA), so to make $24$, $36$, etc. experiments or to make some central points measurements.\\

	We will denote as usual $y$ the variable of interest and in this case $x_1,x_2,x_3$ the variables factors. We are seeking a simple model in the form:
	
	As each factor can have several levels, for will have for example the value of $y$ at the level $2$ of factor $1$ and at level $1$ of factor $2$ and at level $3$ of factor $3$ that will be denoted:
	
	
	The full factorial design is the presentation of the various experiences (measurement) depending on the levels of the variables. For example, to study the tires we will have:
	\begin{itemize}
		\item For the factor 1, the place, denoted $x_1$: city$=1$, town $=2$

		\item For the factor 2, the speed, denoted $x_2$: $40$ [km/h] $=1$, $50$ [km/h] $=2$

		\item For the factor 3, the pressure, denoted $x_3$: $1.5$ [kg] $=1$, $2$ [kg] $=2$, $3$ [kg] $=3$
	\end{itemize}
	The experimental matrix will then be (this is exactly the same as that generally obtained with the software Minitab: a full complete general design for $2$ factors with $2$ levels and $1$ factor with $3$ levels without randomization):
	
	At this table, we can associate the results of the experiments. For example, for the tires, we did three tests (three "\NewTerm{replication}\index{replication}") for each of the levels and we got:
	

	Thanks to these values we can calculate the effects of the various factors. For this we start with principle that their average (effect) is zero for a given factors. Therefore the global arithmetic average (that can be calculated as the arithmetic average if and only the variables are code):
	
	is the constant coefficient of the model (intercept):
	
	The effect of $x_1$ at the level $1$ is the variation of this average when we consider only the cases where $x_1$ is at the level $1$. The arithmetic average becomes $32955.8$ and therefore the effect is:
	
	We will write this obviously as we already know:
	
	At level of of $x_1$, the average becomes $33282.7$, therefore the effect of $x_1$ at level $2$ is equal to:
	
	The difference:
	
	is named the "\NewTerm{effect}\index{effect}" of the factor.\\
	
	We notice that as assumed, the sum of the effects $x_1$ is zero, as it is the sum of the deviations from the arithmetic average of the sub-populations of the same size (and thus the overall average is the arithmetic average of the levels).\\
	
	Exactly in the same way, we get:
	
	and we also find that the sum of the effects of a variable (factor) is zero.\\

	We modelize these results into the form using the average response (notice that the sum of each vector is 
zero):
	
	giving the model found. Function that we can rewrite under the form of absolute average effects (the most interesting form mathematically speaking):
	
	intermediate result to be compared with a simple linear regression that (perfectly identical result between Microsoft Excel 14.0.6117 and Minitab 15.1.1):
	
	but that does not make sense since the linear regression is not built for factors over two levels (\SeeChapter{see section Theoretical Computing}).\\

	From these results, we already derive the best tire usage policy: road, speed $50$ [km/h] and pressure $2$ [kg], we can expect to do:
	
	Let us notice immediately that in some cases, the experiment did better (see the values in the above table), even in town or at $40$ [km / h]. This come for the obvious dispersion of the results. But then... did the effects found come perhaps only from this dispersion!? Furthermore we studied the effects independently of each other. But it may have some that reinforce some others. We will explore these issues in the further below!\\
	
	Let us take back our $36$ experiments, comparing the differences $\Delta$ between the values measured (denoted here $\hat{y}$) and the model predictions (denoted here $y$):
	
	We see that the differences between the model and the predicted values are sometimes important. This may be a natural random dispersion of values, or an incorrect model or it could be due to the fact that combination of factors have more effect than separate. The positive or negative effects of two factors may do more than just add themselves (purely additive model may therefore be reject).\\
		
	It is therefore probably interactions (chemists speak of "\NewTerm{potentiation}\index{potentiation}"). To see if there are second orders interactions, a good way is to study all the factors in pairs (and ignore the existence of others). So if we start with the factors (location) $x_1$ and (speed) $x_2$ by ignoring the tire pressure $x_3$, we get (the explanations of how the values in this table are calculated are given below the table):
	
	So in the table above $32,682.2$ is the arithmetic average of the experiences done at level $x_{1,1}$ (City) and level $x_{2,1}$ ($40$ [km/h]). By subtracting the main average $x_0$ (always $33,119.3$), we get an effect of $-437.1$, while the effect of $x_1$ ($-163.4$) plus the effect of $x_2$ (-78.7) gives only $-242.2$.\\

	So there a surplus off $-194.9$, largely due to the interaction $x_{1,1,}x_{2,1}$ (notice that this is the same for $x_{1,2}x_{2,2}$, and the opposite for $x_{1,1}x_{2,2}$ and $x_{1,2}x_{2,1}$). We notice immediately that the interaction  $x_{1,1}x_{2,1}$ has a much greater effect than the effects of $x_1$ and $x_2$!\\

	Let us see the two other possible interactions of respectively factors $1$ and $3$ and $2$ and $3$:
		
	
	We can represent the interactions differently by crossing the factors:
	
	
		       
	We notice then that each line or each column has a total of zero. We also see that in this case, it is difficult to neglect the interactions, and that $x_2$ (for example) influence the results more by its interactions that its own effect!\\

	We can still try to check if the interaction of the three factors has a significant effect. As the experiment has been repeated $3$ times at each level (we then speak of "three-blocks"), we can calculate the mean of $y$, and subtract to it the model with the interactions calculated above (notice that the sum of each vector is zero!):
	
	that it is customary to write for simplification purposes (notic that the sum of each vector is still zero):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It suffices to know that for the coefficients of interaction terms the missing effects (not written) are of opposed sign.
	\end{tcolorbox}
	Some statistical software go further in the simplification of notation by writing (still in the same idea that the other coefficients are of opposed sign):
	
	and other software (such as Minitab 17.1.3 for example) gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/doe_regression_equation_minitab.jpg}	
	\end{figure}

	We can then summarize all these results in the form:
	
	We see then that the values of the model with interactions continues to diverge quite significantly for the average values, which can be interpreted either by the fact that the effects are not linear, either by an interaction between the three factors.\\
	
	It would be necessary to redo the calculations above with the interaction of the three factors. But as it is always the same principle, we will do that if some readers explicitly request us to add it.\\

	But there is still a source of error in the model is to take into account effects that do not occur in reality. To ensure that the effect calculated on a variable or interaction is real, we will use to start the one-way fixed factor analysis of variance (ANOVA) that we have studied in detail in the section Statistics. For this, it is important to have repeated the experiments in order to highlight the dispersion due to external and almost unknown uncontrolled factors.\\

	For example, the factor $x_2$ has a rather small effect. Does he have a real influence? For this, we separate the $36$ experiments in two samples (level $x_{2,1}$ and level $x_{2,2}$, and we calculate the sums of squares and the degrees of freedom (we can detail the following calculations on request) as we have see it in the section of Statistics:
	
	And we get from this table:
	
	What gives with Minitab 17.3.1 (we can see also that the model is probably not linear) as detailed in the Minitab companion book:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/engineering/minitab_speed_factor_anova.jpg}
		\end{center}	
	\end{figure}
	and visually:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/engineering/minitab_speed_factor_box_plot.jpg}
		\end{center}	
	\end{figure}	
	
	As we can see we cannot reject the null hypothesis $H_0$ of this ANOVA. Therefore this factor has no significant impact on our experience.
	
	We can do the same calculations for $x_1$ (the place: city or town):
	
	And we get from this table:
	
	What gives with Minitab 17.3.1 (we can see also that the model is probably not linear) as detailed in the Minitab companion book:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.7]{img/engineering/minitab_placefactor_anova.jpg}
		\end{center}	
	\end{figure}
	and visually:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.7]{img/engineering/minitab_place_factor_box_plot.jpg}
		\end{center}	
	\end{figure}	
	As we can see we also cannot reject the null hypothesis $H_0$ of this ANOVA. Therefore this factor has also no significant impact on our experience.\\
	
	We can do the same calculations for $x_3$ (the pressure):
	
	And we get from this table:
	
	What gives with Minitab 17.3.1 (we can see also that the model is probably not linear) as detailed in the Minitab companion book:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/engineering/minitab_pressure_factor_anova.jpg}
		\end{center}	
	\end{figure}
	and visually:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.6]{img/engineering/minitab_pressure_factor_box_plot.jpg}
		\end{center}	
	\end{figure}

	As we can see here we reject the null hypothesis $H_0$ of this ANOVA at a threshold of $5\%$. Therefore this factor has a significant impact on our experience.\\
	
	Let us recall that the fact we reject the null hypothesis $H_0$ highlight an influential factor, the one who is controlled. So the basic model to consider (leaving aside the interactions) is not:
	
	but (the sum of the vectors still being equal to zero):
	
	Caution!!! That fact that $x_1$ (speed) and $x_2$ (place) have no significant influence does not result that the interactions between $x_1$ and $x_2$, or $x_2$ and $x_3$, or even $x_1x_2$ and $x_3$ are not. For example, here, $x_2x_3$ is has no significant influence (the Fischer-Snedecor test succeeds), but at the level $1$ of $x_3$, the interaction $x_1x_2$ has a significant influence (in the sens $x_1=x_2$ against $x_1\neq x_2$).\\

	Let us see this again with Minitab 17.3.1 (the calculations below can be detailed on request):
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.9]{img/engineering/minitab_interactions_anova.jpg}
		\end{center}	
	\end{figure}
	We see that by doing a multifactorial ANOVA, that finally the factor \textit{Place} is statistically significant ($p$-value less than $5\%$), the factor \textit{Speed} is not ($p$-value greater than $5\%$), the pressure has a very significant influence (almost zero $p$-value). The interaction \textit{Place * Speed} is significant ($p$-value less than $5\%$). For cons, the interactions \textit{Speed * Pressure} and \textit{Place * Pressure} are not significant ($p$-value greater than $5\%$). The triple interaction \textit{Place * Speed * Pressure} is significant!\\
	
	So as we can see it, its not because a single given factor is not significant in the model without interaction that it will remain not significant when we take into account the interactions!!!\\
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	All the results that we have obtained above and also the charts can be obtained with a simple spreadhseet software like Microsoft Excel. So if the reader want we add the corresponding screenshots he must not hesitate, as always, to contact us.
	\end{tcolorbox}
	To close this subject, remember that we talked earlier about the interest to randomize the order of the measurement to eliminate unknown confounders. In reality, we must consider three important cases of harmful factors in practice and which will only change a little bit the used ANOVA and that most modern statistical software implement nowadays:
	\begin{enumerate}
		\item We have "\NewTerm{unknown harmful and uncontrollable factors}\index{unknown harmful and uncontrollable factors}". Therefore we make a usual ANOVA but simply where the order of measurement were randomized.

		\item We have "\NewTerm{known harmful factors but still uncontrollable}\index{known harmful factors but still uncontrollable}". We speak then of "\NewTerm{covariates}\index{covariates}" or "\NewTerm{cofactors}\index{cofactors}" and we use an ANCOVA (\SeeChapter{see section Statistics}) instead of a simple ANOVA

		\item We have "\NewTerm{known harmful factors and that under control}\index{known harmful factors and that under control}" that we want to eliminate from the ANOVA. The idea is to use a technique named "\NewTerm{blocking}\index{blocking}" it is simply a hierarchical ANOVA (\SeeChapter{see section Statistics}).
	\end{enumerate}
	
	\pagebreak
	\subsubsection{Taguchi Designs and Nomenclature (robust designs)}
	"\NewTerm{Taguchi designs}" also named "\NewTerm{robust designs}" or "\NewTerm{Taguchi Orthogonal Array Designs}" (abbreviated "Taguchi OA designs" in some softwares) are only a particular technique to find factorial or multifactorial fractional or full designs (from a matrix, a triangular table or from graph). In other words: design is a type of general fractional factorial design!
	
	\textbf{Definitions (\#\mydef):} An "\NewTerm{orthogonal array}\index{orthogonal array}"  of $s$ elements, denoted $\text{OA}_N(s^m)$, is a $N\times m$ matrix whose columns have the property that in every pair of columns each of the possible ordered pairs of elements appears the same number of time. Taguchi refers to $\text{OA}_N(s^m)$ by the notation $L_n(s^m)$ where $L$ stands for "Level".
	
	Below for example are given $\text{OA}_4(2^3)$ and $\text{OA}_8(2^7)$ respectively:
	
	
	With softwares that automatically generate the corresponding designs, this technique has become a bit old fashioned but it had the advantage at the time of its use to provide a list of over a hundred of tables with factors at $2$ or more levels with or without interactions. Let us see some classics examples  for the general culture (as it is well to know and plus it's nice and still use in practice and is softwares!!!!).
	
	Dr. Genichi Taguchi has proposed to organized experiences following tables and graphs to identify interactions (the Taguchi tables include some Plackett-Burman designs) and afterwards to be able to found the settings for the controllable variables that minimize the variability transmitted to the response from the uncontrollable variable.
	
	In the Taguchi OA design, only the main effects and two-factor interactions are considered, and higher-order interactions are assumed to be nonexistent!!!! In addition, designers are asked to identify (based on their knowledge of the subject matter) which interactions might be significant before conducting the design.

	Let us start with an example of table that we know well, the $L_8$ table. By Taguchi this table can be read as the table of a fractional factorial design of $7$ factors (thus without interaction) or  as the table of a full factorial design for $3$ factors. This is why it is denoted $L_8(2^7)$ or $L_8(2^3)$ and represented by:
	
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/taguchi_L8.jpg}
		\end{center}	
	\end{figure}
	We'll see if this table really differs from what we already know. First, each factor also takes two levels in this table, we can replace the Taguchi notation with our usage notation for the factorial replacing $1$ by "$+$" and "$2$" by "$-$":
	
	We see already much better than all the rows and columns are orthogonal taken in pairs (Hadamard matrix). We add a column of "$+$":
	
	and we notice that we fall back on the full factorial design with $3$ factors at $2$ levels if we rewrite the title row of the columns as follows:
	
	and that this also corresponds to the fractional factorial design of a $7$-factor experimental design at $2$ levels (without interactions).
	
	The question that will obviously ask the reader to himself is how we have would have identify what factor belonged to which column in the context of a full factorial design with $3$ factors if we did not know the table prepared with the techniques view previously? Well  in fact just by using the following linear graph:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/taguchi_L8_linear_graph.jpg}
		\caption{Taguchi L8 linear graph}
		\end{center}	
	\end{figure}
	that indicates the main factors in the vertices of the graph (therefore the columns $1$, $2$ and $4$ are the main factors). The column $3$ is the interaction between the columns $1$ and $2$ (hence the fact that it is on the edge between the two vertices), the column $5$ is the interaction between the columns $1$ and $4$ (hence the fact that it is on the edge between the two vertices), and column $6$ is the interaction between the columns $2$ and $4$ (hence the fact that it is on the edge between the two vertices). The fact that $7$ is outside the linear graph outside is because a triple interaction can not be represented with the technique of planar graph. We then used the symbols of vertices to signify that this is the superposition of the vertices $1$, $2$ and $4$ (circle + ring + dics).

	The second linear graph associated with this table permits to highlight another potential use:
	\begin{figure}[H]
		\begin{center}
		\includegraphics{img/engineering/taguchi_L8_linear_graph_complement.jpg}
		\end{center}	
	\end{figure}
	That is to say, to make use of it for an analysis of $4$ factors (always with two modality in this case) represented by columns $1$, $2$, $4$, $7$ (the reader can indeed check that these four columns correspond to a fractional factorial design with for generator \textbf{12} for $4$ factors either by hand or with software) with $3$ interactions ($1$ and $2$; $1$ and $4$, $1$ and $7$).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The practitioner must be careful because there are therefore two Taguchi tables therefore denoted $L_8$: one that is a full table for $3$ factors to $2$ levels and one for $7$ factors at $2$ levels without interactions as we have just seen (so with $7$ columns), but there also tables $L_8$ for $5$ factors splitten into $4$ factors at $3$ levels and $1$ factor to $4$ levels (without interactions) but only with $5$ columns. This is why books listing the tables Taguchi normally explicitly specify the scope of application of the given tables.
	\end{tcolorbox}
	Let's see some other tables (a small part of the complete list):
	\begin{itemize}
		\item Table $L_4 (2^3)$:
		
		\begin{figure}[H]
			\begin{center}
			\includegraphics{img/engineering/taguchi_L4.jpg}
			\end{center}	
		\end{figure}
		
		\item Table $L_9 (3^4)$:
		
		\begin{figure}[H]
			\begin{center}
			\includegraphics{img/engineering/taguchi_L9.jpg}
			\end{center}	
		\end{figure}
		
		\item $L_{16} (2^{15})$:
		
		\begin{figure}[H]
			\begin{center}
			\includegraphics{img/engineering/taguchi_L16.jpg}
			\end{center}	
		\end{figure}
		
		\item Table $L_{16} (4^5)$:
				
		\begin{figure}[H]
			\begin{center}
			\includegraphics{img/engineering/taguchi_L16_4.jpg}
			\end{center}	
		\end{figure}
		
		\item etc.
	\end{itemize}
	The Taguchi tables together with their graphs are therefore full or fractional factorial designs with all their advantages. Taguchi's merit is to have tried to simplify the use of factorial designs to make them accessible to a large number of experimenters. 
	
	Taguchi's designs are usually highly fractionated, which makes them very attractive to practitioners. Doing a half-fraction, quarter-fraction or eighth-fraction of a full factorial design greatly reduces costs and time needed for a designed experiment. The drawback of a fractionated design is that some interactions may be confounded with other effects. It is important to consider carefully the role of potential confounders and aliases. Failure to take account of such confounded effects can result in erroneous conclusions and misunderstandings.

	When using a Taguchi design, one needs to guess which interactions are most likely to be significant - even before any experiment is performed.
	
	The Taguchi methodology is say to have generated considerable debate and controversy. Part of the controversy arose because Taguchi's methodology was advocated in the West initially (and primarily) by entrepreneurs, and the underlying statistical science had not been adequately peer-reviewed. By the late 1980s the results of a very comprehensive peer review indicated that although Taguchi's engineering concepts and the overall objective Taguchi's robust parameter design were well-founded\cite{co2008confirmation}, there were substantial problems with his experimental strategy (maximizing signal to noise does not necessarily minimize variability...) and methods of data analysis\cite{kacker1991taguchi}.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For example to see how this is vicious... Taguchi's catalog contains $20$ arrays. However, only $18$ of these arrays are in reality orthogonal arrays...
	\end{tcolorbox}

	\pagebreak
	Finally let summarize what we have seen so far with the respective names (this list is to take with precaution as there is a priori no clear international standard to our knowledge regarding these definitions) in the order of mathematical generalization:
	\begin{itemize}
		\item Designs that contain only factors at two levels, are referred to as "\NewTerm{factorial design}\index{factorial design}" that they are based on a linear or nonlinear model, additive or not.

		\item The factorial designs that allow only to determine the coefficients of the main factors (ie additive model: without interactions) are referred to as "\NewTerm{Koshal designs}\index{Koshal designs}" or "\NewTerm{screening designs}\index{screening designs}" (also named "\NewTerm{one shot at a time designs}\index{one shot at a time designs}" because as the technique of "one at a time" it can't analyze any interaction but the comparison stops there!).

		\item The factorial designs where all the interactions of three and higher order are neglected are referred to as "\NewTerm{Rechtschaffner designs}\index{Rechtschaffner designs}".

		\item The factorial designs where the coefficients are aliased while keeping interactions are referred to as "\NewTerm{fractional factorial designs}\index{fractional factorial designs}" or of "\NewTerm{Box and Hunter designs}\index{Box and Hunter designs}" or even under the name of "\NewTerm{matrix Hadamard based designs}\index{matrix Hadamard based designs}". With fractional factorial designs, it should also be specify the method of resolution.

		\item The factorial designs whose order is a multiple of $4$ but not a power of two (ie $12$, $16$, $20$, $24$, etc.) are designated under the name of "\NewTerm{Plackett-Burman designs}\index{Plackett-Burman designs}"

		\item The designs with any number of factors, of interactions of any order and any number of levels are referred to as "\NewTerm{Fisher designs}\index{Fisher designs}" (in honor of to the original creator of the concept of experimental design ).

		\item The designs which contain as many tests  as the number of coefficients to be determined are designated under the name of "\NewTerm{saturated designs}\index{saturated designs}".

		\item The designs which contain fewer tests than  the number coefficients to be determined are referred to as "\NewTerm{oversaturated designs}\index{oversaturated designs}".
	\end{itemize}
	One aspect still needs to be clarified: it is the verification of the validity of the mathematical model of the first degree. None of these designs provides, as far as we know, such a validity test using elaborated statistics. This is why it is recommended to always add at least one test point at the center of the experimental range. The value of the response at this point will be compared to the deduced value of the other data points through mathematical model. If both values are similar, the mathematical model will be adopted if they are not we will have to reject this model and complete the results already obtained by experiments by going through a model of the second degree.

	
	Finally if we have one response variable for the problem to take up for DOE then we have the following situations (options):
	\begin{enumerate}
		\item The multiple responses are independent. Then we do our job as usually (as optimizing one under the independence assumption must not change the other responses)

		\item The multiple responses are not independent. The we add each model as a weighted sum and we optimize that latter (Overall Evaluation Criterion method as suggested in Taguchi designs) with any solver.

		\item We use $2$ stage least squares regression techniques (\SeeChapter{see section Theoretical Computing}) and after we optimize the whole with a solver.				
	\end{enumerate}
	
	
	
	\pagebreak
	\subsubsection{Response Surface Methodology (Box Domains)}
	In statistics, "\NewTerm{response surface methodology (RSM)}\index{response surface methodology}" explores the relationships between several explanatory variables and one or more response variables. The method was introduced by G. E. P. Box and K. B. Wilson in 1951. The main idea of RSM is to use a sequence of designed experiments to obtain an optimal response. Box and Wilson suggest using a second-degree polynomial model to do this. They acknowledge that this model is only an approximation, but use it because such a model is easy to estimate and apply, even when little is known about the process.
	
	An easy way to estimate a first-degree polynomial model is as we have seen to use a factorial experiment or a fractional factorial design based in the two variables case for recall by:
	
	This is sufficient to determine which explanatory variables affect the response variable(s) of interest. Once it is suspected that only significant explanatory variables are left, then a more complicated design, such as a central composite design can be implemented to estimate a second-degree polynomial model, which is still only an approximation at best given by:
			
	However the second-degree model can be used to optimize the process (maximize, minimize, or attain a specific target for).
	
	Obviously in the case of two variables, we speak of "response surface" (beyond we speak of a "volume" or "hypervolume") and then the previous relation will be written:
	
	The designs including the following three terms:
	
	are named "\NewTerm{Doehlert's composite designs}\index{Doehlert's composite designs}". We will have then to determine $4$ terms:
	\begin{itemize}
		\item $a_0$: the constant term
		\item $a_i$: term of first degree
		\item $a_{ij}$: rectangle term
		\item $a_{ii}$: quadratic term
	\end{itemize}
	We see that in the case of a $2$-factor design, we need then to estimate $6$ coefficients:
	
 	and for $3$ factors we will have to estimate $10$ coefficients:
	
	Therefore after trial and error we found that a Doehlert design has a cost (number of runs) that can be simply written as:
	
	where $k$ is the number of factors and $N_0$ the number of additional points.
	
	The idea often adopted to pass to a model of the second degree after a factorial study while retaining the tests already carried out is simply to complete the existing design by the tests for the estimation of the model of higher degree when possible.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For example, a response surface associated with a relation like the previous one looks like with Maple 4.00b:\\
	
	\texttt{>5+3*x1+2*x2+4*x1*x2+2.5*x1\string^2+3*x2\string^2,x1=-10..10,x2=-10..10,\\
view=[-10..10,-10..10,0..50],contours=10,style=PATCHCONTOUR,\\
		axes=frame,numpoints=10000);
		}
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=0.65]{img/engineering/quadratic_response_surface_design.jpg}
		\end{center}	
		\caption{Generic example of interaction for two factors quadratic polynomial}
	\end{figure}
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Pure quadratic curvature test}\mbox{}\\\\
	In running a two-level factorial experiment, we usually anticipate fitting the first order model, but we should be alert to the possibility that the second order model (response surface methodology) is really more appropriate. Therefore before running response surface will should be aware of any curvature assumption!
	
	 There is a method for replicating certain points in a $2^k$ factorial that will provide protection against curvature from second-order effects as well as allow an independent estimate of error to be obtained. The method consist of adding center points to the $2^k$ design. These consist of $n$ replicates run at the points $x_i=0$. One important reason for adding the replicate runs at the design center is that center points do not affect the usual effect estimates in a $2^k$ design.
	
	To illustrate the approach, consider $2^2$ design, with one observation at each of the factorial points $(-,-)$, $(+,-)$, $(-,+)$, and $(+,+)$ and $n_c$ observations at the center point $(0,0)$. Let $\bar{y}_F$ be the average of the four runs at the four factorial points and let $\bar{y}_C$ be the average of the $n_c$ runs at the center point. If the difference:
	
	is small then the center points lie on or near the plane passing through the factorial points, and there is no quadratic curvature. On the other hand, if this same difference is large, then quadratic curvature is present. A single-degree-of-freed sum of squares for pure quadratic curvature is given by (see proof just below):
	
	where in general, $n_F$ is the number of factorial design points. This quantity may be compared to the error mean square to test for pure quadratic curvature. More specifically, when point are added to the center of the $2^k$ design, then the test for curvature actually test the hypotheses the coefficients of the quadratic terms are all non zero):
	
	Or more explicitly
	
	First let us prove that:
	
	Indeed, let us recall the full model:
	
	And let us rewrite if for the for the $(x_1,x_2)$ combinations $(-1,+1)$, $(-1,-1)$, $(+1,+1)$, $(+1,-1)$:
	
	We see immediately that the sum gives:
	
	Therefore the average is (the sum divided by the number of corresponding measurement points):
	
	and in general:
	
	It is immediate that for the center point $(0,0)$ we have:
	
	Therefore:
	
	Now the variance of $\bar{y}_F-\bar{y}_C$ is under the assumption of equality of variances and independence:
	
	Consequently a hypotheses can be conducted using the homoscedastic Student T-test (\SeeChapter{see section Statistics}):
	
	with $n=n_F+n_C$.
	
	As we have proved in the section Statistics, we have $T_{n-2}^2=F_{1,n-2}$ (it is convenient in statistical packages to put the curvature test in the ANOVA table where all other tests are based on Fisher distribution). Therefore:
	
	and therefore:
	
	This finish our proof of the "\NewTerm{pure quadratic curvature test}\index{pure quadratic curvature test}" and notice that this do not give us the factors that are responsible of the curvature!
	
	The reader can found an application example of this test in our Minitab companion book.
	
	\pagebreak
	\paragraph{Box-Wilson Central Composite Designs}\mbox{}\\\\
	Let us recall that so far we have seen that according to the principle of analysis of factorial experimental designs that in the 1D case (a single explanatory variable), the interpolated point must have a left measurement point ($-1$) and a right measurement point ($+1$). In the 2D case (two explanatory variables) it is natural to think then to have at least $3$ points around the interpolated point  such that we have a triangle whose barycenter is the point to be interpolated. Identically in the 3D case (three explanatory variables) it is natural to think of having at least $4$ points surrounding the point to be interpolated and forming a cube or tetrahedron as needed. Thus, by generalizing the reasoning, in an $n$-dimensional space we will need at least $n + 1$ points to construct a simplex around the point to be interpolated.

	Now let us recall that we saw in the section of Theoretical Computing different techniques of linear regression and we also studied the fact that a polynomial could be analyzed with a simple linear regression. We obtained in the same section (during the study of the factor of inflation of the variance) the following relation with its abusive notation ... dangerously confusing matrix of covariance and simple variance:
	
	with for recall:
	
	We have therefore to obtain a scalar from the explanatory values:
	
 	And therefore:
	
	Experts in the field routinely write the latter relation in the following form and name it "\NewTerm{variance function}\index{variance function}":
	
	The idea now is to come back to our experimental designs is therefore to choose our measurement points which minimizes the standard deviation of the predictions introduced just previously!

	Let us consider as an example the problem of finding a linear regression to the polynomial:
	
	in the square $-1\leq x_1 \leq 1$,$-1\leq x_2\leq 1$ and let us compare the maximum value of the predictive variance for the case of a complete factorial design (ie a square in the plane) and then for a fractional factorial design (ie a triangle in the plane) that omits the point $(1,1)$.
	
	In the first case of the complete factorial design, we have therefore (\SeeChapter{see section Linear Algebra}):
	
	and therefore:
	
	What is written more frequently:
	
	where $R$ is the distance (radius) to the central point. When a predictive variance depends only on the distance $R$, we say that we have a "\NewTerm{rotation invariant experimental design}\index{rotation invariant experimental design}" or simply a "\NewTerm{rotational experimental design}\index{rotational experimental design}" (the variance function is then say to be "spherical"). This type of plan is attractive to practitioners who do not know a priori where in the space of experimentation the most precise forecasts must be found!

	Therefore:
	
	The variance has then for value at the point of origin $(0,0)$:
	
	And:
	
 	at the ends. This case thus shows a small variation between the origin and the extremities (a variation exactly of $\sqrt{3}$).

	In the second case of the fractional factorial design we have:
	
	and therefore:
	
	Therefore:
	
	So first this design is by definition not invariant by rotation since the variance does not depend only on $R$!

	The variance corresponding to the point of origin $(0,0)$ is then:
	
 	and at the end $(1,1)$ we have:
	
 	If we now compute by the partial derivatives the coordinates which minimizes the error such that:
	
 	We get:
	
 	Hence:
	
	and therefore:
	
	Therefore it comes:
	
	is the point of minimum variance and at this point (which coincides with the barycenter of the triangle, that is to say the center of the inscribed circle), we have:
	
 	From this example, we can already easily conclude that for a complete factorial design, the variance of the predictive error is written:
	
 	and as well as the maximum error is immediate and is equal in the case of a complete factorial design:
	
 	Thus, the quality of the fit (regression) becomes very good when $n$ grows!

	Let us now consider now the fractional factorial design based on three points in the plane forming an equilateral triangle:
	
	Similar calculations to before give us (as always on readers request we can detail!):
	
	Thus, we notice that, by judiciously choosing the points of the design, we may have a lower prediction error than with a complete factorial design both at the origin and at the points $(1, 1)$, $(-1,1)$, $(1, -1)$ and $(-1, -1)$. However the price to pay is that we have to make measurements outside the unit square which complicates the task of the practitioner.
	
	
	
	\subparagraph{Circumscribed Center Designs}\mbox{}\\\\
	The "\NewTerm{Composite Center Design (CCD)}\index{Composite Center Design}" is a two-level experimental design for surface planes. In the 2D case (two explanatory variables) this is equivalent to taking the usual points of the squares to which we add the points of intersection of the axes with the circumscribed circle (+ in practice one or more central control points):	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/composite_center_design_2d.jpg}	
		\caption{2D Composite Center Design}
	\end{figure}
	We thus have the following points, which are all at the same distance from the origin of the axes:
	
	where of course the root of $2$ comes from the fact that the radius of the circle is given by the vertices of the square and thus by stupidly applying Pythagoras:
	
 	Due to the fact that in the above case the circle (in 2D) or the sphere (in 3D) is circumscribed to the square (or the cube in the 3D case) we also talk about CCC design for "\NewTerm{Circumscribed Center Design (CCD)}\index{Circumscribed Center Design}".

	Let's take the example of the CCD case where the columns in the matrix below, the first is filled only with $1$, the other columns corresponding respectively to the terms: $x_1$,$x_2$,$x_1x_2$, $x_1^2$, $x_2^2$ (notice that not all columns are orthogonal!):
	
	where the last row correspond to the fact of adding a control point a the center of the domain. This empirical matrix is sometimes named "\NewTerm{composite equiradial matrix}\index{equiradial}", it request as we see a strong symmetry and therefore implies what we named an "\NewTerm{isovariance}\index{isovariance}" (see plots with Maple further below of the variance).
	
	We also see that the factors have $5$ levels $(-1,1,-\sqrt{2},+\sqrt{2},0)$. This is a characteristic of CCD designs!
	
	Therefore:
	
	Using a CAD software we get:
	
	Then we have:
	
	We therefore have here an experimental design whose variance depends indeed only on the distance $R$ from the origin as many textbooks state but without proof.

	Also at the opposite of $99\%$ of textbooks we will give the Maple 4.00b code to plot the famous variance corresponding figure:

	\texttt{>contourplot3d(sqrt(1-7/8*(x\string^2+y\string^2)+11/32*(x\string^2+y\string^2)\string^2),\\x=-sqrt(2)..sqrt(2),y=-sqrt(2)..sqrt(2),contours=10,filled=true);}
	
	We have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/composite_center_design_2d_variance_plot_perspective.jpg}	
		\caption{2D Composite Center Design variance perspective plot in Maple 4.00b}
	\end{figure}
	Either seen from above we get the corresponding famous chart available in many textbooks regarding to the rotational invariance property of surface design with two factors:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/composite_center_design_2d_variance_plot_top.jpg}	
		\caption{2D Composite Center Design variance top plot in Maple 4.00b}
	\end{figure}
	We can see that CCD have a number cost (run) of:
	
	where $k$ is the number of factors, $r$ the reduction integer and $N_0$ the number of center points. Therefore the three terms can be read as following:
	\begin{itemize}
		\item $2^{k-r}$: Common full factorial of fraction factorial design of the  composite equiradial matrix.

		\item $2k$: Number of star points

		\item $N_0$: Number of center points
	\end{itemize}
	Therefore for a typical two factors design with a full factorial subset we have:
	
	and as $5$ is recommended for $N_0$, it comes that $N_{\text{CCD}}=13$ for a two factor response surface design with $5$ center points.
	
	\pagebreak
	\subparagraph{Face Centered Designs}\mbox{}\\\\
	Repeating the same maneuver but with a experience design said to be "\NewTerm{face-centered design}\index{face-centered design}" such as:
	
	Then we have:
	
	And:
	
	We then have after:
	
	Notice that the Circumscribed Center Design (CCC) was a rotatable design but that the Face Centered Design (CCF) is not as it does not depends only on $R$.
	
	Which give us with Maple 4.00b by proceeding exactly with the same above given code:
	We have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/face_centered_design_2d_variance_plot_perspective.jpg}	
		\caption{2D Faced-centered Design variance perspective plot in Maple 4.00b}
	\end{figure}
	Either seen from above we get the corresponding famous chart available in many textbooks:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/face_centered_design_2d_variance_plot_top.jpg}	
		\caption{2D Faced-centered Design variance top plot in Maple 4.00b}
	\end{figure}
	The two previous designs are frequently summarized with the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/ccd_fcd_summary.jpg}	
	\end{figure}
	In fact there are three Composite Central Design:
	\begin{enumerate}
		\item Circumscribed (CCD)
		\item Inscribed (ICD)
		\item Face centered (FCD)
	\end{enumerate}
	Response surface methodology is obviously often applied to pilot plant operations by research and development personnel but when it is applied to a full-scale production process, it is usually only done once (or very infrequently) because the experimental procedure is relatively elaborate. However, conditions that were optimum for the pilot plant may not be optimum for the full-scale process. Indeed, the operation "scale-up" of the pilot plant to the full-scale production process usually results in distortion of the optimum conditions. Even if the full-scale plant begins operation at the optimum, it will eventually' 'drift" away from that point because of variations in raw materials, environmental changes, and operating personnel.

	A method is needed for the continuous monitoring and improvement of a full-scale process with the goal of moving the operating conditions toward the optimum or following a "drift." The method should not require large or sudden changes in operating conditions that might disrupt production. "\NewTerm{Evolutionary operation (EVOP)}"\index{ Evolutionary operation} was proposed by Box (1957) as such an operating procedure. It is designed as a method of routine plant operation that is carried out by manufacturing personnel with minimum assistance from the research and development staff.
	
	EVOP consists of systematically introducing small changes in the levels of the operating variables under consideration. Usually, a $2^k$ design is employed to do this. The changes in the variables are assumed to be small enough that serious disturbances in yield, quality, or quantity will not occur, yet large enough that potential improvements in process performance will eventually be discovered. Data are collected on the response variables of interest at each point of the $2^k$ design. When one observation has been taken at each design point, a cycle is said to have been completed. The effects and interactions of the process variables may then be computed. Eventually, after several cycles, the effect of one or more process variables or their interactions may appear to have a significant effect on the response. At this point, a decision may be made to change the basic operating conditions to improve the response. When improved conditions have been detected, a phase is said to have been completed. 
	
	In testing the significance of process variables and interactions, an estimate of experimental error is required. This is calculated from the cycle data. Also, the 2k design is usually centered about the current best operating conditions. By comparing the response at this point with the 2k points in the factorial portion, we may check on curvature or change in mean (CIM); that is, if the process is really centered at the maximum, say, then the response at the center should be significantly greater than the responses at the 2k-peripheral points.
	
	In theory, EVOP can be applied to k process variables. In practice, only two or three variables are usually considered. We will give an example of the procedure for two variables. Box and Draper (1969) give a detailed discussion of the three-variable case, including necessary forms and worksheets. Myers and Montgomery (1995) discuss the computer implementation of EVOP.
	
	\pagebreak
	\subsubsection{Optimal Designs}
	In the design of experiments, "\NewTerm{optimal designs}\index{optimal designs}" are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith.

	In the design of experiments for estimating statistical models, optimal designs allow parameters to be estimated without bias and with minimum variance. A non-optimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design. In practical terms, optimal experiments can reduce the costs of experimentation.
	
	The optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion, which is related to the variance-matrix of the estimator. Specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments.

	We will begin here with the study of "\NewTerm{D-optimal design}" which is a form of experimental design provided by a computer algorithm for which we will give the mathematical background a little further below. These types of computer-aided designs are particularly useful when conventional plans do not apply.

	Unlike standard classical models such as factorial design and fractional factorial designs or Taguchi designs, D-optimal designs are generally not orthogonal but are very flexible, hence their great practical importance.

	D-optimal plans are always an option whatever the degree of the model the experimenter wishes to use (eg first order, second order with or without interactions, complete quadratic, cubic, etc.).

	The D-optimal designs are associated with a regression model based on the fact that since in the case of the multiple linear regression we have (\SeeChapter{see section Theoretical Computing}):
	
 	then we must be able to find the component values of the matrix $X$ which minimize the variances (the elements of the diagonal of this matrix as we have proved in the section of Theoretical Computing!). As this optimization operation is tricky, it is necessary to optimize a scalar value relative to the dispersion matrix $(X^TX)^{-1}$

	If we write:
	
	Then let us recall that we have proved in the section of Linear Algebra that the components of the inverse of the matrix $A$ are given by:
	
	Therefore minimize all the components is equivalent as to maximize the determinant of $A$ and hence to maximize the determinant of $(X^TX)^{-1}$. It is for this reason that we speak of D-optimal designs for "\NewTerm{Determinant-optimal designs}\index{Determinant-optimal designs}" because trying to minimize the diagonal components (and in general all components) of the variance-covariance matrix is equivalent to seeking to maximize the determinant of $(X^TX)$:
	
	Another approach of minimizing the diagonal components of the variance-covariance matrix can be in choosing the scalar value to minimize, and therefore to minimize the trace of the information matrix:
	
 	and we then speak of an "\NewTerm{A-optimal design}\index{A-optimal design}" which is numerically more expensive in algorithmic terms than the search for the D-optimal plan because it is necessary to reverse the information matrix but it is a criterion that is often considered more natural.

	However, the majority of software specialized in experimental designs propose to determine the D-optimal or A-optimal designs but there are many other possible criteria that we will not present here such as E-optimality, G-optimality, etc.

	Obviously before that the D or A-optimal design or other can be generated, the experimenter must know the theoretical model he wants! Thus, knowing the model, the number of factors and levels of each factor we known then the complete factorial design and from the number of real tests (runs) that can be made by the experimenter (economically speaking!), we must find the most effective sub-set that will be a D-optimal, A-optimal design or other...

	It seems that to date the software uses evolutionary algorithms (\SeeChapter{see section Theoretical Computing}) to determine the optimal design so in extenso there is no guarantee that the given optimal design is really the optimal one (except if one restarts the calculation a great number of times always falling on the same result).

	The advantage of D-optimal or A-optimal designs or other designs of the same family is that they are very flexible and often more suited to the real needs of the experimenters.
	
	OK... it may seems quite abstract for some readers so let us see a companion example.
	
	Suppose that an experimenter has $3$ controlled variables and that seeks to determine a model of the type:
	
 	and we will consider that it has the following parameters:
	
	and that he can only do $12$ trials (for economic reasons or times reasons or else...!). In this case, given that one of the factors have $5$ levels, it is out of question for this to use fractional factorial designs or the Plackett-Burman designs. Even if Taguchi's designs manage certain factors at more than $2$ levels there is no Taguchi plan for this particular scenario (at least as far as we know...) so we must also forget them. The same applies to composite surface designs or Box-Behnken designs.

	The only thing that can be done is either to use a complete factorial design (but again it would require $20$ tests which is not conceivable!) or a D or A-optimal design.

	Obviously the complete factorial plan would require $5\cdot 2\cdot 2=20$ tests (runs) which would be:
	
	So that everyone can use an accessible tool, we will show how to extract a D or A-optimal design of $12$ tests using a simple spreadsheet software like Microsoft Excel 14.0.7015 by first constructing the following table corresponding to the matrix in its totality:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/full_factorial_design_for_d_optimum_design_microsoft_excel.jpg}
		\caption[]{Starting full design for D-optimal design seeking in Microsoft Excel 14.0.7015}	
	\end{figure}
	Then we construct the following structure a little further on the same sheet:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/d_optimum_design_structure_preparation_in_microsoft_excel.jpg}
		\caption[]{Preparation of D-optimal design with formulas in Microsoft Excel 14.0.7015}	
	\end{figure}
	Now let's us recall that finding a D-optimal design is equivalent to find the components of $X$ such as:
	
	so in one of the cells we should write (be careful this is a matrix function so we must validate by Ctrl + Shift + Enter):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/d_optimum_design_formal_determinant_in_microsoft_excel.jpg}
	\end{figure}
	But if we follow the method advocated by the NIST on their page here:
	\begin{center}
	\url{http://www.itl.nist.gov/div898/handbook/pri/section5/pri521.htm}
	\end{center}
	We would have to maximize the determinant of the submatrix (which in all rigor is false but anyway let us do as they say...) and then we write rather:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/d_optimum_design_nist_determinant_in_microsoft_excel.jpg}
	\end{figure}
	Then we launch the solver of this version of Microsoft Excel in evolutionary mode:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/d_optimum_design_nist_determinant_solver_settings_microsoft_excel.jpg}
		\caption{Evolutionary solver settings for seeking D-optimal design in Microsoft Excel 14.0.7015}
	\end{figure}
	You may perhaps be wondering why more variable cells should be selected than necessary. This is because of the intrinsic operation of the Microsoft Excel solver. If we take that cells \texttt{I2} to \texttt{I13} it will not find any solution that you put additional constraints or not to the solver.

	If we run the solver, we get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/d_optimum_design_fina_according_nist_in_microsoft_excel.jpg}
		\caption{Final D-optimum design following NIST recommendation in in Microsoft Excel 14.0.7015}
	\end{figure}
	that give us a determinant of $1296$. The result provided by  the NIST has in comparison a determinant of $1156$. The D-optimal experimental design given by the NIST is indeed:
	
	Where does this difference comes from???!!!!! This is quite easy to understand: It is impossible to found all coefficients with the system we get with Microsoft Excel as we still have $4$ levels, $2$ levels and $2$ levels so this system cannot be solved as we would need $16$ runs to solve it without alias!!!! So the real algorithm to reach $12$ runs has to eliminate one of the level of the first factor and the main purpose of $D$-optimum design is to found which level should be eliminate to keep the maximum information!
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Caution! Depending on the software packages of used for DoE, you might come across a different result because they basically use almost all different algorithms (Federov's Method, Modified Federov's Method or K-Exchange Method for most known one):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/reliasoft_optimal_design.jpg}
		\caption[]{Reliasoft DOE++ Screenshot of algorithms options dialog box}
	\end{figure}
	\end{tcolorbox}
	Then, it is customary in practice and if we conform ourselves to the NIST results... to make the calculations of the following efficiency indices relative to the matrix obtained (the choice is completely empirical but well... it seems that most statistical software has its own indicators...):
	
	where in the G-efficiency, the term $\sigma_G$ is the smallest variance in the diagonal of the projection matrix $X(X^TX)^{-1}X^T$.
 
	What gives in Microsoft Excel:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/index_efficiency_optimal_designs_formulas_excel.jpg}
		\caption{Explicit formulas for index efficiency conforming to NIST in Microsoft Excel 14.0.7015}
	\end{figure}
	where in the relations of the cells \texttt{S1} to \texttt{S3}, the $5$ is the number of columns of the experiment matrix and the $12$ is the number of rows. To do the calculations of the A-efficiency and G-efficiency we need in Microsoft Excel both matrices explicitly (since this spreadsheet does not have functions for the trace of a matrix nor for the maximum of the trace) as that we have in \texttt{R6} (and the whole range of the relative matrix) the following matrix relation (to be validated with Ctrl+Shit+Enter):
	\begin{center}
		\texttt{=MINVERSE(MMULT(TRANSPOSE(J2:N13),J2:N13))}
	\end{center}
	and in \texttt{R13} (and in all the corresponding range) the following matrix relaction (to be validated with Ctrl+Shit+Enter):
	\begin{center}	
\texttt{=SQRT(MMULT(J2:N13,MMULT(MINVERSE(MMULT(TRANSPOSE(J2:N13),J2:N13))\\
		,TRANSPOSE(J2:N13))))}
	\end{center}
	What gives:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/index_efficiency_optimal_designs_excel.jpg}
		\caption{Index efficiency conforming to NIST in Microsoft Excel 14.0.7015}
	\end{figure}
	and perfectly matches (obviously...) the values given by the NIST! We also see that JMP (SAS) also gives (with the difference that it multiplies D-efficiency and G-efficiency by a factor $100$) as shown in the printscreen of the french version below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/index_efficiency_optimal_designs_jmp.jpg}
		\caption{JMP (SAS) efficiency index of optimal designs}
	\end{figure}
	while the Minitab software (version 16.1.1 and earlier) gives the D and G optimality but calculates it in a bizarre and obscure way in my point of view.
	
	In order to fit an optimality criterion the companion example above show us that we have to define the number of experiments we want to have in the design. The selection of this factor is very important because changing the number of the design runs alters the model matrix and another optimal design is chosen. There are no rules to define this number, but the minimum is model-dependent. A model with $p$ coefficients can only be investigated with a D-optimal design which has at least $p$ runs. In most cases, it is useful to create different designs that differ in the number of runs and compare the efficiency of the designs. A design with a few more or less design runs than the desired one can have a higher determinant and hence is the best design to perform. A software like Minitab for reasons that are unknown to us don't give however the possibility  to have a number of runs equal to the number of factors.
	
	\pagebreak
	\subsubsection{Mixture Design}
	When a product is formed by mixing together two or more ingredients, the product is named a "mixture", and the ingredients are named "mixture components". In a general mixture problem, the measured response is assumed to depend only on the proportions of the ingredients in the mixture, not the amount of the mixture. For example, the taste of a fruit punch recipe (i.e., the response) might depend on the proportions of watermelon, pineapple and orange juice in the mixture. The taste of a small cup of fruit punch will obviously be the same as a big cup.
	
	Sometimes the responses of a mixture experiment depend not only on the proportions of ingredients, but also on the settings of variables in the process of making the mixture. For example, the tensile strength of stainless steel is not only affected by the proportions of iron, copper, nickel and chromium in the alloy; it is also affected by process variables such as temperature, pressure and curing time used in the experiment.

	One of the purposes of conducting a mixture experiment is to find the best proportion of each component and the best value of each process variable, in order to optimize a single response or multiple responses simultaneously. In this chapter, we will discuss how to design effective mixture designs and how to analyze data from mixture experiments with and without process variables.
	
	There are several different types of mixture designs. The most common ones are "simplex lattice", "simplex centroid", "simplex axial" and "extreme vertex designs", each of which is used for a different purpose.
	\begin{itemize}
		\item If there are many components in a mixture, the first choice is to screen out the most important ones. Simplex axial and Simplex centroid designs are used for this purpose.

		\item If the number of components is not large, but a high order polynomial equation is needed in order to accurately describe the response surface, then a simplex lattice design can be used.

		\item Extreme vertex designs are used for the cases when there are constraints on one or more components (e.g., if the proportion of watermelon juice in a fruit punch recipe is required to be less than $30\%$, and the combined proportion of watermelon and orange juice should always be between $40\%$ and $70\%$).
	\end{itemize}
	So often in practice we are interested in studying mixtures. In other words, we want to know the proportion of ingredients needed to achieve a certain result. The cases of application are so numerous that it would be futile to enumerate them all here.

	The fact that the proportions must be added is the key element of the simple mixture design since if we denote by $x_i$ the proportion of the ingredient $i$ then under unconstrained mixture we have:
	
 	or written slightly differently:
	
	where the last relation is named logically "\NewTerm{fundamental constraint of unconstrainted mixtures}\index{fundamental constraint of unconstrainted mixtures}".

	So we already notice that it is not possible to vary one parameter (ingredient) without the other being modified. So the control variables are not independent! We can not then use the techniques seen so far!

	As we will see just below, the response surface of a mixture design is a simplex (connected surface as introduced in the section of Geometric Shapes) of dimension $k$ with $k-1$ edges (and generally plunged into a space of dimension $k$).

	For example, with two factors (binary mixture) the simplex  is simply a line segment ranging from $(0,1)$ to $(1,0)$ in terms of coordinates. With three factors, the simplex is a triangle with three edges with three vertices of coordinates $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_three_factors_simplex.jpg}	
		\caption{Three factor mixtures design simplex}
	\end{figure}
	where the segments of the point $M$ at the concentration points  are logically the translations of the edges of the triangle with for condition that the sum of the segments being equal to a unit length.

	There are several ways of arranging the experimental points whose in our field of study are (from top to bottom in the figure below):
	\begin{itemize}
		\item Simplex lattice designs
		\item Simplex-Centroid designs
		\item Augmented Simplex-Centroid designs
		\item and some other exotics one...
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_simplex_arrangements.jpg}	
		\caption{Various arrangements of three factors mixture simplex}
	\end{figure}
	Giving some classes I have notice that also an example of a $4$ factors simplex-centroid example design is also useful for some students:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_centroide_simplex_four_factors.jpg}	
		\caption{$4$ factors simplex-centroid example}
	\end{figure}
	
	\paragraph{Network mixture designs (simplex lattice designs)}\mbox{}\\\\
	The "\NewTerm{network mixture designs}\index{network mixture designs}" more commonly named "\NewTerm{Simplex lattice designs}\index{simplex lattice designs}" and sometimes just "\NewTerm{L-simplex design}\index{L-simplex design}" or "\NewTerm{Scheffé's designs}\index{Scheffé's designs}" are those most often studied in schools because they are the least abstract and algebraically the most accessible. Thus, in a network mixture design of $p$ variables and where the intervals of values are cut in $m$ pieces, the reader will notice by trying manually on a sheet of paper that we get the following table of number of runs (that is to say: the cost of the corresponding design):
	
	OK this table table be a quite abstract for some people. So let us consider an illustration of the case $p=3$ and $m=1$ (we see clearly the $N=3$ runs):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_design_non_centered_p3_m1.jpg}
	\end{figure}
	and for the case $p=3$ and $m=2$ (we see clearly the $N=6$ runs):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_design_non_centered_p3_m2.jpg}
	\end{figure}
	and finally for the case $p=3$ and $m=3$ (we see clearly the $N=10$ runs):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_design_non_centered_p3_m3.jpg}
	\end{figure}
	A very common mixture design is the $\{p,m\}={3,2}$ with $6$ experimental points as seen above but with an addition point. This is why in many textbooks we can found: $\{p,m\}={3,2}=7$ as there is an addition center point!
	
	In the general case of non-centered lattice mixture designs we find by a little bit trial and error that the cost (runs) of such a simplex is given by:
	
	and for a centered lattice design of $q$ factors by:
	
	Let us also indicate that logically nothing prevents the support (domain of definition) of the mixtures from being constrained, we then speak of "\NewTerm{extreme vertices designs}\index{extreme vertices designs}" that cover only a subportion or smaller space within the simplex. Thus, in the case of a ternary mixture design, nothing prevents us from having a case where each of the components has only upper limit values but the lower value can be zero! Here is for example a special case of constraint mixture design:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_constraint_domains.jpg}
		\caption{Three factor mixture constraint domains}
	\end{figure}
	or lower ($L$) and at the same time upper ($U$) limit values:	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/mixture_highly_constraint_domains.jpg}
	\end{figure}
	or that the ratio of two ingredients is constant, or that the sum of two ingredients gives a constant proportion. In short there may be many scenarios, but in the case of limited boundary, it is customary to write:
	
	and this is what we name "\NewTerm{linear constraints}\index{linear constraints}" (as it is obvious if each ingredient is bounded that the sum is also bounded!).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	It is quite possible to treat both the mixing variables (the proportions of the constituents) and the design factors of the experiments. To illustrate this situation, we can take the example of a chocolate manufacturer. The study of the composition of the chocolate gives rise to a mixtures design and the conditions of preparation give rise to a factorial design (or other ...). At each point of experiments of the factorial design, it is necessary to realize mixture design!!!! There is thus rapidly a large number of tests to be carried out since it is necessary to carry out $np$ experiments if the mixture design has $n$ runs and the factorial design has $p$ runs.
	\end{tcolorbox}
	We then consider the particular simple linear model (thus still an approximation ... and there are many other theoretical models that are not linear):
	
	or seen with the statistician point of view:
	
 	But since we have the constraint that $\sum_{i=1}^k x_i=1$ the $\hat{\beta}_i$ can not be determined uniquely if we go through the usual linear system. One of the $x_i$ must be used to determine the coefficients uniquely and must therefore be eliminated (concept of degree of freedom)! It is obviously very annoying in practice.

	So mathematically, nothing prevents us from writing:
	
 	and if we change the notations:
	
 	the previous relation can then be written:
	
	The mathematical model may of course also contain interactions. For example in the case of two factors, we could write with interactions and at the second degree:
	
 	and by applying the identity:
	
 	then:
	
	Therefore:
 	
	with therefore:
	
 	Either with three ingredients this gives for example:
	
 	and so on for $n$ components and the idea is the same for the third degree and beyond. It is Henry Scheffé that suggested to describe mixture properties by reduced polynomials subject to the normalization condition.
	
	The number of terms will then obviously be for this model of the second degree:
	
 	We can then use the following table of control (for those who like to do the calculations by hand) by stopping at the power of three (yes we have to stop one day or another...):
	
	Now let us come back to our initial example:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/simple_lattice_design_three_factor_design.jpg}
	\end{figure}
	for the following both reasons:
	\begin{enumerate}
		\item There are $6$ measurement points and this is good because we have $6$ coefficients to determine and we will need $6$ equations at least.

		\item The choice of the arrangement of the $6$ points is quite intuitive. Indeed, if you imagine first taking $3$ points, which ones are furthest from each other? They will be those of the summits of course! Once these three points are taken, what are the three other potential points that are the furthest away from it, but at the same time the furthest away from each other? They will be those in the middle of the edges!
	\end{enumerate}
	We will then write algebraic the following that resume the above figure:
	\begin{itemize}
		\item For $3$ of the $6$ points on the vertices: $y_i$ for $x_i=1$, $x_j=0$ with $i,j=1,2,3$ under the constraint that $j\neq 1$

		\item For $3$ of the $6$ points on the middle of the edges: $y_{ij}$ for $x_i=\dfrac{1}{2}$, $x_j=\dfrac{1}{2}$, $x_k=0$ under the constraint that $i<j,k\neq i,j$
	\end{itemize}
	What injected into:
	
	gives $6$ equations with $6$ unknowns:
	
	The search for the coefficients for these $6$ equations is immediate:
	
	Either in a generic way:
	
 	If we make repeated measurements in quantities $r_{ij}$, $r_i$, $r_j$, then we have:
	
 	Either by extension:
	
 	and for the variance (which will be useful for making inference on the values of the coefficients):
 	
	Some statistical software and authors even consider that (we must then verify that this hypothesis is well satisfied with typically a Fisher test):
	
	Then it remains:
	
	Some statistical softwares assumes that (so take care the this hypothesis is not rejected in the practice).
	
	It is alos interesting to measure the experimental variance of the model. Therefore in the general second degree case that is for recall written:
	
	Then we have:
	
	or if the measurement are not in equal quantities:
	
	which can sometimes found in the following way in the literature:
	
	and as:
	
	Therefore it comes:
	
	with:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider that three constituents that are polyethylene $x_1$, polystyrene $x_2$ and polypropylene $x_3$, are mixed in order to obtain a fiber whose measure of interest is the elongation at a constant force with the following values of the mean elongations obtained for each of the points of the mixture design network:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/mixture_design_without_process_variable_example.jpg}	
	\end{figure}
	with the measurements detailed in the corresponding table below:
	
	and we will use the model:
	
 	and so we will use:
	
 	Which gives us:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	So at the level of the influences, we already see that (without even needing to make a statistical test of significant difference):
	
	with the obvious that follows. We also observe already that:
	
	which means that the components $1$ and $2$, $1$ and $3$ contributes positively to the effect were are looking for when the interaction $2$ and $3$ decrease that later.\\

	Now, let us calculate and check if we have well:
	
	Therefore:
	
	With for recall (requested by reader) for example:
	
	and then:
	
	and also:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	We see then obviously that we quite far from the assumption of equality of variance...\\

	Therefore what can we do???\\
	
	In fact apart to do the calculations of $\hat{\sigma}$ using the global average $\bar{y}$ as detailed just above, we notice that was is not realistic in the choice of the calculation of $\hat{\sigma}$ as done is that we consider that the average $\bar{y}$ should be the same for all the levels of the components which is obviously not true! Therefore as during our study of the ANOVA, we will use the residual variance $\text{V}_R$ (that is we know is also named "grouped variance" or "pooled variance") and that is given therefore by:
	
	We see then that the compromise is much better ... (without however of course to be exact!). We then have using the results prove earlier above:
	
	Our model is then traditionally written:
	
	which corresponds exactly to the second decimal point to what a software like Minitab gives us. Then, to find the triplet that maximizes the function it is enough to use a solver like the one  integrated into Microsoft Excel.\\
	
	Let us recall now that:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Let us now recall that:
	
	And as in our example the model has no pure quadratic term, it reduces to:
	
	We have then:
	
	We see then that to determine $\hat{\beta}_0$ we have to make a choice on one of the $\hat{\beta}_i$ (as we are not able to refer to the experimental value in the present case since the absence of any ingredient in the mixture will only give a empty entity and then it is quite difficult to test an elongation on nothing... but this, however, raises the principle of the approach). We will choose empirically $\hat{\beta}=0$, then it comes:
	
	The model with the ordinate at the origin (at least if it had a physical sense) would then be written:
	
	with in brackets the variances that do not change for the relative coefficients. There are two coefficients whose variances will change!\\
	
	We have indeed:
	
	Therefore:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Hence:
	
	We can even make an ANOVA of the regression (\SeeChapter{see section Theoretical Computing}) as for the more" traditional" experimental design and as we have studied (proved!) in the section of Theoretical Computing, we will need the following values:
	Let us call that in the one fixed factor ANOVA we have proved that:
	
	Then let us use the following table:
	
	Then we have:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We then have the following one-way fixed factor ANOVA table (\SeeChapter{see section Statistics}):
	
	where the limit has been calculated with the following function of the spreadsheet software Microsoft Excel 14.0.7106:
	\begin{center}
		\texttt{=F.INV(95\%,5,9)=3.481}
	\end{center}
	Hence as $31.5$ (corresponding $p$-value of $0.002\%$) is far above $3.481$ we reject the null hypothesis $H_0$ that the model coefficients are non-significant é(ie they are therefore significantly different from zero). In extenso this also leads us to reject the fact that the response surface is plane (since the coefficients of the interactions are also significant).\\
	
	The reader interested can see the study of this mixture in the R or Minitab companion books.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Full Factorial Combined with Mixture Design-Crossed Design}\mbox{}\\\\
	In 1981, Cornell\cite{cornell2011experiments} published in its book the results of experimental studies where the quality of a fish patty was defined by both its composition and production process. The experiment included seven different mixtures prepared by mixing different species of fish and then subjecting the resulting patty to various cooking conditions and defined in accord with simplex-centroid design (see figure below).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/simplex_centroid_design.jpg}
		\caption{Simplex Centroid Design}
	\end{figure}
	The preparation procedure of the defined mixtures or a formulation included three control process factors with associated
variation levels: baking temperature from $190\;[^\circ\text{C}]$  to $218\;[^\circ\text{C}]$; time in the oven from $25$ [min] to $40$ [min]; deep fat frying time from $25$ [s] to $40$ [s]. Process factor levels have been varied in accordance with the $2^3$ full factorial experiment. Design of experiment $7\times 2^3$ with $56$ trials of a simplex-centroid design $\times 2^3$ full factorial design
has been sufficient for mathematical modeling of the observed phenomenon (see figure below).
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/cross_design_mixture_on_vertices.jpg}
		\caption{Simplex-centroid design in each point of a $2^3$ full factorial experiment}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/cross_design_factorial_on_vertices.jpg}
		\caption{$2^3$ factorial design in each point of a simplex-centroid design}
	\end{figure}
	A regression model with $56$ regression coefficients or reduced regression model with $18$ coefficients has been sufficient for an adequate description of the
problem.	
	
	This was the first example of application of a mixture design $\times$ process factor design in experimental studies. Since such designs contain a relatively large number of factors, it is of interest to replace full factorial designs of process factors with fractional factor designs. Today all modern Statistical softwares include the analysis of "\NewTerm{design-crossed design}\index{design-crossed design}".
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Assume a design has $3$ mixture components and $2$ process variables, as illustrated in the above figure.\\
	
	For the $3$ mixture components, the following special cubic Scheffé model is used:
	
	For the $2$ process variables the following model is used:
	
	The combined model with both mixture components and process variables is assumed, for being able to make an ANOVA, as following (quite strong assumption in the point of view of the physicist...):
	
	The above combined model has total of $7\times 4=28$ terms. By expanding it, we get the following model:
	
	The combined model basically crosses every term in the mixture components model with every term in the process variables model. From a mathematical point of view, this model is just a regular regression model. Therefore, the traditional regression analysis method can still be used for obtaining the model coefficients and calculating the ANOVA table.
	\end{tcolorbox}
	The reader can see a real application example of crossed designs in the Minitab companion book if necessary.
	
	\pagebreak
	\pagebreak
	\subsubsection{General DoE diagnostic tools}
	We have seen so far the almost all DOE use statistical analysis based on ANOVA, ANCOVA and obviously linear regression null hypothesis tests. But most DoE softwares propose a bunch of other more or less empirical diagnostics for the analysis of the designs and this is what we will focus below in details.
	
	\paragraph{Lenth's PSE Pareto Margin Error for unreplicated factorial designs}\mbox{}\\\\
	G.E.P. Box and R.D. Meyer introduced in 1986 a method for assessing the sizes of contrasts in unreplicated factorial and fractional factorial designs. This is was useful technique, and an associated graphical display popularly known as a "Bayes plot" makes it even more effective but very hard to interpret. Therefore Russell V. Lenth proposed a new method 1989 that is still currently used in many statistical softwares.
	
	The "\NewTerm{Lenth's Pseudo Standard Error}\index{Lenth's PSE}" (abreviated Lenth's PSE) is a commonly indicator in DoE softwares Pareto outputs (even if it is not real Pareto charts...) that provides an analytical basis for testing the effects in an unreplicated (single replicate) two-level factorial design\footnote{It should not then be necessary to precise that it therefore not applies to General factorial Designs}. Lenth's method estimates the variance of an effect from the smallest (presumably insignificant) effect estimates.
	
	So... Lenth proposed a method for estimating the standard deviation of the effects on the basis that if the coded factors $x_i$ (for simplification reasons but it also applies to non-coded and non-normalized effects) have their effects that follow a Normal distribution $E_i=\mathcal{N}(0,\sigma)$ then the median of the absolute value of the coded effects $|E_i|$ is in theory equal to the half-Normal distribution (\SeeChapter{see section Statistics}):
	
	and again (!) this last relation can be easily generalized to non-coded effects (just return back to that proof and add a non-zero mean in the development of the proof)!
	
	What Lenth has proposed is first to eliminate all effects that satisfies:
	
	that is to say:
	
	denoted $s_0$.
	
	The R software gives with the \textbf{fdrtool} package:
	\begin{center}
	\texttt{>1-phalfnorm(2.5,theta=sqrt(pi/2),lower.tail=T,log.p=F)=0.012}
	\end{center}
	
	The reason why Lenth's made this choice are explained in a quite obscure way in its original article... in our point of view. His argument is that simulations shows that the $1.5M_e$ is overestimate in practice therefore eliminate factors with high amplitude effect seems to give a more accurate result.
	
	After the elimination the proposition is to calculate a pseudo standard error with the remaining effects (that therefore should converge to the $\sigma$ of all effects):
	
	where Lenth's assume that $\sigma_{\text{effect}}$ is the standard error of the mean and that therefore we can do inference using the Student T distribution such at an $\alpha$ bilateral threshold we have the following margin error
	
	where $m$ is the total number of effects and the division by $3$ comes from a Monte Carlo study that was conducted by Lenth.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	To make an example let us consider the following experiment that we already know (but we have removed the replicaton):
	
	We now already how to perform the calculations manually of the effect so we will not come back on this topic. We will just communicate the output given by Minitab 17.1.3:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/lenth_pse_minitab_effects.jpg}	
	\end{figure}
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Therefore in abolute value, we have:
	\begin{gather*}
		24.5, 5, 1, 1.5, 7.5, 2, 0.5
	\end{gather*}
	Ordered:
	\begin{gather*}
		0.5, 1, 1.5, 2, 5, 7.5, 24.5
	\end{gather*}
	So it is immediate that the median is equal to:
	
	So all the effects that are smaller or equal to $2.5(1.5M_e)=7.5$ are (Lenth takes strictly smaller but Minitab don't follow this rule):
	\begin{gather*}
		0.5, 1, 1.5, 2, 5, 7.5
	\end{gather*}
	Therefore:
	
	So finally::
	
	Where the $T$ value was obtained with R since Microsoft Excel don't accept fractional degrees of freedom:
	\begin{center}
	\texttt{>qt(.975,df=7/3)=3.764123}
	\end{center}
	Minitab 17.1.3 makes a summary of all calculations done above as following:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/lenth_pse_minitab_pareto.jpg}	
	\end{figure}
	\end{tcolorbox}
	
	\paragraph{Pareto Margin Error for replicated factorial designs}\mbox{}\\\\
	Let us recall that we have studied the following replicated experiment earlier above:
	
	and that we get by hand exactly the same results as given by Minitab 17.1.3:
	\begin{figure}[H]
		\begin{center}
		\includegraphics[scale=1]{img/engineering/replicated_design_minitab_analysis.jpg}
		\end{center}	
		\caption[]{Replicated factorial design effects C.I. with Minitab 17.1.3}
	\end{figure}
	It must be remembered to for the constant and all coefficients the $T$-value was respectively of the form:
	
	Therefore we see that all Student's $T$ distribution have always the same degrees of freedom!!! That will be written:
	
	So we could use the limit $T$-value (we have seen earlier above how to calculate it) in absolute value to compare the amplitude of every factor. And this is what Minitab give as output when the design is replicated:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/replicated_design_minitab_pareto_plot.jpg}
		\caption[]{Replicated factorial design $T$-value with Minitab 15.1.1}
	\end{figure}
	Now the question that arise is: What is this red line and how it is calculated?
	
	The choice (not rigorously justify as far as I know) used by most softwares is to compare all these values by the with the quantile $T_{\alpha/2,2^k}$ that gives in our case with Microsoft Excel 14.0.7177:
	\begin{center}
		\texttt{=TINV(97.5\%,8)=2.30600414}
	\end{center}
	and this is the value of the red line we can see on the plot above.
	
	\paragraph{Desirability}\mbox{}\\\\
	The "\NewTerm{desirability function}\index{desirability function}" approach to simultaneously optimizing multiple equations was originally proposed by Edwin C. Jr Harrington (1980). Essentially, the approach is to translate the functions to a common scale ($[0, 1]$), combine them using the geometric mean and optimize the overall metric. The equations may represent model predictions or other equations.

	For example, desirability functions are popular in response surface methodology as a method to simultaneously optimize a series of quadratic models. A response surface experiment may use measurements on a set of outcomes. Instead of optimizing each outcome separately, settings for the predictor variables sought to satisfy all of the outcomes at once.

	Also, in drug discovery, predictive models can be constructed to relate the molecular structures of compounds to characteristics of interest (such as absorption properties, potency and selectivity for the intended target). Given a set of predictive models built using existing compounds, predictions can be made on a large set of virtual compounds that have been designed but not necessarily synthesized. Using the model predictions, a virtual compound can be scored on how well the model results agree with required properties. In this case, ranking compounds on multiple endpoints may be sufficient to meet the scientist's needs.
	
	Originally, Harrington used exponential functions to quantify desirability. In this text we will introduce simple discontinuous functions of G. Derringer and R. Suich.

	Suppose that there are $N$ equations or function to simultaneously optimize, denoted $f_i(\vec{x})$ with $i= 1 \ldots N$. For each of the $i$ functions, an individual "desirability" function is constructed that is high when $f_i(\vec{x})$ is at the desirable level (such as a maximum, minimum, or target) and low when $f_n(\vec{x})$ is at an undesirable value. Derringer and Suich proposed three forms of these functions, corresponding to the type of optimization goal. For maximization of $f_i(\vec{x})$, the function:
	
	can be used, where $L$ is the lower limit that we allow to our variable of interest, $H$ its high (maximal) value that we allow, and $w$ is the weight that are all three chosen by the investigator. When the equation is to be minimized, they proposed the function:
	
	and for target $T$ situations:
	
	These functions are on the same scale (output in range $[0,1]$) and are discontinuous at the points $L$, $H$, and $T$. The values
of $w$, $w_1$ or $w_2$ can be chosen so that the desirability criterion is easier or more difficult to satisfy. The scaling factors $w$ are useful when one equation is of great importance than the other. Examples of the functions are given in the figure below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/desirability.jpg}
		\caption{Derringer and Suich Desirability functions plot (source: Max Kuhn R Software)}	
	\end{figure}
	It should be noted that any function can be used to mirror the desirability of a model. For example, Del Castillo, Montgomery, and McCarville (1996) develop alternative desirability functions that can be used in conjunction with gradient based optimization routines.

	Given that the $N$ desirability functions $d_1,\ldots, d_r$ are on the $[0,1]$ scale, they can be combined to achieve an overall desirability function, $D$. One method of doing this is by the geometric mean\index{geometric mean} and the we speak of "\NewTerm{composite desirability}\index{composite desirability}":
	
	The geometric mean has the property that if any one model is undesirable ($d_i=0$), the overall desirability is also unacceptable ($D=0$). Once $D$ has been defined and the prediction equations for each of the $N$ equations have been computed, it can be use to optimize or rank the predictors.

	In Minitab (see our companion book on that software) the composite desirability is the geometric mean of the individual desirabilities if and only if the "importance" parameter  (whose value must be between $0.1$ and $10$) are all equals to $1$.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/optimizer_setup_minitab.jpg}
		\caption[]{Minitab 15.1.3 Design of experiment optimizer setup}	
	\end{figure}
	
	\pagebreak
	\subsection{Quality Control on Reception (Lot Acceptance Sampling Plans)}
	The "\NewTerm{quality control on reception}\index{quality control on reception}" or simply "\NewTerm{acceptance test}\index{acceptance test}" or "\NewTerm{statistical batch control}\index{statistical batch control}" or following NIST\footnote{National Institute of Standards and Technology} "\NewTerm{Lot Acceptance Sampling Plans (LASPs)}\index{Lot Acceptance Sampling Plans}" is an extremely important field, wide and mathematically technical of statistical quality control in the industry (food, pharma, consumer goods) and services companies (repetitive action, process controls, surveys, assessments). Its purpose, according to the norm NF X06-021, is to allow the application on a controlled batch (products or services) of one or the other following decisions: Acceptance or Rejection at different stages of a production / development to see if a batch is admissible for the following steps or to check the deliverable  before sending it to the customer.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/quality_control.jpg}	
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will introduce here only the basics of this subject because the particular cases require considerable mathematical background! It is also for this reason why many companies (SMEs and multinationals) that use interns or students that don't have a mathematical background to define quality controls in order to save money are found later with considerable problems in compliance with laws and quality standards! However, what we introduce here is should be the minimum-minimorum knowledge of any active  quality responsible or quality engineer or responsible for reception control of suppliers in any organization (industrial or administrative) otherwise they should have absolutely no credibility from people hiring such "specialists" or from external consultants! Also beware of companies - particularly multinationals - who are looking for quality specialists mastering Microsoft Excel or Microsoft Access. Because it will mean that they use non-professional tools to do a job which ought to be with him the appropriate tools (Microsoft Excel and Microsoft Access or are not) !!! So in terms of internal organization, you can ensure that these companies organize and analyze anything, anyhow, with an unsuitable tool and therefore that there is a general mess internally.
	\end{tcolorbox}
	The purpose of quality control on reception (acceptance sampling) is not to estimate the quality of a lot, but only to accept or reject it. The acceptance is not a substitute for process control methods such as control charts or capability analysis. A dynamic use of these tools during manufacturing will result in reducing and, in some cases, eliminate the need to perform incoming or outcoming inspection/controls.

	There are three main approaches to evaluate a batch:
	\begin{itemize}
		\item Either accept the batch without inspection (useful in situations where absolute trust relations exists between the customer and supplier or we now that defects are impossible).

		\item Either take a sample (sampling) and draw conclusions about the whole batch from the sample (useful in most cases because inexpensive and highly motivates the supplier to improve quality but there is a risk of rejecting good batches or accept bad one). For example Mulhouse PSA control $5\%$ of vehicles coming out of their factories every day.

		\item Either inspect all units in the batch (useful when the supplier can not prove that its process control and is capable of producing in the specifications or when the monitored characteristic is very critical).
	\end{itemize}
	For the result of a batch control by sampling to be scientifically reliable, it is trivially important that, when collecting a sample of individuals, no preference is given to individuals from the whole population. Ideally, each individual batch must have the same probability of being draw from the sample.

	The characteristics controlled can be:
	\begin{itemize}
		\item Qualitative (attributes): the appearance of a product, the presence or absence of a non-compliant element, failure to comply with a caliber of control, an achieved result  or not. Individuals are then directly classified as compliant or non-compliant identified by the statistical number of defects they have. 
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		In my job of consultant I saw once a customer that had for process to reject definitively any supplier that do one and only one first failure. This is a zero tolerance approach and is quite not scientific as I warn this customer... But when people don't have any statistical culture it's a waste of time trying to eplain them why it's probable not the best strategy...
		\end{tcolorbox}

		\item Quantitative (variables or measurement): characteristic measured on a continuous scale: volume, length, weight, ...
	\end{itemize}
	Let us notice finally that there are two ways to classify acceptance plans. The first family is to define the following two categories:
	\begin{enumerate}
		\item The "\NewTerm{control of the proportion of non-compliant items by counting}\index{control of the proportion of non-compliant items by counting}" or also simply named "\NewTerm{attribute control}\index{attribute control}": One or more characteristics of qualitative or quantitative type are controlled for each item in order to classify it as compliant or non-compliant according certain criteria. The decision is based on the number of non-compliant individuals found in the samples.

		\item The "\NewTerm{control of the proportion of non-compliant individuals by measuring}\index{control of the proportion of non-compliant individuals by measuring}" or or also simply named "\NewTerm{variable control}\index{variable control}". A characteristic is measured for each item and a decision is made based on the average and dispersion of the characteristic calculated on all the items collected.
	\end{enumerate}
	We can also classify the acceptance plans depending on the number of samples taken.
	\begin{itemize}
		\item A "\NewTerm{simple sampling plan}\index{simple sampling plan}" involves taking $n$ items in a batch of size $N$ and make a decision based on them.

		\item A "\NewTerm{double sampling plan}\index{double sampling plan}" involves taking a sample from the first batch and draw one of these three conclusions: (1) accept the batch, (2) reject the batch or (3) resample. If a second sample is taken, the information of the two samples are gathered to decide whether to accept or reject the batch (if we repeat this strategy, then we speak of "\NewTerm{multiple sampling plan}\index{multiple sampling plan}"). The idea is illustrated in the following figure:
		\begin{figure}[H]
			\begin{center}
			\includegraphics[scale=0.75]{img/engineering/double_sampling_plan.jpg}
			\end{center}	
		\end{figure}
		\item A "\NewTerm{progressive or sequential sampling plan}\index{progressive or sequential sampling plan}" is the extreme case of multiple sampling plan. The items are removed one by one and after each sampling a decision is made: (1) accept, (2) reject or finally (3) take a new item.
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A soft drink producer wants to control a property of the bottles are supplied to him. For this, it uses a simple reception plan by monitoring the proportion of nonconforming items by a measuring when a batch of bottles are delivered to him:

	\begin{enumerate}
		\item Collect a number $n$ of units (bottles) of the batch (single sampling)

		\item Measure pressure at which each of the $n$ bottles broke (destructive testing)

		\item Calculate the statistical indicators of the observed values.

		\item Follow the acceptance decision rule or rejection that will be determined.
	\end{enumerate}
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As for the control charts we will see further below, applying a sampling plan is a hypothesis test. It can lead to right or wrong decisions. The risks are those of accepting a non-compliant batch (Type II Error) or to reject a batch that is compliant (Type I Error).
	\end{tcolorbox}
	Obviously sampling has advantages and disadvantages (pros \& cons if you prefer). Here is the list of the most common advantages:
	\begin{enumerate}
		\item It's cheaper because you do not control everything ...

		\item There is less manipulation of the products thus less waste

		\item There needs to mobilize less people or control machines

		\item This dramatically reduces inspection errors

		\item The return of a whole batch just because of an non-compliant sample tends to motivate more the supplier to the quality ...
	\end{enumerate}
	But it also has disadvantages such as:
	\begin{enumerate}
		\item The risk of accepting non-compliant batches (type II error) and reject good one (Type I error)...

		\item Requires more intellectual skills of employees working in quality department (mathematical capacities...) and well-written contracts ...
	\end{enumerate}
	
	\subsubsection{Simple acceptance sampling plan by measurement for a unique tolerance with known standard deviation}
	For this type of sampling plan, we will assume that the sample $n$ is much smaller than the total batch of size $N$ (a factor of $10$ at least) such that $n\ll N$ and the standard deviation $\sigma$ is known!
	
	The characteristic $X$ measured for each item of the sample is assumed to follow a Normal distribution with  identical mean $\mu$ and standard deviation $\sigma$ (or variance) and the calculation of these last two parameters is based on the use of the sample mean (estimator of maximum likelihood of the mean as seen in the section Statistics) and of the unbiased estimator of the standard deviation (maximum likelihood estimator of the standard deviation as seen in the section Statistics). This hypothesis implies that the product of supplier is manufactured under statistical control!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In our example, the producer of soft drinks, he gets lots of $10,000$ bottles (so we will consider that the sample satisfy the above condition of $n \ll N$).
	\end{tcolorbox}
	We write (\SeeChapter{see section Statistics}):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The proper way to compute probability of acceptance should be to use the hypergeometric distribution as we have seen in the section Statistics. But if the lot size is large relative to the sample size, the binomial distribution may be used. If the probabilities are quite small the binomial distribution can be approximated by a Normal distribution. This approximation is quite satisfactory if the lot size is more than ten times the sample size and non-compliant items are rare. In other words, if the lot size $N$ is large enough to be declared as infinite, the distribution of the number of non-compliants in a random sample of $n$ units will be binomial with parameters $\mu=np$ and $\sigma=np(1-p)$ as proved in the section Statistics.
	\end{tcolorbox}
	We will agree that the customer and the supplier have agreed on specification limits (tolerances). We denote will denote as usual these limits USL and LSL.

	Given $D$ the proportion (unknown!) of non-compliant products (defective), it is naturally given in the field statistical processes control by:
	
	In practice, in order to save money in the context of quality tests, we try to reduce (reformulate) any control problem to a single caliber. In fact the problem can always with or without reformulation be reduced to have a proportion of non-compliant below the LSL or above USL to reject the batch if the supplier's production is in statistical control. Hence the fact that in practice the mathematical developments are made only in relation to a single bound which we will denote by BSL for "\NewTerm{batch specification limit}\index{batch specification limit}".

	Thus, we reduce the problems of single sampling (or we manage the problems such that it can be reduced to such a formulation):
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In the context of our companion example, the producer of soft drinks, it is the pressure resistance of a batch of $10,000$ bottles that we would like to control. The producer has agreed with its supplier to a lower specification limit for this property, choosen at $22.5$ limits a majority of the bottles must exceed without blow up. So:
	\begin{gather*}
		D=P(X<LSL)=P(22.5<LSL)
	\end{gather*}
	We will assume that past data were used to estimate precisely the standard deviation of the blow up pressure limit and which can therefore be considered as known and equal to $1.5$ and is under statistical control.
	\end{tcolorbox}
	Because obviously if the proportion of non-compliant items is more (or less) that defines the quality contract in a boud, it will be also the same the opposite bound in the same proportions as the supplier is supposed to have its manufacturing process under statistical control!!!

	Before going further into the mathematical developments, there is an important thing to remember:

	If we imagine ourselves to be a supplier, it is obvious that we would like to have a level of quality such that the cumulative probability that the customer mistakenly reject a batch is small (because as a supplier we are contractually obliged to take the back the batch even if a full second sampling control later shows that he seems to be wrong!). For this purpose, the supplier defines a value $D_s$ of the percentage of non-compliant products, named "\NewTerm{Acceptable Quality Level (AQL)}\index{Acceptable Quality Level}" below which he assumes that the batch may be rejected only very rarely. In addition, it also defines $\alpha$ as the maximum cumulative probability of seeing a batch being reject that has a percentage of non-conformity less than or equal to $D_f$ (so it is the cumulative probability that a customer would reject by error a batch). This is written as:
	
	more of often written:
	
	where is named course ... "\NewTerm{suppliers's risk}\index{suppliers's risk}" and is generally in the range of $0.1$ to $10\%$.
	
	Finally, if we imagine that we are the customer, it is obvious that we would like the cumulative probability of wrongly accepting a batch that does not meet the contract quality to be quite small for obvious cost reasons. For this, the customer must on his side defines a value $D_c$ of the proportion of non-compliant items, know under the name "\NewTerm{Quality Level Limit QLL}\index{Quality Level Limit}" or following NIST "\NewTerm{Lot Tolerance Percent Defective LTPD}\index{Lot Tolerance Percent Defective}", beyond which it considers that the batch can be accepted wrongly only rarely. He also defines $\beta$ as the maximum cumulative probability of having to accept a batch that has a proportion of non-compliant item greater than or equal to $D_c$ (so it is the cumulative probability that wrongly accept a batch of the supplier) . This is written as:
	
	more often written:
	
	where $\beta$ is named obviously ... "\NewTerm{customer risk}\index{customer risk}" and is also generally in the range of $5$ to $10\%$.
	
	To summarize the situation may be illustrated by the following table (which is similar in all point to the table of Type I and II errors in already seen in the section Statistics):
	\begin{center}
	  \renewcommand{\arraystretch}{2.6}
	  \begin{tabular}{|l|c|c|c|}
	  \hline
	    \cellcolor{black!30}   & \multicolumn{2}{|c|}{\cellcolor{black!30}\textbf{State of Nature}} \\ \hline
	\cellcolor{black!30}\textbf{\parbox{3.5cm}{Decision resulting\\ from sampling}} & \parbox{5cm}{The batch is not good (QLL)\\\centering ($H_0$ \textbf{false})} & \parbox{5cm}{The batch is good (AQL)\\\centering ($H_0$ \textbf{true})} \\ \hline
	\textbf{Reject (batch) $H_0$} & \cellcolor{green!30}\parbox{5.5cm}{Correctly reject null decision\\ \centering($1-\beta$: Power of the test)} & \cellcolor{red!30}\parbox{3cm}{Type I Error\\ \centering(Risk $\alpha$)} \\[3ex] \hline
	\textbf{Fail to reject (batch) }$H_0$ & \cellcolor{red!30}\parbox{3cm}{Type II Error\\ \centering(Risk $\beta$)} & \cellcolor{green!30}\parbox{4.5cm}{\centering Correct decision}  \\[3ex] \hline
	  \end{tabular}
	\end{center}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	In the context of our companion example - the producer of soft drinks - which receives batches of $10,000$ bottles whose pressure resistance (BSL) must be greater than $22.5$ with a standard deviation $\sigma$ under statistical control equal to $1.5$, the supplier and the customer each have fixed the risks they are ok to take:
	\begin{itemize}
		\item The supplier would like that a batch with less than $1\%$ of non-compliant to be rejected by error by the customer to be at maximum of $5\%$ of cases (cumulative probability of type I error). Thus:
		\begin{gather*}
			P(\text{Reject a batch}|D\leq \text{AQL})\leq \alpha\Rightarrow  P(\text{Reject a batch}|D\leq 1\%)\leq 5\%
		\end{gather*}

		\item The customer would like that a batch withmore than $5\%$ of non-compliant bottles to be wrongly accepted in less than $10\%$ of cases (cumulative probability of type II error). Thus:
		\begin{gather*}
			P(\text{Accept a batch}|D\leq \text{QLL})\geq \alpha\Rightarrow  P(\text{Accept a batch}|D\geq 5\%)\leq 10\%
		\end{gather*}
	\end{itemize}
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The decision rule to accept or reject a batch for a control by measurement is generally based on the estimated arithmetic mean of the characteristic of the sample rather than based on a estimate of non-compliant proportion.
	\end{tcolorbox}
	For what follows, when we write:
	
	the specificity is that in practice $D$ is requsted and $\sigma$ is given. The remainder is either to calculate or to eliminate (indeed the quality practitioner should normally only talk in terms of cumulative probability and proportions or factors  to define its quality policy and LSL is not a cumulative probability neither a percentage or a factor independent of the thing that we analyze!). Thus we write:
	
	So with the usual score (centered reduce variable as seen in the section Statistics):
	
	Obviously we draw from this that limit average value of the sample is:
	
	where $Z_D$ is the level $D$ of percentile of the standard centered Normal distribution (as usual). Unfortunately, we still have LSL that is there. Can we eliminate it? For this we have in the context that if the measurement (or manufacturing) is under statistical control:
	
	The cumulative probability to accept (we could have done it for the rejection but we had to choose one of the both for the example...) the batch when the non-compliant rate is $D$ is deducted in the context of using the average of the measurements becomes:
	
	in the special case where the measurement should not be less than some given value BSL (as is the case of the above example with the soft drink bottles!). Obviously $k$ could be a real positive or negative number as we know but not equal to $Z_D$ and we will how to found a way to determine it from the supplier and customer risk level!!!

	Then we have:
	
	Then we have a very interesting result! The cumulative probability to accept the batch when the non-compliant rate is $D$ is then given in the particular case there where the measurement should not be below a specific limit by:
	
	and the probability to reject it in the case of a measure beyond which we should not go:
	
	and what is good in this result is that the problem is then reduced to a percentile  $Z_D$ that is easy to choose (norm or policy choices), of the sample size $n$ and a factor $k$ so only independent elements of the type of measurement itself and that can then be used in any job for any type of object and facilitates the writing of contract (plus we have eliminated the explicit use of the standard deviation) !!

	Obviously, as we know, the supplier and the customer must agree to a sampling plan corresponding to build quality tests meeting expectations of the one and the other. What we know so far is that they respectively require an $\alpha$ (supplier risk) and a $\beta$ (customer risk) that allows them to immediately calculate the $Z_{D,\alpha},Z_{D,\beta}$ respectively. However it is necessary to calculate the number of individuals $n$ sampled and the factor $k$ that satisfy the respective requirements.

	To do this, remember that if $\alpha$ is the supplier risk (cumulative probability that the customer mistakenly rejects the batch) and the $\beta$ the customer risk (cumulative that he wrongly accepts supplier batch) and that:
	
	the cumulative probability of accepting a batch with a lower limit, then we must (!) have:
	
	or after rearrangement:
	
	To understand why we write these both relations the reader can take the two extreme situation where $\alpha=\beta=0$ and $\alpha=\beta=1$ (with their respective $Z_{D,\alpha},Z_{D,\beta}$) to better understand (we can detail on request with an explicit example).

	If we solve this system we can get the $n$ and $k$ we are looking for so that the customer and supplier get the sampling plan their both expecting!

	So we can rewrite the previous system as:
	
	Therefore:
	
	from which we can take out $n$:
	
	Finally:
	
	and for $k$:
	
	So that finally:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. In the context of our companion example - the producer of soft drinks - which receives batches of $10,000$ bottles whose pressure resistance (LSL) must be at least of 22.5 with a standard deviation under statistical control equal to $1.5$, remember that the supplier and the customer each have fixed the risks they are ok to assume such that:
	\begin{gather*}
			P(\text{Reject a batch}|D\leq \text{AQL})\leq \alpha\Rightarrow  P(\text{Reject a batch}|D\leq 1\%)\leq 5\%\\
			P(\text{Accept a batch}|D\leq \text{QLL})\geq \alpha\Rightarrow  P(\text{Accept a batch}|D\geq 5\%)\leq 10\%
	\end{gather*}
	We then have using Microsoft Excel 11.8346:
	
	and (calculated with the same Microsoft Excel functions so we omit to explicit them again):
	
	The sampling plan is then to collect and analyze a sample of size $n=18$, calculate the arithmetic average $\bar{X}$ and apply the following decision rule:
	
	Let us suppose now that a sample size of $18$ is taken and gives the following results:
	\begin{gather*}
		25.6, 26.0, 23.0, 27.0, 27.5, 29.0, 28.5, 26.0, 25.6\\
25.8, 26.5, 28.8, 27.3, 25.2, 27.1, 29.8, 26.5, 27.8\\
	\end{gather*}
	Then we have:
	\begin{gather*}
		\bar{X}=26.8\geq 25.4
	\end{gather*}
	and therefore the batch is accepted. You should know that many practitioners prefer the following calculation using a "\NewTerm{quality index}\index{quality index}", denoted $Q$, simply using the following equation:
	\begin{gather*}
		Q=\dfrac{\bar{X}-LSL}{\sigma}=\dfrac{26.8-22.5}{1.5}\cong 2.86>k
	\end{gather*}
	therefore the batch is accepted.
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In the latter case it is still recommended in practice to calculate the unbiased estimator of the standard deviation of the sample to ensure it is not too far from the standard deviation of the contract (even if it may not be the same for small samples)!
	\end{tcolorbox}	
	E2. A second typical calculation made by practitioners (often just for curiosity or sometimes to make a quality report to the board committee) resulting from the first example is: What is the cumulative probability that we reject with $n$ and $k$ given, a batch of which the proportion of non-compliant $D$ would be $5\%$? Then, we use the following proved relation:
	\begin{gather*}
		P_D=P(Z\leq \sqrt{n}(k+Z_D))
	\end{gather*}
	Thus with the previous values:
	\begin{gather*}
		P_D=P(Z\leq \sqrt{18}(1.943+\texttt{NORMSINV(5\%)}))\cong 89.70\%
	\end{gather*}
	and the cumulative probability that we accept the batch is then of $1-89.70\%$ or $10.29\%$.
	\end{tcolorbox}
	
	\paragraph{Calculation of the parameters using the norms AF-X06-023}\mbox{}\\\\
	The norm AFNOR X06-023 offers a slightly different method to determine $k$ and $n$, not directly based on the customer's risk and the supplier's risk, but only base on the "Acceptable Quality Level" (AQL.). In addition, this norm also takes into account the size $N$ of the tested batch (considered previously as infinite) and of the desired level of control.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The equivalent international norm to the  AFNOR X06-023 seems to be the ISO 3951:1981 that to my knowledge gives very different values from that of the AFNOR for reasons that I don't understand quite well (I hope it's just because I did not understand something...).
	\end{tcolorbox}	
	The method proposed by AFNOR X06-023 is of of the type "recipe" but is requested by certain European standards so the industry have no choice but to apply it. But at the end, the numerical values are relatively close to those obtained with the previously demonstrated relations. In fact their real usefulness is that they are simple to implement in much more complex situations than those we have just studied.

	Depending on the context, the norm proposes to apply different levels of control:
	\begin{itemize}
		\item The "normal control" that has to be chosen when we are in close trusted relation with the supplier and we have a look in real time on the quality of its production using computer remote monitoring tools .

		\item The "reinforced control", which is stricter than the normal control ($n$ is greater) and is intended to better protect customers against the risk of having to accept bad batches (Type I Error). This type of control should be performed temporarily when there are serious reasons for considering that the production quality is not (or is no more) what is should be until we return back to the normal (deliveries are suspended between this time).

		\item Finally, the "reduced control" which is the most economical one and can be applied when based on previously tested and accepted batches, we can believe that the supplier master well the quality its processes
	\end{itemize}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. In the context of our companion example - the producer of soft drinks - which receives batches of $10,000$ bottles whose pressure resistance (LSL) must be at least of 22.5 with a standard deviation under statistical control equal to $1.5$, remember that the supplier and the customer each have fixed the risks they are ok to assume such that:
	\begin{gather*}
			P(\text{Reject a batch}|D\leq \text{AQL})\leq \alpha\Rightarrow  P(\text{Reject a batch}|D\leq 1\%)\leq 5\%\\
			P(\text{Accept a batch}|D\leq \text{QLL})\geq \alpha\Rightarrow  P(\text{Accept a batch}|D\geq 5\%)\leq 10\%
	\end{gather*}
	We must then use the norm AFNOR X06-023 with a NQA of $1\%$.\\

	The norm requires the choice of a control level . We will take that of type II. A table in this norm tells us that we must use the code \textbf{L} for the next step. The final table thus gives us for a NQA of $1\%$ and a $L$ coll the following cell:
	\begin{center}
	  \begin{tabular}{|c|c|c|}
	  \hline
	    \multicolumn{2}{|c|}{$1.06\%$} \\ \hline
		$n=25$ & $k=1.97$ \\ \hline
		\multicolumn{2}{|c|}{$4.28\%$} \\ \hline
	  \end{tabular}
	\end{center}
	corresponding to:
	\begin{center}
	  \begin{tabular}{|c|c|c|}
	  \hline
	    \multicolumn{2}{|c|}{$D_s$} \\ \hline
		$n$ & $k$ \\ \hline
		\multicolumn{2}{|c|}{$D_c$} \\ \hline
	  \end{tabular}
	\end{center}
	or more traditionally:
	\begin{center}
	  \begin{tabular}{|c|c|c|}
	  \hline
	    \multicolumn{2}{|c|}{\textbf{AQL}} \\ \hline
		$n$ & $k$ \\ \hline
		\multicolumn{2}{|c|}{\textbf{LTPD (QLL)}} \\ \hline
	  \end{tabular}
	\end{center}
	Therefore, the norm indicates that a sample size $n$ of $25$ should be used and a $k$ of $1.97$! The plan insures risks slightly smaller than the plan calculated above but is more expensive because requires bigger samples.
	\end{tcolorbox}
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E2. Practitioners when they use norms often want to calculate the customer and supplier risk associated to the selected AQL and the corresponding parameters given by the norm. Then using:
	
	It comes:
	
	Either as part of our example and always using Microsoft Excel 11.8346:
	
	Therefore:
	
	to be compared to the $5\%$. As well as:
	
	Either as part of our example and always using Microsoft Excel 11.8346:
	
	it comes:
	
	and finally:
	
	to be compared to the $10\%$.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Simple acceptance sampling plan by attribute}
	The attributes acceptance plans are simpler than the sampling plans by measurement, but they are often more expensive (in number of samples) than acceptance plans by measurement and are therefore advise only when we can not summarize information on the quality of a unit by measurement or that the cost of analyzes my measurement are too high compared to a simple classification of compliant or non-compliant units.

	The "\NewTerm{simple acceptance plan by attributes}\index{simple acceptance plan by attributes}", as already mentioned, is an acceptance test procedure the consist to draw a random sample of a given size $n$ in a batch. The sampled units (in a homogeneous way!) rre then inspected and classified as compliant or non-compliant (avoid using the terme "defective" as it is not same!). 

	Given $X$ be the number of non-compliant components found in the sample. If $X\leq A$, the quality of the batch is considered good and the batch is accepted. If $X\geq R$ with in the case of simple acceptance plan by attribute: $R = A+1$ the batch quality is considered as poor and the batch is not accepted.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Bottles by batch size of $N = 10,000$ are received by a customer. The control procedure used empirically a simple acceptance plan by attribute: it collects a sample of size $n = 200$. The acceptance criterion $A$ is $X\leq 1$ and the rejection criterion $R$ is $X\geq 2$.
	\end{tcolorbox}
	Let us recall before going futher into the mathematical details of the calculations of the strategy of a single sampling plan by attributes the principles of double and multiple sampling plans by attributes for the general culture:
	\begin{itemize}
		\item The "\NewTerm{double acceptance plan by attributes}\index{double acceptance plan by attributes}" is an acceptance test procedure having two stages of decision. It involves first taking a first sample size of a size smaller  than the one we would collect for a single acceptance plan. The first stage of decision is based on information provided by the first sample. If the quality of the first sample is considered good enough, the batch is accepted. If judged bad enough, the batch is not accepted. However if the quality of the first sample is judged as intermediate, a second sample is taken and inspected in order to accept or reject the batch. the second stage of the decision criteria are based on information collected on the two samples.
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Bottles by batch size of $N = 10,000$ are received by a customer. The control procedure used empirically the following acceptance plan by attributes: it takes a first sample size $n_1=125$. The acceptance criterion $A$ in the first stage is the $X_1=0$ and the rejection criterion $R$ is $X_1\geq 2$. If the quality is not satisfactory, we take a second sample of size $n_2=125$ with an acceptance criterion $A$ in the second stage of $X_2\leq 1$ and the rejection criterion $R$ is $X_2\geq 2$.
		\end{tcolorbox}
		We will come back on a much more detailed and formal example of double acceptance plan by attribute further below!!
		
		\item The "\NewTerm{multiple acceptance sampling plan by attribute}\index{multiple acceptance sampling plan by attribute}" is simply a generalization of the double acceptance sampling plan by attributes having $D$ decision steps instead of $1$ for the simple acceptance plan or $2$ for the double acceptance plan...
	\end{itemize}
	Let us come back to our mathematical developments of a single sampling acceptance plan by attributes. If we denote now $n$ the batch size, $p$ the sample size and $m$ the total number of defective in the batch, then the probability of having $k$ non-compliant items among $p$ is (always using as from the beginning of this book a notation for the binomial coefficient that is not-compliant with the norm ISO 31-11):
	
	To be consistent with the tradition of the field of acceptance plan, let's first change these notations. Let us denote by $n$ the sample size and $N$ the batch size. It comes then:
	
	Let's do a second change of notation. Let us denote by $p$ the proportion of non-compliant items in the batch. It then comes the traditional customary notation:
	
	Thus, the cumulative probability of accepting a batch of quality $p$ is given by:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	What is the cumulative probability of drawing $5$ non-compliant items (acceptance criterion) of a batch of $10,000$ bottles ($N$) from which we took a sample of $200$ individuals ($n$), the proportion of non-compliant is known to be of $5\%$ ($p$). So with Microsoft Excel 14.0.6123 we get as cumulative probability of acceptance:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A classic common mistake business that I see also Quality Manager of the best luxury international brand based in Switzerland is to think that the acceptance criteria is proportional to the batch size. In other words that if for $10,000$ bottles, the fact to a test sample of $200$ itmes ($2\%$) gives a cumulative probability of acceptance of $\cong 6\%$, then on $1,000$ bottles, a sample of $20$ items (still $2\%$) gives the same cumulative probability of acceptance. In fact, with Microsoft Excel 14.0.6123, we then not have $6.05\%$ anymore but $99.7\%$ (!!!)... the difference is then considerable !!
	\end{tcolorbox}	
		If we recall (see above the simple acceptance plans by measurement) that if $\alpha$ is the supplier risk (cumulative probability that the customer rejects by error the batch whose proportion of non-compliant items is equal to $p_\alpha$: type I Error) and $\beta$ the customer risk (cumulative probability of accepting by error a batch of a supplier whose proportion of non-compliant items is $p_\beta$: type II Error), then so that the supplier and the customer have the sampling plan corresponding to each of their quality requirements we have to solve following system:
	
	As far as we knoe it is not possible to solve this system analytically. The techniques of operational research techniques  (\SeeChapter{see section Theoretical Computing}) using the simplex method, the gradient method or Newton method fail miserably in finding a solution to this system (which is quite normal in fact...). By cons, with evolutionary algorithms (tool available in Microsoft Excel 14.0.6123 or also in MATLAB) we can find acceptable solutions, but the settings are not easy.

	Therefore, assuming that the size of batches submitted for inspection is large relative to the sample size $N\gg n$, and using the binomial approximation of the hypergeometric distribution (approximation proved in the section Statistics), we have:
	
	The advantage of this approximation is huge! The total size $N$ of the batch is no longer involved in the problem!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. What is the cumulative probability of drawing $5$ non-compliant items (acceptance criterion) of a batch of $10,000$ bottles ($N$) from which we took a sample of $200$ individuals ($n$), whose proportion of non-compliant is known to be as equal to $5\%$ ($p$). While  Microsoft Excel 14.0.6123 we get:
	
	E2. The latter relation is also used in reliability engineering (see earlier above)!!! Indeed, suppose we wish to know the number of elements $n$ we need to take to make a reliability demonstration plan showing with $90\%$ of cumulative probability that the number $A$ of failures  (non-compliance) is equal to zero knowing that their reliability is $80\%$. Then this is equivalent to search $n$ such that we are the closest to:
	
	Therefore:
	
	So the nearest value $n$ for this is to take $11$ items (this is the value that also returns the worldwide reference reliability software what Weibull++). We are then let make use of the same previous calculation:
	\begin{center}
		\texttt{BINOMDIST(0,11,20\%,1)}$=8.59\%$
	\end{center}
	Within the area of reliability, it is customary to name this approach "\NewTerm{non-parametric binomial demonstration test plan of the  reliability}\index{non-parametric binomial demonstration test plan of the  reliability}"... (we will come back on this subject further below).\\
	
	E3. Let us see now a last example relatively to reliability. Remember that we have proved earlier above that for the Weibull distribution with one parameter, we had:
	
	And let suppose that we previous tests have shown that at a time $t$ equal to $2,000$ days, we had a reliability of $80\%$ and we know that the shape parameter $\beta$is equal to $2$. We would like to know what sample size we need to take for a reliability test of $1,500$ days shows with maximum $90\%$ cumulative probability $1$ non-compliance (that is: $1$ failure). For this, we first determine the scale parameter:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Then it comes for this same test at $1,500$ day a reliability of:
	
	By injecting into the binomial law, we then have:
	
	Solving for $n$, we get the integer value $n$ of $32$ items (the same value as that returned the by the worldwide reference reliability software Weibull++). We then left with:
	\begin{center}
		\texttt{= BINOMDIST (1, 32, 1-88.2\%, 1)} = 10.50\%
	\end{center}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Again... a common (beginner) mistake that is classic in corporation is to think that the acceptance criterion is proportional to the batch size. In other words that if if for $10,000$ bottles, the fact to test a sample of $200$ items ($2\%$) gives a cumulative probability of acceptance of $\cong 6\%$, the a on a batch of $1,000$ bottles, a sample of $20$ itmes ($2\%$) gives the same cumulative probability of acceptance... In fact, with Microsoft Excel 14.0.6123, we have don't have $6.23\%$ anymore but $99.7\%$... the difference is huge!!
	\end{tcolorbox}	
	\end{tcolorbox}
	Returning to the theory .... the system we had then with the Hypergeometric law can be written when $N\gg n$ as:
	
	But once again, it is not possible (as far as we know) to solve the previous system analytically. In 1967, HR Larson then created a chart of the cumulative binomial frequently referred to as the "\NewTerm{Larson's binomial nomograph}\index{Larson's binomial nomograph}" (I never found or seen an equivalent for a hypergeometric law unfortunately...) which allows approximately to solve the problem of determining the value of $A$ and $n$ for the acceptance plan:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/larson_binomial_monograph.jpg}
		\caption{Larson's Binomial Monograph (source: Montgomery)}	
	\end{figure}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A batch of bottles is delivered under the form of $10,000$ units corresponding to $N$. We seek to establish an approval control acceptance plan by attributes with the cumulative probability (supplier risk) $\alpha$ of $1\%$ that the customer rejects the batch by error with less than $2.5\%$ ($p_\alpha$) of non-compliant. At his side the customer wishes a cumulative probability (customer risk) $\beta$ of $10\%$ of accepting by error a batch with more than $5\%$ ($p_\beta$) of non-compliant (in fact the requirements are independent of the size $N$ of the batch).\\

	The goal is therefore to determine the value of $A$ and $n$ using the nomograph. For this, as shown in the nomograph, we must draw two lines:

	\begin{itemize}
		\item Starting points of the two lines: $0.99$ ($100\%$ -$1\%$) and $0.1$ ($10\%$) of the right axis.

		\item Finishing point of the two lines: $0.025$ ($2.5\%$) and $0.05$ ($5\%$) of the left axis.
	\end{itemize}
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	and the intersection of the two lines will give the desired parameters. Thus in this case:
	
	We also see that if we put these two values inside:
	
	with Microsoft Excel, we're actually far from the expected result! So the error of the nomograph can lead to the impression of rough approximations! But change in Microsoft Excel  the value $30$ by $27$ and the value $700$ by $695$ and you'll see what you can conclude...
	\end{tcolorbox}
	We also see through this example that an attribute acceptance plan is actually also much more expensive than a acceptance plan by measurement (since we took the same values of $\alpha$ and $\beta$ as for the example with the same parameters as during our study of the acceptance plans with measurement but we have $n=700$ instead of $n\cong 18$).
	
	The reader will also have perhaps notic that $N$ does not affect the value of $A$ and $n$ when we use the Larson's nomograph. So if we use computer tools to find the solution, we can therefore put that $n$ is between $0$ and $1,000$ and $A$ between $0$ and $150$ as we can see it on the nomograph.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Finally, let indicate that some use the Poissonian approximation of the binomial distribution (when the probability (proportion) of non-compliants is very small and that the batch size is very big and sometimes even when these criteria are far from being respected ...) following:
	
	\end{tcolorbox}
	
	\paragraph{Calculation of the parameters using the norm ISO 2859-1}\mbox{}\\\\
	The norm ISO 2859 (which derives from the norm MIL 105E) is dedicated to sampling procedures for inspection by attributes. Along with the norm AFNOR X06-023 it is based on the concept of AQL (percentage of non-compliant items who should not be exceeded for a production, controlled on a series of batches, so that it can be considered ass satisfactory).

	Following this norm, the NQA must be one of the $26$ values recommended by the norm. We will choose for example here the inspection level number II of this norm (they are described in the norm documentation). With respect to the chosen effective, as the AFNOR norm, ISO 2589 provides a symbol (letter) which will then be used in a table. Finally, depending on the type of acceptance plan (simple, double or triple) we find the parameters $A$ and $n$.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. In the context of our companion example - the producer of soft drinks - which receives batches of $10,000$ bottles, the AQL quality level is $1\%$. As for example with the AFNOR norm, we will take a type II control based on a simple acceptance plan.

	A table in the norm (see below) tells us that we must use the code $L$ for the next step. Then for simple control, the standard says to use to use the table IIA.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.5]{img/engineering/mil_acceptance_plan.jpg}
		\caption{MIL 105E acceptance sampling table}	
	\end{figure}
	We can then read that $n$ must be equal to $200$ and $A$ is equal to $5$. We therefore find that the value obtained is very different from that the one calculated theoretically. Honestly the reason for this difference is unknown to me...\\

	E2. Practitioners in the use of this norm often want to calculate the supplier and customer risk factors associated to the values of $n$ and $A$ provided by the norm in relation to the NQA. selected for a batch of a given quality. So as in this case, $200/10,000$ is smaller than $10\%$, we can use the binomial approximation of the hypergeometric law. Suppose we therefore wish to calculate the risk for the supplier of the previous example (the one with the use of the nomograph):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	hence a supplier risk of $100\% -61.59\%$ that is $38.40\%$ which is considerable! Thus, this sampling plan obtained via the norm is suitable for proportions of non-compliants much lower that $2.5\%$ (in fact less than $\cong 1.25\%$).\\
	
	For the customer risk, we proceed in the same way:
	
	which is here a more acceptable result. In fact we find that the norm is more appropriated to the criteria of the customer rather than that of the supplier.
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The software Quick Control Pro (used sometimes by the Swiss Watch industry) of the company Logystem SA manages the acceptance sampling according to the norms ISO 2859 and ISO 3951.
	\end{tcolorbox}
	
	\subsubsection{Double acceptance sampling plan by attribute}
	Now let us come back on the double acceptance sampling using the binomial approximation that if for recall:
	
	and let us consider a double sampling plan with:
	
	and where the proportion of non-compliant is known as being equal to $5\%$. So, in this case the cumulative probability of having, for example, at least $1$ non-compliant item at the first sampling and corresponding to the probability of accepting the batch is equal to:
	
	So far nothing special! Now to get the probability of acceptance of the batch on the second sampling, we list all the ways we can get the second sampling. 

	For this, suppose for example that the batch is accepted if and only if the cumulative number of non-compliant items overall the both sampling is between $2$ and $3$ units (arbitrary simplified choice for the example).
	
	We then have the following possible ordered combinations of drawing between the two samplings (notice that the sum is always between $0$ and $3$):
	\begin{gather*}
		 \{0,2\}, \{0,3\}, \{1,1\}, \{1.2\}, \{3.0\}, \{2.1\}, \{ 2.0\}
	\end{gather*}
	 But as already mentioned we accept the batch if and only the first sampling gives a number of non-compliant equal or less than $1$, it then remains only the following combinations:
	\begin{gather*}
		 \{3,0\}, \{2,1\}, \{2,0\}
	\end{gather*}
	to consider in this double acceptance sampling. So we need to calculate the following probabilities:
	
	
	\subsubsection{Operating characteristic curve (OC)}
	The "\NewTerm{operating characteristic curve (OC)}\index{operating characteristic curve}"  depicts the discriminatory power of an acceptance sampling plan (by measurement or by attribute!). The OC curve plots the probabilities of accepting a lot versus the fraction defective.

	If we assume that the lot size $N$ is very large, as compared to the sample size $n$, so that removing the sample doesn't significantly change the remainder of the lot, no matter how many defects are in the sample. Then the distribution of the number of defectives, $d$, in a random sample of $n$ items is approximately binomial as we know.

	When the OC curve is plotted, the sampling risks are obvious. You should always examine the OC curve before using a sampling plan.

	The idea in practice is to compare OC curves to help choose the appropriate sampling plan for various $n$ (sampling size) and $A$ (acceptance number often denoted $c$ in the US literature of "criterion").
	
	This curve that that is obtained by plotting on a graph the cumulative probability of acceptance of a batch in function of the proportion of it non-compliant item (named sometimes "effective quality" axes).

	We can build this curve for the Normal approximation, Binomial approximation or also we the exact case of the Hypergeometric law!
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	For a quality control by attribute acceptance plan, we apply the control plan on a sample of $n=700$ items, the batch will be accepted if the number of non-compliant items is less than or equal to $30$ ($A\leq 30$) or in percentages ($A\leq 4.28\%$) as determined in our previous example!\\

	We want to plot the cumulative probability to accept the batch for a range of non-compliant proportion between $0-7\%$ by step of $0.1\%$.\\
	
	Thus the purpose is explicitly to plot:
	
	That is often written in the US literature as:
	
	Therefore in our example:
	
	In a spreadsheet software like Microsoft Excel 14.0.7166 this gives (first rows only):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/oc_curve_binomial_excel.jpg}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	And if we do a plot adding at the same time the OC curve using the hypergeometric law and the ideal OC curve (sampling plan that discriminated perfectly between good and bad batches) we get:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/oc_curve_binomial_hypergeometric_ideal.jpg}
		\caption{Binomial, Hypergeometric and Ideal OC for AQL$=2.5\%$ and $n=200$}	
	\end{figure}
	Where remember that the values of $A$ and $n$ came from the fact that we seek to establish an approval control acceptance plan by attributes with the cumulative probability (supplier risk) $\alpha$ of $1\%$ that the customer rejects the batch by error with less than $2.5\%$ ($p_\alpha$) of non-compliant. At his side the customer wishes a cumulative probability (customer risk) $\beta$ of $10\%$ of accepting by error a batch with more than $5\%$ ($p_\beta$) of non-compliant (in fact the requirements are independent of the size $N$ of the batch).\\
	
	So as we can see, for $A=30$, $n=700$ we have almost $100\%$ probability to accept a batch that has between $0$ and almost $2.5\%$ proportion of non-compliant items. After the probability to accept a batch by error decrease as the real proportion of non-compliant items increase (this is quite obvious!).\\
	
	The reader can see in the Minitab companion book that we get the same binomial OC curve!
	\end{tcolorbox}
	An elegant way to represent the concepts of an operating curve without forgetting that in the binomial case:
	
	is as follows:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/oc_curve_summary.jpg}
		\caption{Operating Curve Summary}	
	\end{figure}
	The operating curve shows that the probability of accepting a defective batch decreases - for a constant batch size - when the percentage of defective items in the batch increases: when the quality is good, the probability of accepting the batch by error is high; when the quality is bad, the probability of accepting the batch by error is small.
	
	This type of operating curve based on large sampled  batches (dixit using the binomial distribution) is named in the case of using the binomial distribution "\NewTerm{operating  curve of type B (OC-B)}\index{operating  curve of type B}". If obviously the sample size is significant relatively to the size of the batch we will use the hypergeometric and we'll talk then of "\NewTerm{operating curve of type A (EC-A)}\index{operating curve of type A}".

	\subsubsection{Average outgoing quality (AOQ)}
	The batches not accepted by a sampling plan will usually be $100\%$ inspected or screened for nonconforming or defective units. After screening, nonconforming units may be rectified or discarded or replaced by good units, usually taken from accepted lots. Such a programmed of inspection is known as a "\NewTerm{rectifying inspection}\index{rectifying inspection}" or "\NewTerm{screening inspection}\index{screening inspection}". For those batches accepted by the sampling plan, no screening will be done and the outgoing quality will be
the same as that of the incoming quality $p$.

	Since the cumulative probability of accepting by error a bad batch is $ P_a$ , the outgoing batches will contain obviously proportion of $P_a\cdot p$ defectives. If only the nonconforming units found in
the sample of size $n$ are replaced by good ones, the "\NewTerm{average outgoing quality (AOQ)}\index{average outgoing quality}" (its an average proportion!!!) in in batches size of $N$ will be given obviously by:
	
	and for large $N$ we get the approximation:
	
	Using always our companion example with the bottle batch and using always the same spreadsheet software we have (first rows only):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/average_outgoing_quality_binomial_excel.jpg}
	\end{figure}
	and explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/average_outgoing_quality_binomial_explicit_excel.jpg}
	\end{figure}
	and the correspoding chart made made by selecting the both last columns:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/average_outgoing_quality_binomial_chart_excel.jpg}
		\caption{AOQ (Average Outgoing Quality) binomial chart $A=30$, $n=700$}
	\end{figure}
	The maximum ordinate of the AOQ curve is known as the "\NewTerm{average outgoing quality limit (AOQL)}\index{average outgoing quality limit}".
	
	So to close this subject we can that Minitab 17.3.1 gives us a resultat that is not far from our estimation of the Larson's nomograph (yes remember that we get earlier above $A\cong 30$ and $n\cong 700$) and that we also get the same operating curve and average outgoing quality plot:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/oc_by_attributes_minitab.jpg}
	\end{figure}
	
	\pagebreak
	\subsection{Quality Control Charts (CC)}
	A "\NewTerm{control chart}\index{control chart}" (sometimes also refereed to as "\NewTerm{sequence chart}\index{sequence chart}" or "\NewTerm{Shewhart charts}\index{Shewhart charts}" or "\NewTerm{process-behavior charts}\index{process-behavior charts}") is a planar empirical representation of the variation of an accurate measurement of a process or a method where the vertical axis represents the quantitative indicator selected and the axis horizontal the time or corresponding number of outgoing  units (so it is a subfamily of "run charts").

	This nondestructive method of quality control that is say to be "\NewTerm{on-line}\index{on-line quality control}" (as the acquisition can be done in real time) permits to highlight special causes of variation and then to optimize the frequency of maintenance operations or recalibration) in the purpose to determine if a manufacturing or business process is in a state of statistical control (or move it in such a state) and to separate signal from noise! 
	
	This technique seems to have been created by physicist engineer Walter A. Shewhart while working for Bell Labs in the 1920s and is new since some decades (1991 to be exact) an ISO norm under the name \textit{ISO 8258:1991 - Shewhart control charts}. Moreover, it is also Shewhart who inspired the well known W. Edwards Deming (statistician) in the field of project management and quality (all the masters in the field of advanced project management have a scientific background...).

	In the early use of control charts, it is advisable to put control charts on all important measurable product characteristics. The resulting chart generally show quickly what charts are necessary or unnecessary (in reality the quality manager has also to conduct a CoQ- Cost of Quality - analysis). Unnecessary or inappropriate control charts will be removed and other control charts might be added.
	
	The most basic and intuitive planar control chart could looks like this:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_naive.jpg}
		\caption{Naive control chart}
	\end{figure}
	The upper and lower limits can be requested / specified by a customer and therefore we note them, as we already know, USL (Upper Specification Limit) and LSL (Lower Specification Limit), otherwise they are calculated internally and therefore we denote them by "\NewTerm{UCL (Upper Control Limit)}\index{Upper Control Limit}" and "\NewTerm{LCL (Lower Control Limit)}\index{Lower Control Limit}". To improve the effectiveness of a control chart, we sometimes add on chart narrower limits named "\NewTerm{monitoring limits}\index{monitoring limits}" (in reality we almost always represent the limits  calculated AND requested by the quality control policy on the control chart). 
	
	The "\NewTerm{Center Line (CL)}\index{Center Line}" is most of time chosen as the arithmetic average of all measurement but it should be better to choose the Median as we already know. The "\NewTerm{Target ($T$)}\index{target}" is the value request as an ideal by the customer specifications.
	
	The equivalent run chart of the above chart made with Minitab 17.3.1 is:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/control_chart_minitab_run_chart.jpg}
		\caption{Run Chart with Minitab 17.1.3}
	\end{figure}
	
	In practice I highly recommend to represent  the center line (CL) that  and also UCL, LCL and as in reality there is normally always limits and goals that are imposed contractually by the customer, to also represent the target $T$ and  USL, LSL on the same control chart when possible!!!
	
	The control chart is therefore a measurement tool for viewing and sometimes anticipating the changes and thus determine when an assignable cause occurs causing a drift of a business process or manufacturing processor a variation of a financial value on stock markets requiring rapid corrective action to reduce the costs of non-quality or losses. Thus, ideally the process will be stopped or corrected at the right time, that is to say before it produces too many non-compliant deliverables / services (outside the tolerance) and therefore to avoid overcontrol (useless re-calibrating because of common variability or application of acceptance plan).

	The type of control chart corresponds to the type of characteristic that is the subject of a control. However the economic aspect may be an important factor in the choice of the control chart to implement (time for measurement, destructive or non-destructive procedure and son...). If we look at whether a characteristic complies or not with certain norms, an "\NewTerm{attribute control chart}\index{attribute control chart}" (see further below for the details) will be used, but requires a big sample size. Control on quantitative and almost continuous characteristics requires the use of "\NewTerm{measurement control charts}\index{measurement control charts}" and if the measurement are not independent of "\NewTerm{time series control charts}\index{time series control charts}". This type of measurement control is more efficient and more precise but also more expensive since it requires the use of measuring instruments that should be checked or re-calibrated regularly. We have also for two dimensional correlated controls the "\NewTerm{multivariate control charts}\index{multivariate control charts}".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The subject of control charts is immensely vast and there is an abundant English literature on the subject. We wish to give here only the fundamental (and therefore only basic control charts!) to show a practical application of the theory or Rank Statistics and of Statistics of extreme values outliers developed that we have developed in detail in the section Statistics. To place an order of magnitude, most specialized software offers between 10 and 30 different control charts (Minitab, R, JMP, QI Macros MATLAB)!
	\end{tcolorbox}
	Before going into the mathematical details notice that the control charts among the most used one are the control charts by measurement  of the average $\bar{X}$ in combination with the control charts by range $R$ with Shewhart's limits (when we speak of "\NewTerm{Shewart's limits}\index{Shewart's limits}" we are implicitly referring to the fact that the upper control limit UCL and the lower limit control LCL are empirically calculated at $\pm 3\sigma$ of the arithmetic average of the distribution that the latter is symmetrical or not!). These both control maps are used and interpreted most of time together because both parameters are independent and complementary and simple to understand (even if the majority of employees in companies does not know what is a standard deviation). Indeed, the arithmetic average value can vary without that the range varies and vice versa\footnote{Remember the trap of the Anscombe's quartet that we saw in the section Statistics}.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} In the case of a control charts with Shewhart's limits, monitoring limits are traditionally placed at $\pm 3\sigma$ (at the opposite of the control limits that are placed at $\pm 3\sigma$).\\
	
	\textbf{R2.} In the case of a Normal distribution $\mathcal{N}(\mu,\sigma)$, $\pm \sigma$ corresponds to a $99.73\%$ confidence interval as we have seen earlier in this section and also in the section Statistics. This therefore corresponds to an $\alpha$ of about:
	
	Most of the industry therefore takes $\pm 3\sigma$ as limits, but strictly speaking distributions of certain control chorts are not symmetrical. We should then take terminals (limits) corresponding to a cumulative probability of $\alpha/2$ which can be calculated quite easily with many distributions (but not all!) just by having available an inexpensive spreadsheet software. The choice of the value $\alpha$ depends on the company's quality policy and has the advantage of being a much more accurate indicator that the Shewhart's limits (the latter being wrong when the distributions are skewed).\\

	\textbf{R3.} Shewhart seems to proposed that there must b at least $25$ samples of $4$ individuals for the validity of certain control cards (that we will see further below) begins to be acceptable.\\
	
	\textbf{R4.} As we will see it, almost all measurement control charts to measures assume Normal distribution and that the observations are independent (between sub-groups and within subgroups!!). For example (example that will be detailed mathematically further below), for already stable processes as since started a long time ago, we perform several individual observations on several subgroups numbered at a given time frequency (hourly, three times per day ...). On each ordered subgroup $k$ (chronological incrementation), we perform $n$ observations. We report on the control chart the arithmetic average $\bar{X}$ of subgroup according to its chronological number $k$ to be reported on the horizontal axis of the control chart. Because of the central limit theorem (CLT) proved in the section Statistics, the average values $\bar{X}_k$ of the control chart follows a Normal distribution $\mathcal{N}(\mu,\sigma/\sqrt{n})$ (so symmetrical distribution!) that the observations of the subgroups are Normally distributed or not but at the condition there are independent and identically distributed over time!!! This assumption is valid even for small sample size and a manufacturing process under control, which is common in quality control. Production is said to be "stable" as we know if the tendency and dispersion are statistically constant over time (and considered identically distributed independent variables).
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{WECO's empirical rules}
	The interpretation of a control chart as mathematically  powerful and elegant it is is not easy and requires experience and expertise to know if corrective action is required or not.  The "\NewTerm{Western Electric Rules}\index{Western Electric Rules}" were codified by a specially-appointed committee of the manufacturing division of the Western Electric Company and appeared in the first edition of its \textit{Statistical Quality Control Handbook} in 1956. Their purpose was to ensure that line workers and engineers interpret control charts in a uniform way (the French health ministry did the same 50 years later ... but with rules that differ a little).

	The WECO based at this time on the assumption that the statistical distributions are always symmetrical (same assumption as Shewhart) and has then adopted in the statement of control  rules limits that are integer multiples $k$ of the standard deviation. Of course, nothing prevents the quality specialist to adapt these rules with probabilistic limits corresponding to a given confidence interval (the majority of specialized software gives the possibility to select the WECO rules to be applied automatically for the detection of non-conformities).

	In the measurement control charts type, among these rules here are those that are applicable in order to identify whether a process has failed and that have been completed over the years by other specialists (we have represent below only some  rules into a single control chart for obvious pedagogical reasons):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_weco_rules_example.jpg}
		\caption[]{Some WECO rules on a Run Chart}
	\end{figure}
	On the next page you can found some of the WECO rules:
	
	\pagebreak
	\begin{enumerate}
		\item[W1.] A measured point is beyond the USL, LSL specified by the customer or beyond the empirical or probabilistic UCL, LCL corresponding to $\pm 3\sigma$.

		\item[W2.] Two consecutive measured points are beyond the empirical or probabilistic UCL, LCL corresponding to $\pm 2\sigma$.

		\item[W3.] Four consecutive measured points are beyond the empirical or probabilistic UCL, LCL corresponding to $\pm 2\sigma$ (in this case named "\NewTerm{alert limits}\index{alert limits}").

		\item[W4.] Eight consecutive points fall on the same side of the mean (even if once again the use of the median is more accurate and robust).

		\item[W5.] Six consecutive points are on an uptrend or downward trend respectively.

		\item[W6.] Fourteen consecutive points on a systematic up/down alternation trend.
	\end{enumerate}
	The rules presented above apply to control charts with symmetric control limits. The WECO handbook provides additional guidelines for control charts where the control limits are not symmetrical, as for $R$ charts and $p$-charts...
	
	As the reader can see below, Minitab includes at least $8$ empirical rules and that can be furthermore customized:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_minitab_weco_measurement_rules.jpg}
		\caption[]{Example of WECO rules implemented in Minitab 17.3.1}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The reader can also refer to "\NewTerm{Westgard Rules}\index{Westgard Rules}" that are a set of rules used for laboratory quality control. They are a copyrighted set of modified Western Electric rules, developed by James Westgard and provided in his books and seminars on quality control.
	\end{tcolorbox}
	If the process/method is out of control, corrective actions must be implemented (process review, training, recalibration, code update, etc.). The causes of variation may be obviously random or deterministic. If the causes are only due to chance, then they are named "\NewTerm{random causes}\index{random causes}" (Shewhart) or "\NewTerm{common causes}\index{common causes}" (Deming) and are therefore treated as instantaneous dispersion. Not all changes are due to chance, of course, some are specific and identifiable in a deterministic a certain way. In the latter case, the variations are named "\NewTerm{assignable variations}\index{assignable variations}" (Shewhart) or "\NewTerm{special causes}\index{special causes}" (Deming) and are assimilated to the overall dispersion (see in the section Statistics the subsection about the propagation of errors). 

	Correct the special causes is obviously most of time much easier than correcting random causes. The primary purpose of control charts is obviously identifying special causes...
	
	OK that's all regarding the customs and traditions of interpretation of measurement points. Now this non-mathematical prelude done, let us see the different types of control charts with a concrete example for each.
	
	\subsubsection{Sample size and Sampling frequency for Control Charts}
	The use of control charts pose early two major difficulties:
	\begin{enumerate}
		\item Choosing the family and type of control chart as we have ($6$ main families, with $20$ control charts and including the typical variants to calculate a same control chart, we have a total almost $60$ control charts...!):
		 \begin{itemize}
			\item Variable charts for subgroups
			\begin{itemize}
				\item Xbar-R/S
				\item I-MR-R/S
				\item R
				\item S
			\end{itemize}
			\item Variable charts for individuals
			\begin{itemize}
				\item Levey-Jennings
				\item I-MR
				\item Z-MR
				\item Individuals
				\item Moving Range
			\end{itemize}
	
			\item Attribute charts
			\begin{itemize}
				\item P
				\item Laney P'/U'
				\item NP
				\item U
				\item C
			\end{itemize}
	
			\item Time-weighted charts
			\begin{itemize}
				\item Moving Average
				\item EWMA
				\item CUSUM
			\end{itemize}
	
			\item Multivariate charts
			\begin{itemize}
				\item T-squared
				\item Multivariate EWMA
			\end{itemize}
	
			\item Rare events charts
			\begin{itemize}
				\item G
				\item T
			\end{itemize}
		\end{itemize}

		\item The choice of the sample size $n$ and of the sampling frequency $f_s$.
	\end{enumerate}
	We would here discuss the second point! We must first observe that it is obviously the pace of execution and the quality of operation and also the time it takes to measure the feature (characteristic) that will give us the answer. This quality is appreciated by the average number of corrective interventions observed during a specific period in the past (MTBF typically). The more corrective action there was, the less the quality of the operation of the method / process is good and higher will be the sampling frequency.
	
	The choice of the number of the sample size is not a problem because as it will result from the statistical calculation of the confidence interval of the statistical indicator that we impose ourselves (see the study of Acceptance Plan earlier above) and therefore of the used distribution law and its properties and also of the assumptions of the chosen the control chart (see below in the text that follows the mathematical description of main control charts).
	
	The challenge is there to determine the sampling frequency. There are for this four main methods and the two first are quite empirical (rule of thumbs):
	\begin{itemize}
		\item Choose a frequency such that the corrective actions $T_c$ are at least four times lower than the sampling frequency (this criterion is independent of the sample size and therefore sometimes not very applicable...):
		
		
		\item We consider $T_p$ as the life of the method / process between each change / modification. We denote as usual $n$ the sample size respecting acceptance plan rules (see previously). We will denote $N$ the number of executions of the method / process (number of produced items) during the period $T$. So we have a maximum $N/n$ samples that are possible.:
		
		
		\item Another method consist to use the MTBF (Mean Time Between Failure) and that as we have proved during our study of reliability techniques is given for recall in the case of the Weibull distribution with three parameters by:
		
		or in the case of an exponential law:
		
		
		\item The last method consist to use a concept we will prove further below and that give a lower bound for the sampling frequency and that is the "average run length" given by:
		
	\end{itemize}
	In all control charts that we will present below, we have presented examples with constant sample size (hence the limits of control charts are always constant with some exceptions that we will also see). Indeed, if the size of the samples change it is that the production rate (or batch size of delivery) has changed or also the overall quality and therefore the manufacturing parameters too. Then we should start a new control chart! It is the same after a re-engineering or changes of the process as shown in the Minitab 15.1.3 screenshot below:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_before_after_minitab.jpg}
		\caption[]{Example of before/after control chart in Minitab 15.1.4}
	\end{figure}
	
	\subsubsection{Attributes Control Charts (qualitative CC)}
	To measure qualitative variables (\% of defective, \% failures, ...), we use attributes $p$ control charts (binomial with proportion), $np$ control charts (binomial with counts), $c$ control charts (Poisson with counts), $U$ control charts (Poisson with percentages) or Laney control charts (two by two range normalized proportions) to monitor the attributes quality control over time.

	The attributes by control charts are very easy to study (undergraduate maths only!) use and interpret this is why we teach them always first, but has several disadvantages such as the high asymmetry of distributions, no lower limit for small sample sizes, low efficiency of deterioration detection.
	
	\pagebreak
	\paragraph{$P$ Control Charts (binomial proportion CC)}\mbox{}\\\\
	The $p$ control chart (binomial by proportion) is used when it comes to working with ratios, proportions or percentages of compliance or non-compliance of samples.

	Good examples of $p$-type control charts are: product inspection of a production line or receiving batches of a supplier or control malfunction of a number of devices, respect of deadlines, product specifications or also coding line errors (bugs).

	\textbf{Pros}: This type of control chart is very simple to construct because does not require any advanced statistical knowledge to be calculated.

	\textbf{Cons}: 
	\begin{itemize}
		\item In reality its a control chart which control limits may be asymmetric if the practical application are at the limit of acceptable when the theoretical assumptions of construction of this chart are not met. 
		
		\item Sometimes a difficulty encountered by practitioners of this chart is to determine the batch size and the level of acceptance ($A$) or rejection ($R$) equivalent to the USL by taking the decision based on the developments (and norms) of acceptance plans samples see earlier above. 
		
		 \item This control chart is not suitable for exhaustive sampling since it would then required the use of the hypergeometric law. Unfortunately, as far as we know, there is no analytic relation for the confidence interval of the proportion of a hypergeometric distribution, and therefore is not feasible to build a control chart  based on a hypergeometric law.
		
		\item A final difficulty is that some employees and board committee are not proficient with the concept of percentages.
	\end{itemize}

	The probability law used in this context is the binomial distribution (\SeeChapter{see section Statistics}) where $p$ will represent the proportion in $\%$ of non-compliants and $q$ (which therefore is $1-p$) that proportion in $\%$ of compliants.

	We saw in the section Statistics that we were able to write formally, under certain assumptions, the following confidence interval for the real proportion:
	
	So we can calculate the probabilities of non-compliant batches by excess or by defect using the fact that $p$ follows under statistical control a Normal distribution of parameters:
	
	In short, we have no choice but to base this control chart on this assumption. Therefore, the first step in creating a $p$-type control chart is to calculate the proportion of non-compliance, at least its estimator for each batch $i$:
	
	where $k_i$ is the number of non-compliant items in the batch of size $N_i$.

	The average proportion of non-conformities, which corresponds to the center line (CL) of the control chart is given by (don't forget that using the median would be better in reality!):
	
	and if the $n$ batches of size $N_i$ are all of equal size $N$ this simplifies obviously to:
	
	Therefore we have:
	
	and if the batches size are equal  we have obviously:
	
	According to the study that we made during our study of acceptance plan they can not be any lower limit (LSL) imposed by the customer/board committee or target ($T$) in this field of application.

	So, with the following list where the batches size (special case) are identical (made with Microsoft Excel 14.0.7166):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_p_excel_list_data.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/engineering/control_chart_p_excel_list_data_with_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/control_chart_p_plot_excel.jpg}
	\end{figure}
	with Minitab 17.1.3 this gives for people that don't trust our calculation made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.95]{img/engineering/control_chart_p_plot_minitab.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	
	obviously not visible on the chart as too far away on the $y$-axes.

	This control chart can also be interpreted as any another Shewhart chart using the empirical WECO rules.
	
	\paragraph{$NP$ Control Charts (binomial counting CC)}\mbox{}\\\\
	The $np$ control chart is obviously similar to the previous $p$ control but is the number (count) of non-compliant items units than the proportion that is plotted. This chart is easier to interpret as the $p$ control chart since not all employees are able to represent themselves the concept of percentage, but against it is very difficult to compare objectively two points coming from batches that do not have the same size.
	
	\textbf{Pros}: This type of control chart is very simple to construct because does not require any advanced statistical knowledge to be calculated. For some employees whose concept of percentage is poorly mastered, it is easier to read a $np$ chart.

	\textbf{Cons}: 
	\begin{itemize}
		\item In reality its a control chart which control limits may be asymmetric if the practical application are at the limit of acceptable when the theoretical assumptions of construction of this chart are not met. 
		
		\item Sometimes a difficulty encountered by practitioners of this chart is to determine the batch size and the level of acceptance ($A$) or rejection ($R$) equivalent to the USL by taking the decision based on the developments (and norms) of acceptance plans samples see earlier above. 
		
		 \item This control chart is not suitable for exhaustive sampling since it would then required the use of the hypergeometric law. Unfortunately, as far as we know, there is no analytic relation for the confidence interval of the proportion of a hypergeometric distribution, and therefore is not feasible to build a control chart  based on a hypergeometric law.
		
		\item It is very difficult to compare objectively two points coming from batches that do not have the same size
	\end{itemize}
	
	For the values of the control chart, we have still using:
	
	And therefore using the results of the previous control chart is comes immediately in the general case:
	
	But since it is very difficult (unlike the $p$ control chart) to compare objectively with this control two points when the size of the sample is not the same, some specialists required - or recommended - that the batch size is the same such that finally:
	 
	According to the study of acceptance plans we know that they can not be any lower limit (LSL) required by the customer / board committee or target ($T$) in this field of application.

	So, with the following list where the batches size (special case) are identical (made with Microsoft Excel 14.0.7166):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_np_excel_list_data.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/control_chart_np_excel_list_data_with_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_np_plot_excel.jpg}
	\end{figure}
	with Minitab 17.1.3 this gives for people that don't trust our calculation made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.95]{img/engineering/control_chart_np_plot_minitab.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	

	This control chart can also be interpreted as any another Shewhart chart using the empirical WECO rules.
	
	\paragraph{$C$ Control Charts (Poisson counting CC)}\mbox{}\\\\
	The $C$ control charts are used to look at variation in counting type attributes data. They are used to determine the variation in the number of defects in a constant subgroup size. To use the $C$ chart, the opportunities for defects to occur in the subgroup must be very large, but the number that actually occurs must be small. 
	
	Indeed, during our study of statistical laws (or statistical distributions), we have proved that when the probability $p$ is very small and tends to zero, but that instead the average value $\mu=np$ tends to a fixed value when $n$ approaches infinity, the binomial distribution of mean $\mu=np$ with $k$ trials was therefore given by a Poisson law:
	
	with:
	
	In practice, some replace the binomial distribution by a Poisson law since $n>30$ and $np<5$ or when $n>50$ and $p<0.1$... In practice I personally recommend the use of the approximation when $n>1000$ and $p<0.01$.
	
	As in practice such situations does not happen very often. Some books and authors say that this control chart is reserved to the count of defects per unit opportunity (see our study of Six Sigma at the beginning of this chapter). Indeed, the fact that each unit can be composed of multiple items this increase "artificially" the $n$ (as $1$ unit opportunity can have dozens, hundreds of millions of components) and decrease most of time the probability $p$ such that the assumption of using the Poisson Law are respected.
	
	Following the previous rules it comes obviously:
	 
	what is traditionally noted in this field:
	 
	where $\bar{c}$ is simply the average of the count (that's form this word that comes from the "c" in the name of the chart) of nonconformities:
	
	According to the study that we made during our study of acceptance plan they can not be any lower limit (LSL) imposed by the customer/board committee or target ($T$) in this field of application.

	\textbf{Pros}: This type of control chart is very simple to construct because does not require any advanced statistical knowledge to be calculated.

	\textbf{Cons}:
	\begin{itemize}
		\item In reality its a control chart which control limits may be asymmetric if the practical application are at the limit of acceptable when the theoretical assumptions of construction of this chart are not met. 
		
		\item Sometimes a difficulty encountered by practitioners of this chart is to determine the batch size and the level of acceptance ($A$) or rejection ($R$) equivalent to the USL by taking the decision based on the developments (and norms) of acceptance plans samples see earlier above. 
		
		 \item This control chart is not suitable for exhaustive sampling since it would then required the use of the binomial or hypergeometric law. 		\end{itemize}
		 
		Once again using our spreadsheet software here is an example where the batch sizes are necessarily identical for the reasons mentioned just above (and once again... non-compliant items can not be expressed in $\%$ of the batch size as the Poisson distribution is a discrete law!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_c_excel_list_data.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_c_excel_list_data_with_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_c_plot_excel.jpg}
	\end{figure}
	with Minitab 17.1.3 this gives for people that don't trust our calculation made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.95]{img/engineering/control_chart_c_plot_minitab.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	

	This control chart can also be interpreted as any another Shewhart chart using the empirical WECO rules.
	
	\paragraph{$U$ Control Charts (normalized Poisson)}\mbox{}\\\\
	The $u$ control chart is similar to the $c$ control chart except that it has a for role to normalize the data to the unit to have a $c$ control chart based on a Poisson distribution but with percentages.

	According to the study that we made during our study of acceptance plan they can not be any lower limit (LSL) imposed by the customer/board committee or target ($T$) in this field of application.

	\textbf{Pros}: This type of control chart is very simple to construct because does not require any advanced statistical knowledge to be calculated and it is now more easy to compare objectively two points coming from batches that do not have the same size

	\textbf{Cons}:
	\begin{itemize}
		\item In reality its a control chart which control limits may be asymmetric if the practical application are at the limit of acceptable when the theoretical assumptions of construction of this chart are not met. 
		
		\item Sometimes a difficulty encountered by practitioners of this chart is to determine the batch size and the level of acceptance ($A$) or rejection ($R$) equivalent to the USL by taking the decision based on the developments (and norms) of acceptance plans samples see earlier above. 
		
		 \item This control chart is not suitable for exhaustive sampling since it would then required the use of the binomial or hypergeometric law.
		
		\item A final difficulty is that some employees and board committee are not proficient with the concept of percentages.
 	\end{itemize}
 	For this, we use the same working assumptions as those demonstrated in the during our study of the Poisson distribution:
	
	This makes that our previous $c$ control chart is then written:
	
	If we divide the random variable $k=n_i\bar{\hat{p}}$ by the batch size (or number of components) we have then by the property of the mean and variance:
	
	what is traditionally written:
	
	with:
	
	
	Once again using our spreadsheet software here is an example where the batch sizes are non-necessarily identical this time using the "trick" above (and once again... non-compliant items can now be expressed in $\%$ of the batch size!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_u_excel_list_data.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/engineering/control_chart_u_excel_list_data_with_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_u_plot_excel.jpg}
	\end{figure}
	with Minitab 17.1.3 this gives for people that don't trust our calculation made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.95]{img/engineering/control_chart_u_plot_minitab.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	

	This control chart can also be interpreted as any another Shewhart chart using the empirical WECO rules.
	
	\paragraph{Laney's $p'$ and $u'$ control charts}\mbox{}\\\\
	The $p$ and $u$ control charts as we hav seen it have a big problem that resides in the fact that the limits use a standard deviation that is proportional to:
	
	which therefore tends to $0$ when the $N_i$ are very by (which typically happens in international call centers where its value is often greater than $5,000$ per day) and then the standard deviation becomes so small that we get almost systematically false alarms (therefore the control chart is a little bit much ... too sensitive). Moreover, their standard deviation assumes that the proportion (probability) is constant over time and this is also unrealistic.

	It should be noticed that the problem of too much sensitivity almost never appears in the industry since it is often less than a hundred of items!

	An alternative proposed by David Laney in 2002 is then to consider in a a first time the proportions as continuous quantitative values and then of using a control chart taking first the range by paired values of the consecutive proportions:
	
	And as (\SeeChapter{see section Statistics}):
	
	It comes:
	
 	because comparing only the proportion two-by-two, the Hartley constant $d_2(n)$ must be taken with $n=2$.

	We then have the possibility to take:
	
	Practice shows that this control chart is already much less sensitive, which is a progress but... however it suffers from a big issue!: the upper and lower limits are constant whereas we would like to take into account that the size of the sample varies (or may vary) for each $i$. Then we have to find another solution!
	
	A track (a little drawn by the hair) then consists first of all in considering that if the $N_i$ are very large and the proportion (probability) is neither too close to $0$ nor too close to $1$, then each can be approximated By a Normal law and to be centered reduced:
	
	and therefore:
	
 	Then if follows that:
 	
 	and therefore the true variance of $\hat{p}_i$ can be thank to:
	
	Obviously, in the reality the standard deviation of $Z$ will not really be equal to $0$. So the idea (questionable ...) is to consider that as now we have a continuous variable we will be able to estimate the true variance of $Z$ thanks to:
	
	and
	
	Therefore:
	
	We then have:
	
	where if $\text{LCL}_{Z(p)}$ is smaller than zero then we will assume that this limit is zero (as for the previous control charts).
	
	We thus have a control map which is an attribute-range mixture which according to the return on experience (REX) is less sensitive (less systematic false alerts) and which has variable limits depending on the size of the subgroups.

	The same reasoning is applicable with the $U$ control chart (as the Poisson's law can also be approximated by a Normal law, as we have demonstrated in the section of Statistics):
	
	We notice that if by hazard we have we have for the both set of relations above:
	
 	Then we fall back on the standard parameters of a control chart of type $p$ or respectively of type $u$.	

	There are however corrections/variants to these two models ... so it is necessary to know well what your software does! It is for this reason that we will not take screenshots of the procedure with Microsoft Excel this time and compare it with Minitab (excepted if requested by our readers).
	
	\subsubsection{Measurement Control Charts (quantitative CC)}
	Quantitative variables are continuous measurements of the type: weights, lengths, thicknesses, temperature, diameter ...

	Unlike autocorrelated control charts and quality (attributes) control charts , quantitative control charts (with measurement) are often - if not always - presented in pairs (or should be!):
	\begin{itemize}
		\item A control chart showing the analysis / evolution of the central statistical trend (often the mean, because it is relatively easy to construct confidence intervals on it).

		\item A control chart showing the analysis / evolution of the dispersion via the range or the standard deviation.
	\end{itemize}

	The controls limits are often taken (for lack of thinking time in business) as the Shewhart or WECO limits and not probabilistic limits.
	
	We also make the distinction between:
	\begin{itemize}
		\item "\NewTerm{Individual control charts}\index{Individual control charts}" (each point on the control chart invole one and only one measurement)

		\item "\NewTerm{Subgroups control charts}\index{Subgroups control charts}" (each point on the control chart invole implicitly more than one item)
	\end{itemize}
	
	\paragraph{Individual measurement control chart with required limits}\mbox{}\\\\
	This is the easiest control chart and most used one by non specialist because it does not require any special knowledge and statistical hypothesis. It consist only to report the measurements and to take corrective actions when it seems necessary by the rule of thumb.
	
	\textbf{Pros}: Simple to build because does not require any special knowledge or statistical hypothesis and as all previous control charts, permits the user to check in a timely manner whether a production meets or not the constraints specified by customers.

	\textbf{Cons}:
	\begin{itemize}
		\item It may represent temporary random deviations that may make the reader think mistakenly that there is problem in the method / process and will lead to unnecessary corrective actions (false alarms). 
		
		\item  It does not allow to identify if the process / method is under statistical control or not that is to say that as for attribute control charts, it is not possible to make a Capability SixPack analysis.
 	\end{itemize}
 	For example, let us consider the following list of data still in the same spreadsheet software:
 	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_individual_measurement_excel_list_data.jpg}
	\end{figure}
	where we took empirically (rule of thumb) the limits:
	
	Which gives the following control chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_individual_measurement_plot_excel.jpg}
	\end{figure}
	While this card is the most used in practice (since most people don't have training in SPC), the problem remains that it does not really tell us whether the method or process appears to be stable in time and if the statistical control are in the customer limitations (it shows us only the punctual statistics which is as we know not very robust...!). We will partially remedy with this with the next control chart.
	
	This type of control chart is available in most statistical software but as there is nothing complicated we will not provide here any Minitab (or other...) screenshot.
	
	\paragraph{Individual measurement control chart with moving limits}\mbox{}\\\\
	When we begin the first observations (measurements), the method or process is rarely stable and there is no way to have fun do manufacture a huge quantity of trial pieces to do a control chart of the average in such conditions and especially if a piece is quite expensive and time consuming!
	
	\textbf{Pros}: Highlights the evolution over time of the statistical indicators of positions (average or median) and dispersion and permits to observe the process/methods stabilization   during the prototyping phase (often called "\NewTerm{Phase I}\index{Phase I}" in the literature).

	\textbf{Cons}:
	\begin{itemize}
		\item It may represent temporary random deviations that may make the reader think mistakenly that there is problem in the method / process and will lead to unnecessary corrective actions (false alarms). 
		
		\item  It does not allow to identify statistically if the process / method is under statistical control or not that is to say that as for attribute control charts, it is not possible to make a Capability SixPack analysis.
 	\end{itemize}
 	We stay with a measured control chat to which we add the arithmetic average and the unbiased standard deviation recalculated at each new item. That is, as we have not enough samples to know the probability distribution and is out of the question make waste pieces in this purpose and to train people for weeks to statistics, we add to the previous Shewhart control chart control the following limits:
	
	where CL means "\NewTerm{centerline}\index{centerline}".
	
	Obviously for the estimator of the mean and the standard deviation, we take:
	
	Once again using our spreadsheet software here is an example where $N=150$ and therefore where $k$ moves from $1$ to $150$ with the $63$ first rows visible:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_running_statistics_measurement_excel_list_data.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/control_chart_running_statistics_measurement_excel_list_data_with_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/control_chart_running_statistics_plot_excel.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	
	We see for example the in the above control chart that the CL indicator (so the estimator of the mean) stabilizes close to the target $T$ from the beginning to the end. It's the same for UCL and LCL (because the estimator of the standard deviation stabilizes) by cons following the usage in engineering the results are not good, since the UCL and LCL are beyond the required specifications by the customer! Note that this control chart would according to the WECO rules have only one single point beyond the $\pm 3\sigma$ (we indicate that for comparison reasons  with the next control charts).
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	On all control charts with no exceptions, the customer specifications (limits) must be indicated in addition to the calculated limits!
	\end{tcolorbox}
	
	This type of control chart is as far as we know not available in  statistical softwares. But there is an easy one with the following very common and obvious fixed limits:
	
	named a "\NewTerm{Levey-Jennings control chart}\index{control chart!Levey-Jennings control chart}".
	
	With Minitab 15.1.1.0 this gives for people that don't trust simple Microsoft Excel calculations (we can also do such a chart with R but the plot quality is not acceptable in a corporation):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.95]{img/engineering/control_chart_levey_jennings_minitab.jpg}
	\end{figure}
	At this level, we supposed that we are unable to determine the probability distribution of the data (sample problem or statistical skills problems) and therefore unable to calculate the confidence intervals for the mean or the standard deviation!

	The next control charts is supposed to remedy to this situation under some assumption!
	
	\paragraph{Subgroups measurement control chart with standard error}\mbox{}\\\\
	Once the process or method stabilized, we consider that the mean and standard deviation are stabilized and even if the probability distribution that follows a group of $k$ samples is not the same from one day to another, by the central limit theorem, the set of averages follows a Normal distribution as we know of the type:
	
	and written in SPC as (for the reason we will see further below):
	
	Therefore it may be more interesting to represent the change in the average over time and not just the single measurement. This also gives us the opportunity to do a capability analysis of our process or method.
	
	\textbf{Pros}:
	\begin{itemize}
		\item Hides (smooth) non-systematic and random variations and often allows to avoid unnecessary corrective actions (false alarms).
		
		\item Gives the possibility to associate the control chart with a capability analysis.
	\end{itemize}

	\textbf{Cons}:
	\begin{itemize}
		\item This control chart assumes that the samples are identically distributed by the mathematical assumptions it is build on. Moreover, in the framework of fixed Shewhart limits, the distribution law is assumed symmetrical. 
		
		\item  The major issue of this control chart is that the number of individuals per sample must be large enough for the unbiased estimator of the standard deviation is not too far from reality. This is why we do not found this control chart in statistical software (this is why this control chart is replaced with another chart that we will see a little further below).
 	\end{itemize}
 	As companion example let us consider the following example of $25$ samples (that we will identify by the variable $k$) of $6$ individuals each (that we will identify by the variable $n_i=n$) taken from a continuous manufacturing process (without machine recalibration or without machine change!):
 	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_subgroup_measurement_standard_error_excel_list_data.jpg}
	\end{figure}
	where every day we took $6$ individuals of all manufactured items produced the same day. On the control chart, then we will represent each point as the average of these $6$ individuals.

	We have proved in the section Statistics that the standard deviation of the mean (standard error) of a series of $n$ independent and identically distributed random variables were if the samples are big enough:
	
	and using the finite population correction factor if the sampled population is not infinite (remember that $N$ is the population size):
	 
	We then have the following Shewhart control limit indicators (obviously as we use the average estimators and standard deviation, the greater the number of samples and individuals, the greater the limits are asymptotically accurate but also... known problem... if the samples $n_i$ are too big we will have systematic alarms as we know!):
	
	With:
	
	and if all the $n_i$ are equals:
	
	Let us notice that it is the first control chart where we can calculate the probabilistic limits of non-compliance by excess or defect of subgroups by using the fact $\overline{X}$ that follows a Normal distribution of parameters $\mathcal{N}\left(\overline{X},\sigma_X/\sqrt{n}\right)$.
	
	Once again using our spreadsheet software here is the calculations we get in the simple case where all sample still have the same size (calculations based on the previous list of data given just earlier above):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/control_chart_subgroup_measurement_standard_error_excel_calculations.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/engineering/control_chart_subgroup_measurement_standard_error_excel_explicit_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_standard_error_plot_excel.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	
	Let us notice that this control chart would according to the WECO rules have $6$ points beyond the $\pm 3\sigma$ (we indicate that for reasons of comparison with the previous control chart and the next one). It is therefore more sensitive to changes card than the control chart. Also notice that the UCL and LCL limits are wider than the imposed USL and LSL.
	
	With the $\pm 3\sigma_{\text{X}}$ limits and according to the hypothesis of Normally distributed date, we have using the symmetry of the Normal distribution and writing as usual $\Phi$ the cumulative probability function of the Normal centered reduced distribution (that is to where we suppose $\text{LCL}_\text{SE}=\text{UCL}_\text{SE}=\sigma_{\overline{X}}=1$):
	
	So we will have statistically $1$ point (piece/action) on the control chart on $370$ outside the $\pm \sigma_{\overline{X}}$ by unit time/sample measured (so if each point on the map is made by frequency of $1$ hour then we have an ARL of $0.0027$ [h$^{-1}$] ) only for statistical purposes and that must be therefore considered as false alarms (Type I Error\footnote{Obviously there are also type II errors (no alarm when there should have). But as I've never seen anyone use it in practice for control charts.}!!!) that the data are centered and reduced or not!!!!
	
	We name the latter number "\NewTerm{Average Run Length (ARL)}\index{Average Run Length}" (some use the French appellation "Average Operational Period (AOP)"). When it reported to a specific unit of time, then we speak of of "\NewTerm{Average Time to Signal (ATS)}" and write it:
	
	or more commonly:
	
	Obviously, by extension, the addition of the WECO empirical rules add false alarms and it has for immediate effect of reducing the ARL.
	
	As we have already mention it at our beginning of the study of control charts, the ARL is important in practice, because it gives the interval sampling size beyond which we must not go! We will ensure in practice (when possible) to take a lower value close to the ARL for large productions but possibly not beyond!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As we have already mentioned, sometimes several levels of controls are added to the control charts, so we can observe the $\pm 3\sigma_{\overline{X}}$ and simultaneously $\pm 2\sigma_{\overline{X}}$ . In the U.S.A factor $2$ or $3$ seems to be mainly used that the distribution is symmetrical or not ... as in European countries like France and Germany (industrial countries), we use probabilistic limits that have a much better reliability when data are not distributed following a symmetric (as is the case for well known cards using the range $R$ that we will see later).
	\end{tcolorbox}
	Recall that the problem of this card is that equation is asymptotically correct if equation is great. While in reality it is by far not the case since often this value is less than 10. Therefore we will see below a map of the average, the fixed limits are calculated based on the extent and not R no equation. However, in the chapter of Statistics, we have shown that when the standard deviation is estimated, then we have:
	
	So we can use the Shewhart control chart with the following probabilistic limits that is more accurate than the previous one:
	
	Also as part of a process or method under statistical control, it can be interesting to communicate the average control chart (with fixed or probabilistic limits) previously presented with a control chart showing also the evolution of the standard deviation considering a Normal distribution of the random variables. Indeed, remember that the Normal distribution is completely defined by its parameters $\mu,\sigma$. Hence the reason to have a look on both of them and this is why many SPC softwares plots automatically two control charts at the same time!
	
	
	\paragraph{$\overline{S}-S$ Subgroups measurement control chart for standard deviation}\mbox{}\\\\
	Also as part of a process or method under statistical control, it can be interesting to communicate the average control chart (with fixed or probabilistic limits) previously presented with a control chart showing also the evolution of the standard deviation considering a Normal distribution of the random variables. Indeed, remember that the Normal distribution is completely defined by its parameters $\mu,\sigma$. Hence the reason to have a look on both of them and this is why many SPC softwares plots automatically two control charts at the same time!
	
	\textbf{Pros}:
	\begin{itemize}
		\item Complete well the previous control chart and permits to visualizes if the volatility (standard deviation) of the process stabilizes. 

		\item Represents well the variation within the data of each sampling (instantenous dispersion).
	\end{itemize}

	\textbf{Cons}:
	\begin{itemize}
		\item This control chart must be used only when the prototyping phase is completed (often named "Phase II" in the literature) because it is based on an inescapable strong assumption that the data are Normally distributed. 

		\item The limits of Shewhart controls chart are not adapted in this case because, as we will prove in detail the variance distribution law is not a symmetrical law while Shewhart limits suppose this is the case.
 	\end{itemize}
 	The idea is to represent:
	
	always with the estimator of mean ($k$ being the number of samples for recall...):
	
	and always with the unbiased maximum likelihood estimator of the standard deviation denoted $S_i$ in the field of quality (at the opposite $\hat{\sigma}$ in the field of pure statistics):
	
	where $n$ corresponds obviously to the number of individuals for the sample $i$.

	To calculate the mean and the variance of $\sigma_S$, we must consider $S$ as also as a Normally distributed random variable (it is the strong and essential assumption in this control chart that we had mentioned in the "cons" just before!). We then have proved in sections Statistics that we had:
	
	which we will denote $t$ thereafter as the corresponding random variable. 

	For the remaining development, we will consider $(n-1)$ and $\sigma$ as simple constant coefficients of $S$. So the random variable $t$ is actually implicit function of $S$ as:
	
	We will focus now to the calculation of mean of $S$. It is then more convenient to take the square root of $t$ such that:
	
	Therefore it comes:
	
	Let us recall that we proved in the section Statistics that the chi-square law was defined by the following probability density function:
	
	Therefore it comes:
	
	Let us put for what follows:
	
	Then we have:
	
	Therefore:
	
	Finally on finish on this point, be aware that it is traditional to denote the "\NewTerm{standard deviation unbiasing constants $c_4$}\index{standard deviation unbiasing constants}"  as follows:
	
	and for which we find the values in the tables of various book or softwares. But still almost everybody has a spreadsheet software, these tables are useless as we can obtain their values for examples with spreadsheet software like Microsoft Excel 14.0.6123 by writing the following formula:
	\begin{center}
		\texttt{=SQRT(2/($n_i$-1))*EXP(GAMMALN($n_i$/2))/EXP(GAMMALN(($n_i$-1)/2))}
	\end{center}
	or in OpenOffice Calc (the formula is there little bit simpler):
	\begin{center}
		\texttt{=SQRT(2/($n_i$-1))*GAMMA($n_i$/2)/GAMMA(($n_i$-1)/2)}
	\end{center}
	Let us now calculate the variance of $S$ using the Huygens relation proved in the section Statistics that is for recall:
	
	Therefore we can write:
	
	
	Therefore:
	
	hence:
	
	Therefore we have can write now:
	
	that is to say finally as sometimes written in the litterature:
	
	where by definition:
	
	That can be also sometimes be found in some books written as:
		
	with by definition the following unbiasing constants:
	
	The unbiasing constants $c_4,B_3,B_4$ (all dependent of $n_i$) can obviously be found in many tables in various books or softwares, but once again the can be easily computed using a simple spreadsheet software like Microsoft Excel as proved earlier above.

	As companion example let us once again consider the following example of $25$ samples (that we will identify by the variable $k$) of $6$ individuals each (that we will identify by the variable $n_i=n$) taken from a continuous manufacturing process (without machine recalibration or without machine change!):
 	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_sbar_s_measurement_excel_list_data.jpg}
	\end{figure}
	where every day we took $6$ individuals of all manufactured items produced the same day.
	
	Once again using our spreadsheet software here is the calculations we get in the simple case where all sample still have the same size (calculations based on the previous list of data given just earlier above):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/control_chart_subgroup_measurement_sbar_s_excel_calculations.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/control_chart_subgroup_measurement_sbar_s_excel_explicit_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_sbar_s_plot_excel.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_sbar_s_plot_minitab.jpg}
	\end{figure}
	Let us notice that this control chart would according to the WECO rules have $0$ points beyond the $\pm 3\sigma$ (we indicate that for reasons of comparison with the previous control chart and the next one). It is therefore more sensitive to changes card than the control chart. Also notice that the UCL and LCL limits are wider than the imposed USL and LSL.
	
	For recall, the major problem with this control chart is that it is correct if and only if the data are Normally distributed and that Shewart limits suppose that $S$ is symetric and here in fact it is note case (if we took probabilistic limits). Therefore, the quality manager then combine with the average control chart another optional control which we will introduce now.
	
	\pagebreak
	\paragraph{$\overline{X}-S$ Subgroups measurement control chart}\mbox{}\\\\
	During our study of the control chart with averages with fixed limits based on the standard deviation of the mean (standard error), we have clearly shown that it suffered from the handicap that $\sigma_X$ is too approximative in practice (because $n_i$ is often too small).

	Therefore, a workaround to this is to use the results obtained from the study of the previous control chart.
	
	\textbf{Pros}:
	\begin{itemize}
		\item Corrects the problem of the control chart for averages with standard error when the number of individuals per sample is too small. 

		\item Represents well the change of level in the average of data (central tendency).
	\end{itemize}

	\textbf{Cons}:
	\begin{itemize}
		\item The mathematical proof of the origin of the control chart limits is quite difficult for undergraduates (see the section Statistics for the detailed proof).  

		\item This control chart only works if and only if the data are identically distributed and are Normally distributed! 
		
		\item Moreover, since the limits are based on the estimator of the standard deviation it would require however $n_i$ that in the idea should not be below the value of $10$ ...
 	\end{itemize}
 	First let us recall that for the average control chart with standard error limits we get:
	
	and we have prove earlier above:
	
	We have therefore:
	
	and that we sometimes find in the literature written as:
	
	with the unbiasing constant:
	
	Notice then that we can calculate the probabilities of non-compliance by excess or defect of subgroups using the fact that $\overline{X}$ follows a Normal distribution of parameters:
	
	As companion example let us once again consider the following example of $25$ samples (that we will identify by the variable $k$) of $6$ individuals each (that we will identify by the variable $n_i=n$) taken from a continuous manufacturing process (without machine recalibration or without machine change!):
 	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_xbar_s_measurement_excel_list_data.jpg}
	\end{figure}
	where every day we took $6$ individuals of all manufactured items produced the same day.
	
	We can calculate, or find in the tables that:
	
	We then have the following table:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/control_chart_subgroup_measurement_xbar_s_excel_calculations.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_subgroup_measurement_xbar_s_excel_explicit_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_xbar_s_plot_excel.jpg}
	\end{figure}
	where we took as example for limits imposed by the customer/board committee:
	
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_xbar_s_plot_minitab.jpg}
	\end{figure}
	Let us notice that this control chart would according to the WECO rules have $1$ point beyond the $\pm 3\sigma$ (we indicate that for reasons of comparison with the previous control chart and the next one). It is therefore more sensitive to changes card than the control chart. Also notice that the UCL and LCL limits are wider than the imposed USL and LSL.
	
	For recall, the major problem with this control chart is that it is correct if and only if the data are Normally distributed and that Shewart limits suppose that $S$ is symmetric and here in fact it is note case (if we took probabilistic limits). Therefore, the quality manager then combine with the average control chart another optional control chart which we will introduce now.
	
	\paragraph{$\overline{X}-S_P$ Subgroups measurement control chart with pooled variance}\mbox{}\\\\
	As we have seen it the standard error based control chart suppose all measurements as whole (that means we suppose that each subgroup has the same mean). This is in fact not really accurate as every group is supposed as being independent and has to be compared with the others. So  identically at the $t$-test or ANOVA for independent groups we use the "pooled variance\index{pooled variance}" as overall indicator and as a generalization of the standard error (the pooled estimate the variance of several different populations when the mean may be different\footnote{Therefore it increase the power of a corresponding test}, but one may assume that the variance of each population is for recall the same!!!).
	
	Therefore the pooled variance control chart is based on the following relation proved in the section Statistics during our study of ANOVA:
	
	or in a more common obvious form for control charts:
	
	But as above... we can use if we want the unbiasing constant $c_4$ of the variance as it applies similarly at the difference that:
	
	where the $1+$ is here because in the case where $n_i=2$ (minimum case for the calculation of a variance) we would have without it $c_4(1)$ and that latter is not defined, that's why!
	
	But in fact it's quite useless in the situation of the pooled variance. Normally unbiasing constant $c_4$ is for small samples... but when the parameter of the constant is $\sum_{i}(n_i-1)$ then:
	
	and it follows that:
	
	To convinced yourself you can see the calculated some $c_4$ values using the relation constant given earlier above. As requested by some of my students here are some tabulated values:
		
	But as most of time the correction is smaller than the measurement precision... I let you therefore guess if it is very useful...
	
	But we still can write (we like to write it by highlighting the fact that we are using the pooled variance so we avoid to use too much condensed notations!):
	
	where we often write in practice:
	
	As companion example let us consider the following example of $25$ samples of $6$ individuals each (that we will identify by the variable $n_i=n$) taken from a continuous manufacturing process (without machine recalibration or without machine change!):
 	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_subgroup_measurement_standard_error_excel_list_data.jpg}
	\end{figure}
	and let us focus on an example with the use of the unbiasing constant and always providing a detailed Microsoft Excel example and showing afterwards the corresponding Minitab plot!
	
	So every day we took $n_i=6$ individuals of all manufactured items produced the same day.
	
	We can calculate, or find in the tables that:
	
	We then have the following table:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/control_chart_subgroup_pooled_variance_measurement_xbar_sp_excel_calculations.jpg}
	\end{figure}
	or explicitly (the reader can zoom on it with its PDF reader):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.4]{img/engineering/control_chart_subgroup_pooled_variance_measurement_xbar_sp_excel_explicit_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_xbar_sp_plot_excel.jpg}
	\end{figure}
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.93]{img/engineering/control_chart_xbar_sp_plot_minitab.jpg}
	\end{figure}
	Let us notice that this control chart would according to the WECO rules have $0$ point beyond the $\pm 3\sigma$ (we indicate that for reasons of comparison with the previous control chart and the next one). 
	
	For recall, the major problem with this control chart is that it is correct if and only if the data are Normally distributed!
	
%	\pagebreak
	\paragraph{$\overline{R}-R$ Subgroups measurement control chart}\mbox{}\\\
	The range control chart is often associated with the average control chart. It aims to advantageously replace the standard deviation control chart $\overline {S}-S$ when data are not Normally distributed, because it's mathematical construction allows if desired to choose the underlying probability distribution! So we use this new control chart when $n_i\leq 10$ (the number of individuals per sample), while for the control chart for the standard deviation we must instead $n_i\geq 10$.
	
	\textbf{Pros}:
	\begin{itemize}
		\item Completes well the average control chart and replaces well the control chart of the standard deviation when the number of samples is small. 

		\item Represents well the variation within each sample of data (instantaneous dispersion) that we could detect.
	\end{itemize}

	\textbf{Cons}:
	\begin{itemize}
		\item The mathematical proof of the origin of the limits of controls is quite hard (see the section of Statistics for the detailed proof) and the determination of the corresponding unbiasing coefficients requires the use of Monte Carlo simulations (\SeeChapter{see section Theoretical Computing}) . 

		\item Another problem is that even if the probability distribution of sample size can be chosen, it is often given in specialized books only for the Normal distribution... (but softwares such as Minitab manage them).
 	\end{itemize}
 	The idea is to represent (due to the possible assymetry of $R$ it is impossible - nonsense - that the customer sets coherent USL and LSL or a target $T$ as regards to the range, hence the fact that these parameters are this will time not be listed or represented):
 	
	and this time we will not use estimators of the standard deviation for the range as it do not converge fast enough. Therefore, we will use the results proved in the section Statistics during our study of rank statistics. We obtained for the estimator of the standard deviation with the unbiasing Hartley's constants:
	
	with:
	
	or depending of the softwares (the latter one is more logic if we think a little bit):
	
	Therefore:
	
	which is written traditionally:
	
	The unbiasing coefficients $D_3,D_4$ defined by:
	
	are available in the literature (or on Internet or also in the Help of Minitab) as a function of the $n_i$ of the sample size and often (unfortunately) only for a Normal distribution (see the example in the section Statistics).
	
	As companion example let us consider the following example of $25$ samples (that we will identify by the variable $k$) of $6$ individuals each (that we will identify by the variable $n_i=n$) taken from a continuous manufacturing process (without machine recalibration or without machine change!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_rbar_r_measurement_excel_list_data.jpg}
	\end{figure}
	where every day we took $6$ individuals of all manufactured items produced the same day.
	
	We can find in the tables that:
	
	We then have the following table:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_subgroup_measurement_rbar_r_excel_calculations.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/control_chart_subgroup_measurement_rbar_r_excel_explicit_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_rbar_r_plot_excel.jpg}
	\end{figure}
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_xbar_s_plot_minitab.jpg}
	\end{figure}
	which (is it necessary to recall it?) underestimates the number of error detections since the probability distribution of the range is in reality not symmetrical (hence the application of fixed Shewhart's control limits is not really suited ...) and because almost all of the tables assume the data still Normally distributed as well.
	
	Let us notice that this control chart would according to the WECO rules have $0$ point beyond the $\pm 3\sigma$.
	
%	\paragraph{$\overline{X}-R$ Subgroups measurement control chart}\mbox{}\\\
	During our study of the averages control chart averages with fixed limits based on the standard deviation of the mean and also of the $\overline{S}-S$ control chart, we have clearly discusses that it suffered from the handicap that $\sigma_X$ is too approximate in practice (because $n_i$ is often too small).

	Therefore, a workaround is to use the results obtained from the study of the previous $\overline{R}-R$ control chart.
	
	\textbf{Pros}:
	\begin{itemize}
		\item Corrects the problem of the control charge with for averages using the traditional standard deviation or also the unbiasing standard deviation approach when the number of individuals per sample is quite small. 

		\item Represents well the change in the average level of data (central tendency).
	\end{itemize}

	\textbf{Cons}:
	\begin{itemize}
		\item The mathematical proof of the origin of the limits of controls is quite hard (see the section of Statistics for the detailed proof) and the determination of the corresponding unbiasing coefficients requires the use of Monte Carlo simulations (\SeeChapter{see section Theoretical Computing}) . 

		\item Another problem is that even if the probability distribution of sample size can be chosen, it is often given in specialized books only for the Normal distribution... (but softwares such as Minitab manage them).
 	\end{itemize}
 	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	This control chard is used a lot as part of the study of reproducibility and repeatability (R\&R Studies). This is why it is found it often in statistical process control software as automatically processes control chart when launching a R\&R analysis (typically what do Minitab).
	\end{tcolorbox}
	Let us recall first that we had:
	
	and let us recall that we have proved in the section Statistics in the context of our study of order statistics that:
	
	It comes (it is impossible that the customer sets a limit for the USL, LSL or target $T$ as regards the range as in general it is asymmetric, hence the fact that these parameters are this time not listed):
	
	still with:
	
	and the whole is traditionally written in the field:
	
	Notice that we can then calculate the probabilities of non-compliance by excess or defect of the subgroups by using the fact that $\overline{X}$ follows a Normal distribution of parameters:
	
	
	As companion example let us consider the following example of $25$ samples (that we will identify by the variable $k$) of $6$ individuals each (that we will identify by the variable $n_i=n$) taken from a continuous manufacturing process (without machine recalibration or without machine change!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_xbar_r_measurement_excel_list_data.jpg}
	\end{figure}
	where every day we took $6$ individuals of all manufactured items produced the same day.
	
	We can find in the tables that:
	
	We then have the following table:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_subgroup_measurement_xbar_r_excel_calculations.jpg}
	\end{figure}
	or explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.8]{img/engineering/control_chart_subgroup_measurement_rbar_r_excel_explicit_formulas.jpg}
	\end{figure}
	and the corresponding chart still with the same spreadsheet software:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_xbar_r_plot_excel.jpg}
	\end{figure}
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel (we can also do such a chart with R but the plot quality is not acceptable in a corporation):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_xbar_r_plot_minitab.jpg}
	\end{figure}
	which (is it necessary to recall it?) underestimates the number of error detections since the probability distribution of the range is in reality not symmetrical (hence the application of fixed Shewhart's control limits is not really suited ...) and because almost all of the tables assume the data still Normally distributed as well.
	
	Let us notice that this control chart would according to the WECO rules have $0$ point beyond the $\pm 3\sigma$.
	
	\subsubsection{Autocorrelated Measurement Control Charts (time weighted control charts)}
	The family ofautocorrelated control charts is defined by the fact that either the points on the chart or the control limits are calculated based on a number of the foregoing points.

	These conthrol chart have the advantage of requiring less data than others and to exacerbate the problem of deviation and therefore detect anomalies faster.

	The WECO rules do not apply to autocorrelated control chart. If a point is outside the calculated limits the process requires an immediate corrective action.
	
	\paragraph{$\text{I}-\text{MR}/\overline{X}$ Individual moving range measurement control chart}\mbox{}\\\
	In prototyping phase, we can not put in place the concept of sampling. In addition, the measurement short term control chart on the mean with standard deviation  does not gives the possibility to make statistical inference, because the distribution law is supposed unknown simple because the process is unstable.

	An intermediate solution is to consider then that the process is stable (assumption of Normality of the data) and calculate the limits from a minimum number of individuals that it is possible to have to make a statistic. That is to say: two!

	This situation is then well suited for prototyping phases or in the case of processes/methods with slow frequencies for which the measurements (pieces)/controls are very expensive.
	
	\textbf{Pros}:
	\begin{itemize}
		\item Complete the first control charts (measurement on subgroups) that we saw at the beginning for situations where it is difficult to have a large number of individuals by sample for reasons of cost or time.

		\item This control chart is sometimes regarded as the Swiss Army knife of quality control ... and is increasingly used because it is the policy of the "Just in Time" (JIT) production for  small series (for the decrease of costs).
	\end{itemize}

	\textbf{Cons}:
	\begin{itemize}
		\item The mathematical proof of the origin of the control limits is quite difficult (see the section Statistics for detailed profed) and the determination of the coefficients requires the use of  Monte Carlo simulation (\SeeChapter{see section Theoretical Computing}). 

		\item Another problem is that although the probability distribution of the range can be chosen, it is often given in specialized books only for the Normal distribution (but most advanced softwares manage various distribution laws).
 	\end{itemize}
 	As workaround of the problem of an acceptable estimate of the standard deviation, the idea is to consider two successive measurements $X_{i-1}$,$x_i$ as the random variables of a rank statistics of which we calculate the range in absolute terms:
	
	where MR stands for "Moving Range" (the control chart is sometimes named "\NewTerm{MR($2$) control chart}\index{control chart!MR($2$) control chart}"). And then we have:
	
	since we have necessarily $n-1$ moving ranges.

	Then we take the back the definitions of the control chart my measurement for the mean with short term standard deviation:
	
	and we can get rid of the of the estimation problem of $\sigma_X$ using the relation proved in the section Statistics during our study of rank statistics:
	
	Therefore rearraning and using the prior previous relation:
	
	It comes (it is impossible that the customer sets a limit for the USL, LSL or target $T$ as regards the range as in general it is asymmetric, hence the fact that these parameters are this time not listed):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The same unbiasing constant can be used for subgroup control charts! Indeed, rather than using $R_i=x_{i-1}-x_i$ we use $R_i=\overline{X}_{i-1}-\overline{X}_i$ where $i$ is then the number of the sample!!! This is why in fact Minitab propose in the menu of measurement subgroup control charts the a MR chart!!!
	\end{tcolorbox}
	As companion example let us consider the following data:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_mr_excel_list_data.jpg}
	\end{figure}
	Then we have for the first $17$ visible rows only:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_mr_excel_list_data_with_calculations.jpg}
	\end{figure}
	Explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/control_chart_i_mr_excel_list_data_with_formulas.jpg}
	\end{figure}
	Then we have the following chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_individual_measurement_i_mr_xbar_plot_excel.jpg}
	\end{figure}
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_mr_plot_minitab.jpg}
	\end{figure}
	Notice that this control chart would be according to the WECO rules have $0$ points beyond the $\pm 3\sigma$ (we indicate that for comparison reasons with previous control charts and the next one).
	
	As the reader have probably notice it in the figure above, Minitab also plot an I-MR/$\overline{\text{MR}}$ control chart. So let us study this one just now:
	
	\paragraph{$\text{I}-\text{MR}/\overline{\text{MR}}$ Individual moving range measurement control chart}\mbox{}\\\
	So as we just say and notice it, the previous central tendency control chart must be completed (same normally as for almost all previous control charts with central tendency) with a control chart showing the tendency of dispersion (the benefits and problems of this control chart are the same as the previous one) .

	Then we take the definitions of a measurement control chart to get:
	
	Again, the standard deviation of the moving range (MR) is constructed in the same way as the standard deviation of the range. Then we have:
	
	It comes (it is impossible that the customer sets a limit for the USL, LSL or target $T$ as regards the range as in general it is asymmetric, hence the fact that these parameters are this time not listed):
	
	which is denoted as well as the range to control chart:
	
	with for recall:
	
	As companion example let us consider the following data:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_mr_mr_excel_list_data.jpg}
	\end{figure}
	Then we have for the first $25$ visible rows only:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_mr_mr_excel_list_data_with_calculations.jpg}
	\end{figure}
	Explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/control_chart_i_mr_mr_excel_list_data_with_formulas.jpg}
	\end{figure}
	Then we have the following chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_individual_measurement_i_mr_mr_plot_excel.jpg}
	\end{figure}
	We see that this control chart with moving range seems much more sensitive than any previous control chart. But the problem remains as to know whether our observation of this control chart is correct in the sense that the moving range does not follow a symmetrical distribution law at the contrary to what presupposes fixed Shewhart limits.

	Also be aware that some of the WECO rules (the rules for sequences for example) do not apply to this type of control chart, since the data are autocorrelated there.
	
	\paragraph{Individual Moving Average control chart}\mbox{}\\\
	We always assume here that the characteristic follows a Normal distribution (...) and that the samples were taken of constant size $n$ (yes... Moving Average control charts can also be used for subgroups using moving range standard deviation or or $S$ estimation method, not only for individual measurement!!!). The moving average of duration $h$ at time $t$, written $M_t$ is defined by moving average seen in the section Statistics:
	
	The parameter $h$ is also named "horizon". Greater is the value of $h$ better is the efficiency of this control chart for the detection of small deviations.

	When $t\geq h$, assuming independence between samples, we get using the property of the variance:
	
	And if all the samples have the same size (if samples there are...)
	
	Therefore:
	 
	And:
	 	
	So Finally:
	
	Ok this done, let us now as always make a summary of the pros and cons of this control chart.
	
	\textbf{Pros}:
	\begin{itemize}
		\item The fact that we smooth the fluctuations in the purpose to reduce strong variations makes this control chart very well suited for small productions for which the total number of items is around $10$ (however as already mentioned this control chart can also be used for subgroup measurement). Also in the example that will follow, we will only focus on this particular scenario of individual measurement! 

		\item We also see that the boundaries of this control will be even smaller than $h$ is large. So this control chart can be very sensitive to small variations on the long term.
	\end{itemize}

	\textbf{Cons}:  The standard deviation is assumed to be known or at least calculated precisely so that the estimator converges to the theoretical value. All samples should be of identical size $n$ if we use the method developed above (otherwise most SPC softwares implement the necessary mathematical tools of this condition is not satisfied).
	
	As companion example let us consider the following data:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_ma_excel_list_data.jpg}
	\end{figure}
	Since the control chart is with individual values, we have $n=1$. And the system using the moving range for the estimate of the standard deviation is reduces to:
	
	therefore apart the denominators with the root of $h$, the parameters CL, UCL and LCL are the same as the I-MR $\overline{X}$ control chart. So we fall back on the same calculation method as the one provided par the NCSS Statistical Analysis \& Graphics Software.
	
	We must take in the case of our special example the value of $d_2$ for $n_i=2$. The tables or direct calculations give us:
	
	Which gives us for the calculations:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_ma_excel_list_data_with_calculations.jpg}
	\end{figure}
	Explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_i_ma_excel_list_data_with_formulas.jpg}
	\end{figure}
	Then we have the following chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_individual_measurement_i_ma_xbar_plot_excel.jpg}
	\end{figure}
	Notice that this control chart would be according to the WECO rules have $1$ points beyond the $\pm 3\sigma$ based (we indicate that for comparison reasons with previous control charts and the next one).
	
	We don't get the same results with Minitab 17.1.3 on any known prior version as the use a strange method to calculate the standard deviation for which we can not found any proof:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_i_ma_plot_minitab.jpg}
	\end{figure}
	
	\paragraph{CUSUM (cumulated sum) control chart with empirical V-mask}\mbox{}\\\
	A major disadvantage of the control chart of the Shewhart type is to base that analysis on the latest information collected (assumption of independance of measurements). They ignore the information of trends of the process contained in the latest estimates. You should know that many quality standards institutes worldwide (AFNOR or ASQ) consider Shewhart charts as completely obsolete (wrongly or rightly?) and advice the usage of CUSUM control charts because using autocorrelation and the amplification variations.

	In the years 1950 control charts named "\NewTerm{CUSUM (cumulated sum)}\index{control chart!CUSUM control chart}" or "\NewTerm{Page-Hinkley control chart}\index{control chart!Page-Hinkley control chart}" (in honor of their inventors) were introduced by statisticians to detect small variations. Although they are mathematically more optimal, they are difficult to properly configure and very sensitive to the initial settings.

	The principle of CUSUM control charts (because there are several for one given distribution law...) is to sum the differences between the estimates of the position of the method / process and its target. When the accumulated positive variances or accumulated negative variances exceed a certain value, we conclude with a off-centering of the process.
	
	We put:
	
	and:
	
	that is obviously a sum of centered reduced variables!
	
	Where often the first equality is written:
	
	Therefore there comes a famous writing in the literature:
	
	In general, the CUSUM control chart behaves as follows:
	\begin{itemize}
		\item If the process is under control, the cumulative sum fluctuates randomly around zero.

		\item If the average undergoes a positive drift of the mean, the statistic $C_n$ or $S_n$ grows quick by accumulation of a systematic bias.
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} In many books and numerous application cases, we do not use $S_n$ because the variance is unknown and we restrict ourselves to the use of $C_n$ only. The mathematical developments which then follows can be adapted very easily.\\
	
	\textbf{R2.} The same approach can be made with average of samples following:
	
	
	\textbf{R3.} There are many different versions of the CUSUM control charts and this even for the CUSUM with V-mask there are multiple sub-families. To be honest there is in my opinion quite a mess...
	\end{tcolorbox}
	For the example, let us take again the following data table:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_cusum_excel_list_data.jpg}
	\end{figure}
	and we calculate the cumulative quantity still considerating a target $\mu=T=10$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_cusum_excel_calculations_cumulated_sum.jpg}
	\end{figure}
	Thus explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_cusum_excel_calculations_cumulated_sum_formulas.jpg}
	\end{figure}
	Then we have the following chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_cusum_plot_excel.jpg}
	\end{figure}
	We then very quickly see with this control chart that the cumulate sum seems to decrease. This indicates that the true mean quality is lower than the target (you can try with $9.994$ as the target to see the sensitivity).
	
	We must now consider two cases:
	\begin{enumerate}
		\item[H1.] lt comes under the assumption of normality (process under control) and by the property of stability of the Normal distribution for a centered reduced random variable:
		
		or otherwise:
		
		Therefore, under this hypothesis, we have the cumulative sums $(S_i)_{i=1\ldots n}$ which are distributed around a regression line of slope $0$ (since the average of the sums of a random variables with zero mean is zero...) and the distributions have their standard deviation, which is proportional to the root of $i$ (respectively the variance which is proportional to $i$).
		
		\item If the process is out of control (global dispersion), there is a moment $t<n$ such as $X_i=\mathcal{N}(\mu,\sigma)$ for $i\leq r-1$ amd $X_i=\mathcal{N}(\mu+\delta,\sigma)$ for $i\geq r$ with $\delta>0$ (the latter value being often imposed by the constraints of quality and named "\NewTerm{large middle translation}\index{large middle translation}"). Therefore, we have:
		
		or:
		
		for $i=1,\ldots,r-1$ and:
		
		for $k\geq r$. That said, it is obvious that the cumulative sums:
		
		are arranged around regression line of slope $\delta/\sigma$ right from the moment $r$.
	
		Respectively we have:
		
	\end{enumerate}
	To continue, the idea is to discriminate hypothesis H1 of a regression line of zero slope and the hypothesis H2 of a regression line of $\delta/\sigma$ for $S_k$ by fixing for imaginary border the half of the slope (the right middle in other words...):
	
	respectively:
	
	and a shift (offset) $h\geq 0$ also empirical with $h\in \mathbb{N}$. At the step $n$ we reject H1 if for an integer $m\leq n$, we have:
	
	It is obviously also possible to do it by reject with a geometrical representation (this has the advantage of going much faster and to the control in one time only for any $m$ less than or equal to $n$). The idea then is to draw a line of slope $K$ starting from the point of abscissa $n+1h$. If there exist a point $(m,S_m)$ with $m<n$ below the regression line, then we pull an alarm.
	
	Let us do an example with the previous graph. Consider the following empirical values of the parameters:
	
	Thus:
	
	Therefore:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_cusum_vmask_calculations_excel.jpg}
	\end{figure}
	Explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_cusum_vmask_calculations_explicit_excel.jpg}
	\end{figure}
	We then for example at the point $26$ corresponding to $25 + h$:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_cusum_vmask_plot_excel.jpg}
	\end{figure}
	With Minitab 17.1.3 this gives for people that don't trust our calculations made with Microsoft Excel:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/engineering/control_chart_cusum_vmask_plot_minitab.jpg}
	\end{figure}
	The reader can easily observe that with the choice we made, we would have no alarm with the selected empirical parameters. It would be necessary in practice to reduce the value $\delta$.

	To resume, the benefits of CUSUM control charts compared to Shewhart control charts are:
	\begin{itemize}
		\item They are more effective for detecting small drifts

		\item The drift of the process visually appears on the control chart

		\item It is usually easy to detect at what point the process began to drift
	\end{itemize}
	The disadvantages of these control charts are the following:
	\begin{itemize}
		\item They can be slow to detect large drifts

		\item They are not very interesting to analyze past data and look for cycles or gaits characteristics in the studied distribution.

		\item They are less easily accepted by operators as less intuitive and do not directly represent the characteristic of interest.
	\end{itemize}
	For the reasons discussed above, it is advisable to represent in parallel of CUSUM control charts to communicate a classicl Shewhart control chart.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	There are still other CUSUM V-mask type. So be careful with the software you used to be sure what you are doing!!!
	\end{tcolorbox}
	
	\paragraph{EWMA control charts (exponential weighted moving average) with fixed limits}\mbox{}\\\
	An interesting alternative to the CUSUM control charts in the context of autocorrelation and the detection of small deviations are the EWMA (Exponentially Weighted Moving Average) control charts that have their origine in the field of Time Series Analysis (\SeeChapter{see section Economy}).

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Some also use control charts using empirical techniques like the simple or double moving average as discussed in section Economy when data are serially correlated.
	\end{tcolorbox}
	An EWMA control chart is easy to implement and is not too sensitive to parameter changes and non-Normal data, it remains just as powerful as the CUSUM control charts to detect small variations.

	By definition, the statistical reported on the EWMA control chart is calculated by the following equation (notice that the indices of the different terms are not the same as exponential smoothing of a time series as seen in section Economy since the goal is not to make predictions !!):
	
	
	with the particularity that for some softwares this software control chart (as Minitab) take:
	
	The constant $\lambda$ (smoothing constant) determines the weight that we want to assign to the latest measures. The smallest is this value, more the control chart is sensitive to sudden deviations.

	Let us prove that the last relation can be written as:
	
	Indeed:
	
	Let us suppose for what will follow that:
	
	Then we have:
	
	and remembering one of the properties of the variance (\SeeChapter{see section Statistics}):
	
	It comes:
	
	Let us do a change of variable:
	
	The it comes using the result proved in the context of our study of arithmetic sequences in the section of Sequences and Series:
	
	when $t$ is sufficiently large, then the variance is reduced to (approximation that the majority of softwares - like Minitab - don't do and therefore the limits are not constant depending on $t$!):
	
	the EWMA control charts limits then are also build with the $\pm\sigma$:
	
	We therefore have the a target is defined and also a standard deviation (it is impossible that the customer sets  a LSL, USL or target $T$ regarding to the exponential weighting hence the fact that these parameters are this time not listed):
	that we find in the following form when it comes to working with samples:
	
	Some softwares uses to calculate the standard deviation the following relation that we have proved earlier above:
	
	and others (such as Minitab):
	
	the latter option has the advantage of working if the sample size is unitary (we speak then of course "EWMA for individual values"). Let us recall that in fact we have already mentioned that the EWMA control chart is poorly suited to mass production.
	
	It should be noticed that regardless of the option chosen, as we have:
	
	the controls limits are always significantly below those of a range or moving range control chart. This is also a reason why we smooth the data!

	Let us do an example for each variant of calculation of the standard deviation since each is of equal importance in practice. To do this, let us start with the EWMA control chart with:
	
	and taking the following table of data:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_data_list_excel.jpg}
	\end{figure}
	We get:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_calculations_excel.jpg}
	\end{figure}
	Thus explicitly (we have cut the screenshot into two parts because of horizontal space on the page):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_calculations_formula_excel_part1.jpg}
	\end{figure}
	and for the second part:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/control_chart_ewma_calculations_formula_excel_part2.jpg}
	\end{figure}
	We then have the following control chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_ewma_plot_excel.jpg}
	\end{figure}
	With Minitab 17.3.1 we get a small difference in comparison to our calculations by hand and this is normal as we have made an approximation for a large $t$ and also we made on of the two choice for $F_0$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_plot_minitab.jpg}
	\end{figure}
	For the last example, we take the famous case of an EWMA control chart with individual values and:
	
	with the following data:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_moving_range_excel_list_data.jpg}
	\end{figure}
	by taking therefore (caution! the $n$ in the following relation is not the same as that in the root containing the smoothing constant!):
	
	and:
	
	as well as:
	
	We then with a smoothing constant of $0.6$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_moving_range_excel_calculations.jpg}
	\end{figure}
	Thus explicitly (we have cut the screenshot into two parts because of horizontal space on the page):
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_moving_average_calculations_formula_excel_part1.jpg}
	\end{figure}
	and for the second part:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.85]{img/engineering/control_chart_ewma_moving_average_calculations_formula_excel_part2.jpg}
	\end{figure}
	We then have the following control chart:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/engineering/control_chart_ewma_moving_range_plot_excel.jpg}
	\end{figure}
	With Minitab 17.3.1 we get a small difference in comparison to our calculations by hand and this is normal as we have made an approximation for a large $t$ and also we made on of the two choice for $F_0$:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_ewma_moving_range_plot_minitab.jpg}
	\end{figure}
	As with other control charts, the EWMA control chart must be accompanied by a control chart for monitoring the variability, so either a control chart based on ranges with fixed limits or a control chart with standard deviations with fixed limits.
	
	\subsubsection{Rare events control charts}
	Rare events inherently occur in all kinds of processes. In hospitals, there are medication errors, infections, patient falls, ventilator-associated pneumonias, and other rare, adverse events that cause prolonged hospital stays and increase healthcare costs. 

	But rare events happen in many other contexts, too. Software developers may need to track errors in lines of programming code, or a quality practitioner may need to monitor a low-defect process in a high-yield manufacturing environment. Accidents that occur on the shop floor and aircraft engine failures are also rare events, ideally.

	Whether you're in healthcare, software development, manufacturing or some other industry, statistical process control is an important component of quality improvement. Using control charts, we can graph these rare events and monitor a process to determine if it's stable or if it's out of control and therefore unpredictable and in need of attention.
	
	\paragraph{Frequency $T$ control chart with probabilistic limits}\mbox{}\\\
	The monitoring of frequency of occurrence of defects / accidents / anomalies is a very simple technique to implement and very easy to read for people with little or no knowledge of statistics. Moreover it is a technique adapted to cases where the monitored events are very rare.

	The idea then is not to track the number of special events, but the time $T$ between two appearances of the monitored events. The time $T$ is then measured in either number of pieces or number of accidents, defects, days, etc.

	For companion example let us consider the following list of data:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_frequency_excel_list_data.jpg}
	\end{figure}
	Which gives graphically:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_frequencies_plot_excel.jpg}
	\end{figure}
	Well it's nice, but we we need an UCL and LCL for this control chart to be reaaly useful! To achieve this we will use the queing theory with the same assumptions and the same checkpoints (\SeeChapter{see section Quantitative Managnement Techniques}). Let us first recall that we had showed there  that the probability of observing $k$ events in a time interval $t$ (or an interval of pieces number $t$) under some very specific assumptions (!!!), followed a Poisson distribution:
	
	where the parameter $\lambda$ is the average arrival rate (or "average rate of occurrence") of events per unit time (or "\NewTerm{Poisson Arrivals See Time Average (PASTA)}\index{Poisson Arrivals See Time Average}") assumed constant over the entire period:
	
	and where we have to mean and variance (\SeeChapter{see section Statistics}) the number of events:
	
	This is a probability law with discreet support as we know, which fits well with our case studies so the Poisson distribution can be a good candidate (but there are others, like for example the Geometric law!!!).

	But we also proved that the random variable $\tau$ representing the time (or the number of pieces) separating two arrivals of unwanted events (or desired...) was given by the distribution function based on the exponential law:
	
	Still with:
	
	where:
	
	So we could take the following Shewhart limits :
	
	But it is more rigorous (as for all other control charts) to use probabilistic limits. So if we want to set limits with a usual bilateral risk threshold $\alpha/2$ of $0.5\%$ then we calculate for the lower limit:
	
	and for the superior limit:
	
	Then we have:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/engineering/control_chart_frequency_calculations_excel.jpg}
	\end{figure}
	Thus explicitly:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/engineering/control_chart_frequency_calculations_formulas_excel.jpg}
	\end{figure}
	We then have the following control chart (we have exceptionally not represented the USL on it):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.65]{img/engineering/control_chart_frequencies_plot_with_limits_excel.jpg}
	\end{figure}
	With Minitab 17.3.1 we get something quite different all control limits. As they don't give the proofs but only the formulas we are not able to say who has the most correct way between we and them to make the calculations:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.9]{img/engineering/control_chart_frequencies_plot_minitab.jpg}
	\end{figure}
	
	\paragraph{Frequency $G$ control chart of rare events}\mbox{}\\\\
	Developed by James Benneyan in 1991, the g chart  is a control chart that is based on the geometric distribution. Benneyan has since published several papers about the G chart and a companion chart, the "$h$ chart". The majority of applications cited in these papers are for monitoring infection rates in healthcare, such as nosocomial infections. Nosocomial infections are infections that occur as a direct result of a patient’s treatment in a medical facility.

	$P$ charts and $U$ charts are often used to monitor adverse events such as nosocomial infections. But $P$ charts and $U$ charts require very large quantities of data and specific definitions of the data. For example, if you use a $U$ chart to monitor nosocomial infections, each patient day is considered an area of opportunity in which one or more infections could occur. Thus, the data are the number of infections per patient day. If you use a $P$ chart, the data are the number of patient days in which one or more infections occur. As for the data requirements, if you follow the standard practice of requiring a minimum of $25$ to $35$ subgroups to establish control limits, and the infection rate is low (for example, $< 1\%$), the required amount of data is at least $12,500$ patients ($500$ patients per subgroup multiplied by $25$ subgroups). This means that it could take weeks, months, or perhaps even years to accumulate enough data to detect and respond to changes in the infection rate.
	
	The geometric distribution provides an alternative probability model. In the geometric distribution, you count the number of opportunities before or until the defect occurs. Thus, in a healthcare setting where you monitor the infection rate, the ideal would be to count the number of patients or procedures until an infection is observed. While this is the ideal, it is also rarely done, because of complications with counting the actual number of patients through the system, or the number of procedures. What is most often done is to count the number of days between observed infections. The key assumption used when counting the number of days is that the number of
patients or procedures per day is fairly constant.

	We have proved in the section Statistics that the mean to have $R$ successes before the $E$-th failure knowing that the probability of a failure is $p$ was given by the mean of the negative binomial distribution:
	
	and we get for to the variance:
	
	Obviously, if we set $E$ as the first failure, the mean is reduced to that of the geometric law:
	
	We have proved in the section Statistics that the estimator of the parameter of the geometric law was given by:
	
	Therefore it comes:
	
	and:
	
	and therefore:
	
	Therefore we have:
	
	where if LCL is negative, we put it as always as being equal to zero.
 
 	Let us see an example by considering the following list in Microsoft Excel 14.0.6123:
 	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_frequencies_g_list_data_excel.jpg}
	\end{figure}
	with the following formulas:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/control_chart_frequencies_g_formulas_excel.jpg}
	\end{figure}
	Which gives graphically:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.75]{img/engineering/control_chart_frequency_g_plot_minitab.jpg}
	\end{figure}
	With Minitab 17.3.1 we get something quite different all control limits. As they don't give the proofs but only the formulas we are not able to say who has the most correct way between we and them to make the calculations:
	\begin{figure}[H]
		\centering
		\includegraphics{img/engineering/control_chart_frequencies_g_plot_minitab.jpg}
	\end{figure}
	For a software like Minitab, the lower control limit of the rare events chart is set to the percentile $0.0013499$ of the geometric distribution (taking the nearest decimal value by linear interpolation). The upper control limit is set to the percentile $0.99865$ (also taking the nearest decimal value by linear interpolation). The central line is set at $0.5$ percentile (taking the nearest decimal value by linear interpolation).
	
	It should be also remembered that the Geometric law  (or the Negative Binomial law) is not necessarily symmetrical. This is why, in practice, you should rather use the median for the CL (centerline which can be determined by simple simulations of the law and taking the nearest value of the $50\%$ percentile) and for the LCL and UCL control limits use also the probabilistic simulations with the same simulations.
	
	\pagebreak
	\subsubsection{Control Charts Operating Characteristic (OC) Curves}
	In the usage of control charts we face two types of errors (putting apart human error and incompetence error...) that are well known to us in the context of our study of statistics:
	\begin{itemize}
		\item Type I error: We detect an alarm when there is actually no alarm

		\item Type II error: We are out of control but we do not detect alarm
	\end{itemize}
	The aim here will be to determine the conditions for controlling these two errors.
	
	As we will see the operating characteristic function for a Shewhart chart, is the probability, the statistics of interest falls between the lower un upper control limits.

	\paragraph{OC for $\bar{X}$ measurement control charts}\mbox{}\\\\
	We have already demonstrated earlier above that the capability that a process centered at $3\sigma$ had a cumulative probability of $0.27\%$ ($1/370$) to have a point outside the $3\sigma$ and this only for reasons of statistical fluctuations (error of Type I). So in extenso to $3\sigma$ the error type II is $1-0.27\%$ which is quite huge but acceptable in practice (as practitioner consider a power of $80\%$ as being acceptable)! The reason for this high amplitude of type II error is implicitly due to the fact that we calculate here:
	
 	that is to say that the data have all been centered reduced as well as in extenso that the USL and the LSL and where for recall is the symbol $\Phi$ stand for the cumulative probability function of the Normal law $\mathcal{N}(0,1)$.

	But if the data is not centered, then we must calculate:
	
 	But if we are interested in a certain lag by loss of control of the process, then we have by expressing this shifts in multiples $m$ of $\sigma_X$ as it is done by tradition (this choice will be justified a little later):
	
 	Now, let us recall that we have in the framework of an $\bar{X}$ control chart:
	
	Therefore it comes:
	
 	We see that the choices at the beginning in the way of noting the offset advantage us by the fact that the error of type II is a function of more than three parameters of which $k$ is almost always fixed at $3$ and $n$ is chosen by the experimenter.

	So finally we have:
	
 	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Suppose we use an $\bar{X}$ control chart with a $k$ of $3\sigma_{\bar{X}}$, a sample size of $n=5$ and we want to know the Type II error to detect an offset of $2\sigma_X$ ($m=2$). From then on, it comes:
	
	So the risk of type II is this time much lower but the error of type I then becomes $1-0.0705 = 0.9295$ which is huge. This is why it is often necessary to find the right compromise between the quality policy and the adjustment adjustment costs.
	\end{tcolorbox}
	Let us indicate that the general case, for $k$ equal to $3$, we have the following curve which is named "\NewTerm{$\bar{X}$ control chart operating characteristic curve}\index{operating characteristic curve}":
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/operating_characteristic_curve_xbar_control_chart.jpg}
		\caption{$\bar{X}$ control chart operating characteristic curve}
	\end{figure}
	So on the plot above we have $k=3$, $n=5$, $m=2$:
	
 	either with the English version of Microsoft Excel 14.0.7177:
	\begin{center}
		\texttt{=NORM.S.DIST(3-2*SQRT(5),TRUE)-NORM.S.DIST(-3-2*SQRT (5),TRUE)}
	\end{center}
	
	\paragraph{OC for $P$-type attribute control charts}\mbox{}\\\\
	The same type of reasoning applies for a attributes $P$-type control chart! We have then:
	
	However, we saw in the Statistics section that when $N$ is large the interval could be estimated using a Normal law such that:
	
 	Hence, since $\bar{\hat{p}}$ follow approximately a Normal law, we arrive at the same type of calculations as for the previous control chart:
	
	Thus finally:
	
	
	 \pagebreak
	 \subsection{Design of reliability tests}
	 "\NewTerm{Reliability test plans}\index{reliability test plan}" have two main functions: One is to determine the sample size and testing time needed to estimate model parameters. The other is to demonstrate that we have met specified reliability requirements.

	A test plan includes:
	\begin{itemize}
		\item The number of units you need to test.
		\item A stopping rule: the amount of time you must test each unit or the number of failures that must occur.
		\item Success criterion: the number of failures allowed while the test still passes (for example, all units are tested for the specified amount of time and there are no failures).
	\end{itemize}
	Three types of test plans are available: demonstration, estimation, and accelerated life.
	\begin{itemize}
		\item "\NewTerm{demonstration plan}\index{demonstration plan}" to determine the sample size or testing time needed to demonstrate, with some level of confidence, that the reliability is higher than a particular standard.

		There are two types of demonstration tests:
		\begin{enumerate}
			\item "\NewTerm{Substantiation tests}\index{substantation tests}" that provide statistical evidence that a redesigned system suppressed or significantly reduced a known cause of failure. For example, is the redesigned system better than the previous system?\\
			
			 We can rewrite these hypotheses in terms of the scale (Weibull or exponential distribution) or location (other distributions), a percentile, the reliability at a particular time, or the mean time to failure (MTTF). For example, we can test whether the MTTF for a redesigned system is greater than the MTTF for the previous system.\\

			\item "\NewTerm{Reliability test}\index{reliability test}" that provide statistical evidence that a reliability specification is achieved. For example, is the system reliability greater than a goal value?
		\end{enumerate}

		\item "\NewTerm{Estimation test plans}\index{estimation test plans}" to determine the number of test units that we need to estimate percentiles or reliabilities with a specified degree of precision. Estimation test plans are similar to classic sample-size problems, but computations are more intensive because the data are usually censored.

		\item "\NewTerm{Accelerated life test plans}\index{accelerated life test plans}" to determine - under some very strong assumptions - the number of units to test and the allocation of those test units across stress levels for an accelerated life test. We can also use these test plans to determine the standard error for the parameter we want to estimate for a fixed number of test unit
	\end{itemize}
	 \begin{figure}[H]
		\centering
		\includegraphics[scale=0.52]{img/engineering/pnnl_demonstation_plan.jpg}
		\caption{Pacific Northwest National Laboratory automatic long-term performance measurement equipment for  Philips LED 60 Watt, which can test up to $200$ lamps at a time}
	\end{figure}
	Several methods have been designed to help engineers for all this purposes: Cumulative Binomial, Non-Parametric Binomial, Exponential Chi-Squared and Non-Parametric Bayesian. They are discussed in the following paragraphs.
	
	\subsubsection{Chi-squared time of test}
	A method for designing tests for products that have an assumed constant failure rate, or exponential life distribution, draws on the chi-squared distribution. These represent the true exponential distribution confidence bounds referred to in The Exponential Distribution. This method only returns the necessary accumulated test time for a demonstrated reliability or $\overline{\text{MTTF}}$.
	
	We have seen earlier above that for a survival function of the form:
	
	The corresponding density function was the exponential law:
	
 	and that we had:
	
 	Our objective here will be to find the value that permits a reliability engineer to know how long a test must be performed to demonstrate with a $90\%$ confidence level (CL) that its product has zero failure.

	What looks like in the worldwide reference Weibull++ software for reliability engineering to the following:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/design_of_reliability_exponential_chi_squared_time_of_test.jpg}
		\caption{Exponential Chi-squares time test in Weibull++ 7 software}
	\end{figure}
	We will prove in the following that this result follows from a close relationship between the exponential distribution, the Poisson distribution through the queue theory (\SeeChapter{see section Quantitative Management Techniques}) and the chi-square distribution.

	To prove this result, let us recall that we have prove in the section of Quantitative Management Techniques that under certain hypotheses of a queue, the probability of seeing an event in an interval of duration $t$ is given by the function:
	
 	and that the corresponding density function of the corresponding time between two events is an exponential law. In other words, when the time of failure follows an exponential law, the probability of a certain number of failures in a certain time interval follows the relation given just before!

	The previous relation can be used in the case of reliability to obtain an upper bound of $\lambda$ for a given time interval $t$ since we can then write with the Erlang distribution function (\SeeChapter{see section Quantitative Management Techniques}):
	
	A small and simple change of variable gives:
	
	We therefore recognize the function of the distribution of the Poisson distribution! However, this is impossible to reverse by hand (as far as we know). Reason why it is wiser to find a relation with another known distribution that is continuous in the purpose to reverse it by hand.

	For this, let us recall that we have proved during our study of the chi-square law in the section Statistics that:
	
	Therefore, by re-reading carefully the changes of variables made in the section Statistics we have in other words:
	
 	It is therefore immediate that:
	
	It follows that:
	
	Conversely:
	
	Therefore:
	
	
	\subsubsection{Binomial sampling size}
	In demonstration plans, an important calculation is to determine the size of a sample in order to demonstrate a certain degree of reliability for a given level of confidence. In the case where the test time is equal to the demonstration time, the binomial distribution is widely used in practice because nonparametric and relative only to the estimated parameter $R$ (see earlier above):
	
	with $f$ the number of failures, $n$ the size of the sample, and $R$ the reliability that is sought to be demonstrated. We then speak of a "\NewTerm{nonparametric binomial demonstration plan}\index{nonparametric binomial demonstration plan}".
	
	As the reader can notice, there is no time value associated with this methodology, so one must assume that the value of $R$  is associated with the amount of time for which the units were tested.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider that an engineer wants to set up a demonstration plan in order to demonstrate the zero failure of a product with an $80\%$ reliability with a confidence level of $90\%$. We have then:
	
	This gives by taking the logarithm:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.7]{img/engineering/design_of_reliability_binomial_sample_size_test.jpg}
		\caption{Binomial sample size demonstration plan in Weibull++ 7  software}
	\end{figure}
	\end{tcolorbox}
	However, in reliability tests, in order to better estimate reliability, Bayesian methods are used, especially when sample size is financially limited. If there is an a priori information about the reliability of the system, this information can then be used to build better demonstration plans.
	
	We will see later that the application of Bayesian techniques gives a quasi-similar result to the above example for certain values a priori well chosen...
	
	\subsubsection{Beta-binomial sampling size}
	The regular non-parametric analyses performed based on either the binomial or the chi-squared equation were performed with only the direct system test data. However, if prior information regarding system performance is available, it can be incorporated into a Bayesian non-parametric analysis. This subsection will demonstrate how to incorporate prior information about system reliability and also how to incorporate prior information from subsystem tests into system test design.

	Thus, since $R$ is bounded in the interval $[0, 1]$ a well-known a priori function is the beta function (\SeeChapter{see section Statistics}). We proved in the section Probabilities that the a posteriori law was then a beta-binomial for of which we had specified that reliability engineers noted the cumulative probability under the somewhat unfortunate notation:
	
 	Therefore, we have in our case a posteriori:
	
 	We know $f$ which is imposed to us by the demonstration plan, we are always trying to determine $N$. We know that for the beta law:
	
 	that we have:
	
	With a little elementary algebra we get:
	
 	The question is then what to do now ...? Then here the statistician engineer uses a trick ... It takes the generalized beta law with $4$ parameters including thus the famous optimistic, pessimistic interval $[O, P]$ that we studied in the section of Quantitative Management Techniques and uses the corresponding relation of mean and variance that we have proved:
	
	Where normally we should use for the modal value as we have proved in the section of Quantitative Management Techniques:
	
	but the overwhelming majority of engineering project management and reliability software make the fundamental error of asking the modal value to the user (we will see a concrete example a little later with the most famous reliability software used in the USA).

	Once this is done. The idea is to return to a beta law with two parameters by injecting the values of expectation and variance obtained by the beta law with $4$ parameters in:
	
 	which obviously imposes indirectly that beta laws with $4$ and $2$ parameters have then same variance and even expectation!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider that an engineer wants to set up a demonstration plan in order to demonstrate that the reliability of its product is $90\%$ with an $80\%$ confidence level assuming that a single failure is allowed and that the engineer believes that the optimistic reliability is of $O=87\%$, the most likely (modal) value of of $M_0=90\%$ and the greatest reliability is equal to $P=99\%$.\\

	We have then:
	
	and thus the parameters of the beta law with two parameters are then:
	
	We must then find $N$ such that:
	
 	It is easy to get $N$ with a spreadsheet software like Microsoft Excel using the BETA.DIST with Microsoft Excel 2007 and higher:
	\begin{center}
	\texttt{= BETA.DIST (90\%; N + 184.4125-1; 19.337; 1)}
	\end{center}
	And with the target value tool, we get the result $N=24$ by rounding to the nearest integer. This corresponds well to the Weibull++ software output (see next page):\\
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1]{img/engineering/design_of_reliability_beta_binomial_sample_size_test.jpg}
		\caption{Beta-Binomial sample size demonstration plan in Weibull++ 7 software}
	\end{figure}
	It is interesting to know that if the modal value $M_0$ had been calculated rather than requested, we would have therefore $N=-30$ since in the latter case $M_0=97.246\%$... So we see what any model hat obviously its limits...\\

	If we use the same parameters with $O=87\%$, $M_0=90\%$, $P=99\%$ with our case of the Binomial law that we used as the first example above, we then rounded to the nearest integer $N=13$.
	\end{tcolorbox}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{60} & \pbox{20cm}{\score{3}{5} \\ {\tiny 19 votes,  53.68\%}} 
	\end{tabular} 
	\end{flushright}