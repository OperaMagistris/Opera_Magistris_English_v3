	%to force start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Functional Analysis}\label{functional analysis}
\lettrine[lines=4]{\color{BrickRed}F}unctional analysis is the branch of mathematics and specifically of the analysis that is related to the study of function spaces. It takes its historical roots in the study of transformations such as the Fourier transform and in the study of differential equations and integrals. As such it encompasses so many areas that it is difficult to justify that it be a section of this book because it is rather a field of study. Moreover, it is because of this difficulty to accurately identify the area it covers that the reader will find the Fundamental Theorem of Calculus in the chapter of Integral and Differential Calculus rather than here... 

	Why do we use the term "analysis" in the particular case of functions? The reason lies in the historical study of various phenomena of nature and resolution of various technical problems and therefore mathematics, which often lead us to consider the variation of a parameter correlated with the variation of another or several other variables. To study these variations, many tools are available to each of us:
\begin{itemize}
	\item The engineer, for example, frequently use charts (in cartesian, polar or logarithmic coordinate system... concepts which are discussed further in more detail) to determine the mathematical relations (or "law") linking variables between them. Certainly, this kind of method is (sometimes ...) aesthetic but students know well how it is sometimes painful to transcribe measures points on a sheet of paper or on a computer and consultants know how dangerous can be a chart when not build in a scientific way. This is unfortunately a necessary step (but should avoid an abusive usage) to understand how our predecessors worked and got the results that help us today in our advances in theoretical physics.
	
	\item The mathematician and theoretical physicist usually hate to use the paper-pencil-scrawl methods. Nevertheless, the role of the mathematician or physicist is to develop new theories with mathematical axioms or principles which should require no usage of graphical representation nor access to experimental  measures that are often attached to it.
\end{itemize}

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
Before starting to read what follows, it may be useful to remind the reader that the definition of the concept of "function" (and the basic properties thereto) is given in the section on Set Theory.
	\end{tcolorbox}	

	Function analysis is also strongly linked to Vector Calculus (and not only...). Thus for people who want to increase their knowledge about the fundamentals of function analysis we strongly recommend the reader to have a look to the Vector Calculus section.

\pagebreak
\subsection{Representations}

	We will see in what follows, firstly, how to represent different values related by tables and charts (yes! We must because it helps to understand more complicated stuff) and secondly how to mathematically analyse the properties of these representations only by using abstract mathematical tools.

	\textbf{Definition (\#\mydef):} A function is named "\NewTerm{univalent function}\index{univalent function}" or  "\NewTerm{unary function}\index{unary function}" if the number of its arguments (parameters or variables) is equal to one. In the case of a function of two arguments, we speak about a "\NewTerm{bivalent function}\index{bivalent function}" or "\NewTerm{bivalent function}\index{bivalent function}", etc. Formally a function is $n$-ary if:
	

\subsubsection{Tabular Representation}

Among the possible visual representation of functions, the most intuitive and the oldest is the one where we have in the column or the row of a table in an orderly way the values of the independent variable $x_1,x_2,...,x_n$ and the corresponding values, namely the "\NewTerm{transformed variables}\index{transformed variables}" of the function $y_1,y_2,...,y_n$ in another column or aligned row:

	\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{2.5cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor{black!30}\textbf{$x$}} & 
  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{$y=f(x)$}} \\ \hline
				\centering\arraybackslash\ $x_1$ & \centering\arraybackslash\ $y_1=f(x_1)$ \\ \hline
				\centering\arraybackslash\ $x_2$ & \centering\arraybackslash\ $y_2=f(x_2)$  \\ \hline
				\centering\arraybackslash\ $...$ & \centering\arraybackslash\ $...$  \\ \hline
				\centering\arraybackslash\ $x_n$ & \centering\arraybackslash\ $y_n=f(x_n)$  \\ \hline
		\end{tabular}
	\end{center}
	\caption[]{Values and corresponding transformed variables}
	\end{table}	
	
	In the expression:
	
	we say that the $a_1,a_2,...,a_n$ are the "\NewTerm{arguments}\index{arguments}" of $f$.

Such are, for example, tables of trigonometric functions, logarithmic tables, etc. and during the experimental study of certain phenomena tables which express the existing functional dependence between the measured physical quantities such as the readings of the temperature of the air stored in a meteorological station during one day.

Of course, this concept can be generalized to any multivalent function regardless its definition domain.

However, this method is laborious and does not permit to directly see the behaviour of the function and therefore a simple and attractive visual analysis of its qualitative properties. It still has the advantage of not requiring any special tools or advanced mathematics.

	\pagebreak
	\subsubsection{Graphical Representation}
	The natural, relative, real or purely imaginary numbers  (\SeeChapter{see section Numbers page \pageref{type of numbers}}) can all be represented as simply by points on a numerical infinite axis (straight line).

	To this purpose, we choose on this axis:
	\begin{enumerate}
		\item A point O named "\NewTerm{origin}\index{origin}"
		\item A positive direction, that we indicate by a horizontal arrow
		\item A unit of measure (usually represented by small vertical lines: the "\NewTerm{graduation}\index{graduation}")
	\end{enumerate}
Such that:
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representative_1d.eps}
\caption{Typical representation example of an oriented infinite axis with origin}
\end{figure}
	In most cases we put (traditionally) the axis horizontally and choose the direction from left to right (at least when there is only on axis...).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The point (letter) $O$, frequently represents the number zero in mathematics but we might very well choose to put the origin elsewhere. For example, in physics, the point $O$ is often positioned at the location of the centroid of a system. 
	\end{tcolorbox}
	It is obvious that the fact that the sets of numbers that we discussed in the section Numbers are ordered implies that every number is represented by a single point on this axis. Thus, two distinct real numbers correspond two different points on the axis.

	Thus, there is a correspondence between all numbers and all the points of the axis (in the case of real or complex numbers, it corresponds not a number to each graduation, but a number at each \underline{point} of the axis!). Thus, each number represents a point or a unique graduation and back to each point or graduation is a single number which is the image.

\pagebreak
\paragraph{2D representations}\mbox{}\\\\
There are besides the one dimensional representations, other of higher dimensions (phew!...) like the "\NewTerm{planar representation}\index{planar representation}" that allow us to draw much more than simple points on a one-dimensional straight line but functions of one variable (but also points!). Let's see what this is and looks like:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{img/analysis/cartesian_plane.jpg}
	\caption{Position of a point in a cartesian plane}
\end{figure}
In the above figure we have added two graduation that helps us to identify uniquely the position of the point $P$ given by $P(x,y)=P(+4,+3)=P(4,3)$. We then speak of "cartesian coordinates of a point".

In single-variable calculus, the functions that one encounters are functions of a variable (usually $x$ or $t$) that varies over some subset of the real number line (which we denote by $\mathbb{R}$). For such a function, say, $y = f (x)$, the graph of the function $f$ consists of the points 
	
These points lie in the Euclidean plane, which, in the Cartesian or rectangular
coordinate system, consists of all ordered pairs of real numbers $(a,b)$. We use the word "Euclidean" to denote a system in which all the usual rules of Euclidean geometry hold (\SeeChapter{see section Euclidean Geometry page \pageref{hilbert axioms}}). We denote the Euclidean plane by $\mathbb{R}^2$. where the exponent "$2$" represents the number of dimensions of the plane.

Thus we can for each of a variable $x$ on a horizontal axis, named commonly "\NewTerm{$x$-axis\index{$x$-axis}}" match a value $y$ through a function $f$ such that:
	
plotted on a vertical axis, named commonly the "\NewTerm{$y$-axis}\index{$y$-axis}" which passes through the junction defined by the origin $O$ such tat (arbitrary example):
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{img/analysis/representative_2d_planar.eps}
	\caption{Typical example of a planar representation with orthogonal axes, origin O and the $4$ quadrants}
\end{figure}
All points of the plane (that latter being denoted with the variations $X\text{O}Y$, $XY$ or $x\text{O}y$, $\text{O}xy$, $xy$) have for "\NewTerm{abscissa}\index{abscissa}" traditionally the $x$-values corresponding to the independent variable (horizontal axes by tradition) of the function and for "\NewTerm{ordinate}\index{ordinate}" the corresponding value of the function (vertical axes by tradition). All these generated what is named the "\NewTerm{planar graph}\index{planar graph}" of the function. If there is no confusion, we just say "\NewTerm{graph}\index{graph}".
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{img/analysis/vocabulary_planar_graph.jpg}
	\caption{Vocabulary planar graph}
\end{figure}
An interesting detailed example for middle schools students that may help is the planar representation of the following line equation:

That give in the range $x\in [-3.3,+3.3]$ the following graph:
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{img/analysis/line_equation_plan_representation.jpg}
\end{figure}
with its equivalent tabular representation for some arbitrary chosen points:
\begin{table}[H]
	\begin{center}
		\definecolor{gris}{gray}{0.85}
			\begin{tabular}{|p{2cm}|p{3.5cm}|p{2.5cm}|}
				\hline
				\multicolumn{1}{c}{\cellcolor{black!30}\textbf{$x$}} & 
  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{$2x-3$}} &  \multicolumn{1}{c}{\cellcolor{black!30}\textbf{Point $(x,2x-3)$}}\\ \hline
				\centering\arraybackslash\ $-1$ & \centering\arraybackslash\ $2\cdot(-1)-3=-5$ & \centering\arraybackslash\ $(-1,-5)$ \\ \hline
				\centering\arraybackslash\ $0$ & \centering\arraybackslash\ $2\cdot (0)-3=-3$ & \centering\arraybackslash\ $(-1,-5)$  \\ \hline
				\centering\arraybackslash\ $+1$ & \centering\arraybackslash\ $2\cdot(+1)-3=-1$ & \centering\arraybackslash\ $(-1,-5)$  \\ \hline
				\centering\arraybackslash\ $+3$ & \centering\arraybackslash\ $2\cdot(+3)-3=+3$ & \centering\arraybackslash\ $(-1,-5)$  \\ \hline
		\end{tabular}
	\end{center}
	\caption[]{Values and corresponding transformed variables}
	\end{table}	
In the case of representation by a rectangular coordinate system (cartesian, polar or logarithmic) as the figure above, we can see that the entire coordinate plane is divided into four areas that by tradition we name "\NewTerm{quadrants}\index{quadrants}" as already mentioned just earlier.

	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	When we wish to highlight a particular point on the graph representing the function, we draw most of time a small round as presented in the prior-previous figure for the point of coordinates $(x_n,y_n)$.
	\end{tcolorbox}	

Another classic case of plane graph representation  known by a large number of students is the plot of polynomials (\SeeChapter{see section Calculus page \pageref{polynomial}}) with real coefficients or trigonometric functions (\SeeChapter{see section Trigonometry page \pageref{trigonometry}}).

Indeed, to solve polynomial equations of the second degree (\SeeChapter{see section Calculus page \pageref{second order polynomials}}), it is common in small classes that the teacher asks his students in addition to give an algebraic expression of the roots of:
	
given by for recall (see section Calculus page \pageref{double root} for the proof):
	
a graphics resolution where the two roots (in the case where there are two distinct real roots) are given by the intersection of the parabola with the $x$-axis (of course, if the equation has no solution, there are no intersections...):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/roots_parabola.eps}
\caption{Representation of roots on a planar graph}
\end{figure}

The graphical representation can be generalized to polynomial equations of the 3rd, 4th and 5th degree (we will prove much further, using Galois theory that it is not possible to get a general algebraic expression of the roots of a polynomial equation of the 5th degree and higher).

There is another well-known and interesting example of special graph because when most young people think that after high-school they will never do maths again, in Switzerland many employees are faced to calculate in spreadsheet softwares what we name the "coordinate wage" that is a "\NewTerm{step-wise function}\index{step-wise function}" defined in year 2013 by the government as:

	

Where $R$ is a minimal value defined also by the government as being equal to 25,800.- in 2013 and the wage is denoted by the letter $S$ (for \textbf{S}alary).

When we plot such a stepwise function with for example Maple 4.00b we get:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/analysis/step_wise_function.eps}
\caption{Example of step-wise function for swiss coordinate wage with Maple 4.00b}
\end{figure}

And therefore it is obvious thank to this chart representation that the previous definition can be simplified as:

	

	That is much easier to write in any spreadsheet software or also with Maple 4.00b:

\texttt{>R:=258000;}\\
\texttt{>plot(min(max(R/8,S-7/8*R),17/8*R,S=3/4*R..100000);}

	Also, graphs\index{graphs}\index{charts} are as we know powerful qualitative tools in the field of statistics (\SeeChapter{see section Statistics page \pageref{statistics}}) but also of data mining (\SeeChapter{see section Numerical Methods page \pageref{data mining}}) as a starting point for data analysis (histograms, cheese, box plots, radar, scatter plots, etc.). The assumptions and ideas that are generated by graphical analysis can be investigated with advanced statistical tools (for a few hundred of examples see the \texttt{R} or MATLAB™ softwares companion book).

Below for example, a graph (histogram) taken from the Industrial Engineering section that is very common in the field of statistics and project management in the global industry:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/six_sigma.eps}
\caption{Example of typical histogram in engineering companies (Six Sigma)}
\end{figure}

Histograms allow to observe distributions and determine qualitatively if it fits a particular theoretical model.

Graphics can also be used to observe changes over time (time series, control charts, residual analysis, etc.):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/time_serie.eps}
\caption{Example of OHLC time series with moving averages in financial trading}
\end{figure}
or another type of OHLC (Open-High-Low-Close) trading plot:
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/plot_OHLC.jpg}
\end{figure}
There are different rules for the colors of an OHLC plot! We can first define a color depending if the close price is lower than the open price (the open price is always on the left and the closure price always on the right):
\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/plot_OHCL_color_first_rule.jpg}
\end{figure}
Or with the following rules:
\begin{figure}[H]
\centering
\includegraphics{img/analysis/plot_OHCL_color_second_rule.jpg}
\end{figure}
And still many other charts... that we have already seen and other we will see throughout the pages of this book.

\paragraph{3D representations}\mbox{}\\\\
Of course, in the case of a trivalent function (three-dimensional), that is to say a parameter which depends on two other, the idea is the same as for 2D except that the number of quadrants doubles:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/quadrants_3d.eps}
\caption[Quadrants in a 3D orthogonal system]{Quadrants in a 3D orthogonal system (source: Wikipedia)}
\end{figure}

This 3D method of representation and analysis of a trivalent function was time consuming at the beginning of the 20th century but with the help of computers in the end of the 20th century this time consuming problem was almost solved...

In 3D functional analysis, we will deal with functions of two or three variables (usually  $x, y, z$, respectively). The graph of the arrow of coordinates $(x, y, z)=(x,y,f(x,y))$, lies in Euclidean space. Since Euclidean can be 3-dimensional (and more or less for sure!), we denote it by $\mathbb{R}^3$.

Euclidean space has three mutually perpendicular coordinate axes ($x$, $y$ and $z$), and three mutually perpendicular coordinate planes: the $xy$-plane, $yz$-plane and $xz$-plane:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/euclidian_planes.eps}
\caption{Mutually perpendicular planes in $\mathbb{R}^3$}
\end{figure}

The coordinate system shown above is known as a right-handed coordinate system, because it is possible, using the right hand, to point the index finger in the positive direction of the $x$-axis, the middle finger in the positive direction of the $y$-axis, and the thumb in the positive direction of the $z$-axis, as below:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/algebra/right_hand.eps}
\caption{Right hand system}
\end{figure}

What we are going to represent now further below (special example), purists mathematicians would notice it as follows (it's nice to have seen at least once this notation as you could meet it in other books):
	
and let us see what it gives with  Maple 4.00b:

\texttt{>restart:}\\
\texttt{>with(plots):}\\
\texttt{>f:=(x,y)->12*x/(1+x\string^ 2+y\string^ 2);}\\
\texttt{>xrange:=-10..10;yrange:=-5..5;}\\
\texttt{>plot3d(f,xrange,yrange);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_grid_function.eps}
\caption{Grid representation of a 3D function with Maple 4.00b}
\end{figure}

Let us improve the visual by adding a shading interpolation color with warm color to high positions and cold colors to low positions:

\texttt{>plot3d(f,xrange,yrange, style=patchnogrid, grid=[80,50], shading=ZHUE, axes=FRAME, tickmarks=[3,3,3], labels=[`x`,`y`,`f(x,y)`], labelfont=[TIMES,BOLD,12], title=`Graphique rempli`, titlefont=[TIMES,BOLD,12], scaling=unconstrained, orientation=[-107,68]);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{img/analysis/representation_shading_interp_function.eps}
\caption{Isolines representation of a 3D function with Maple 4.00b}
\end{figure}

Let us plot now the "\NewTerm{contour lines}\index{contour line}", also named "\NewTerm{isoline}\index{isoline}\label{isoline}" (or "\NewTerm{isoquant}\index{isoquant} in Econometry), that represents lines of the same height on the function surface\footnote{It is a cross-section of the three-dimensional graph of the function $f(x, y)$ parallel to the $x, y$ plane. In cartography, a contour line (often just named a "contour") joins points of equal elevation (height) above a given level, such as mean sea level. A contour map is a map illustrated with contour lines, for example a topographic map, which thus shows valleys and hills, and the steepness of slopes. The contour interval of a contour map is the difference in elevation between successive contour lines.} (see section of Differential Geometry page \pageref{isoline} for a rigorous definition):

\texttt{>plot3d(f,xrange,yrange,style=patchcontour);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_isoline.eps}
\caption{Shading interpolation representation of a 3D function with Maple 4.00b}
\end{figure}

It's not very nice so let us improve this a little bit:

\texttt{>plot3d(f,xrange,yrange,style=patchcontour,contours=[seq(-7+k/4,k=0..60)],\\
grid=[80,50],shading=ZHUE,axes=FRAME, tickmarks=[3,3,3],\\ scaling=unconstrained,orientation=[-107,68]);}

This will give:

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_nice_3d_function.eps}
\caption{Better representation of a 3D function with Maple 4.00b}
\end{figure}

With a small rotation to view from above:

\texttt{>plot3d(f,xrange,yrange, style=patchcontour, contours=[seq(-7+k/4,k=0..60)], grid=[80,50], shading=ZHUE, axes=FRAME, tickmarks=[3,3,3], scaling=unconstrained, orientation=[-90,0]);}

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/representation_nice_3d_function_above.eps}
\caption[]{Above representation of a 3D function with Maple 4.00b}
\end{figure}

And in section view (side view):

\texttt{>plot(f(x,2),x=xrange);}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/analysis/representation_nice_3d_function_section.eps}
\caption{Representation of a section of the pseudo-3D surface}
\end{figure}

Or with multiple sections views:

\texttt{>display([seq(plot(f(x,y),x=xrange),y=yrange)]);}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/analysis/representation_nice_3d_function_multiple_sections.eps}
\caption{Representation of multiple sections of the pseudo-3D surface}
\end{figure}

The reader can also animate the graph above with the following command:

\texttt{>display([seq(display([plot(f(x,k/5),x=xrange),}\\ \texttt{textplot([6,5,cat('y=',convert(evalf(k/5,2),string))],font=[TIMES,BOLD,16])])}\\
\texttt{,k=-25..25)],insequence=true, title='Animation',titlefont=[TIMES,BOLD,18]);}

That's all for typical and simple example of standard manipulations of an engineer hired in a company and using graphics (in practice it will instead use MATLAB™ instead of Maple but the reader can refer to the free companion book on MATLAB™ with a few hundreds of pages graphics).

\paragraph{2D Vector representations}\mbox{}\\\\
It is also frequently made use of graphic representations in the context of analytical geometry to simplify analysis or to prove theorems with the help of visual representations (do not abuse of this method!).

Thus, we can easily introduce the concept of "norm" (\SeeChapter{see section Vector Calculus page \pageref{vector norm}}) in a very easy way by plotting the distance between two points (in 2D or in 3D) and applying the Pythagorean theorem that will be assumed to be known (\SeeChapter{see section Euclidean Geometry page \pageref{pythagorean theorem}}).

The main idea of a planar vector representation in physics and engineering labs is that  a point $P_1$ of coordinates $(x_1,y_1)$ that has some physical properties (typically a velocity) will be after a given time at the point $P_2$ of coordinates $(x_2,y_2)$ supposed to be always in the same plane. In this way, the straight line between $P_1$ and $P_2$ is a visualization of the "intensity" of the velocity (and implicitly of the force). When doing that for many points we get a planar representation of a planar vector field (for more example see the companion book on MATLAB™):

\begin{figure}[H]
\centering
\includegraphics{img/analysis/vector_field.jpg}
\caption[]{Typical planar vector field with MATLAB™}
\end{figure}


Now let us represent three points $P_1,P_2,P_3$ on a plane graph in which has been defined a referential as presented below:

\begin{figure}[H]
\centering
\includegraphics{img/analysis/vector_plane.jpg}
\caption[]{Scenario of three points in a plane}
\end{figure}

We can consider the straight line $\overline{P_1P_2}$ as a vector but not translated at the origin of the referential (\SeeChapter{Vector Calculus}).

If $x_1\neq x_2$ and $y_1\neq y_2$ (as in the figure above), the points $P_1,P_2,P_3$ are the vertices of a perpendicular triangle. By applying the Pythagorean theorem (\SeeChapter{see section Euclidean Geometry page \pageref{pythagorean theorem}}) we can easily calculate the metric distance $d$ as:
	
	On the figure, we see that:
	
	Since $\forall x \in \mathbb{R} \; \vert x \vert ^2 =x^2$, we can write:
	
	If $x_1=y_1=0$, we end up with a relation named "\NewTerm{norm}", "\NewTerm{module}" or "\NewTerm{distance}\index{distance}" that we have already defined as part of our study of Vector Calculus when the origin of the vector is translated on the origin of the referential (see section of the corresponding name page \pageref{vector norm}).
	
	\begin{theorem}
	Obviously, if we consider two points $P_1(x_1,y_1),P_2(x_2,y_2)$, we can determine if a third point $P_3(x_3,y_3)$ is on the mediator (\SeeChapter{see section Euclidean Geometry page \pageref{mediator}}) of the first two and for this that it is obviously sufficient that (by definition of the mediator!):
	
	\end{theorem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We hesitated to put this proof in the section of Analytical Geometry but at then end we have decided that it was a nice example of showing how visual representation can help readers to better understand some subjects.
	\end{tcolorbox}	
	\begin{dem}
	As $(x_1,y_1),(x_2,y_2)$ are known, we can easily express an "\NewTerm{analytic expression}" property of the mediator that is that for each point on the mediator we have:
	
	where $a, b$ are therefore constants and wherein any point that satisfies this relation, which is in this case the equation of a straight line, lies on the mediator.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Furthermore, it is easy to see that the midpoint of the line segment that coincide with the mediator is given by:
	
	So we see that with a simple visual representation, we can achieve results that are sometimes (...) more obvious for students.
	
	Let us use this example to define some concepts on that we will come back further and do some reminders.
	
	\textbf{Definition (\#\mydef):} Any function of the form of a polynomial (\SeeChapter{see section Calculus page \pageref{polynomial}}) of degree $1$ with constant real coefficients:
	
	is the analytic expression of what we name a "\NewTerm{straight line}\index{straight line}\label{straight line}" "\NewTerm{linear equation}\index{linear equation}" of "\NewTerm{slope}\index{slope}" $a$ and "\NewTerm{intercept}\index{intercept}" $b$ (when $x=0$).
	
	Obviously, if:
	
	the line is horizontal if we graphically represent it since $y$ is constant for all $x$ and is equal to $b$. Conversely, if:
	
	the straight line will be vertical in the $x\text{O}y$ referential.
	
	\paragraph{Properties of visual representations}\mbox{}\\\\
	Depending on the type of graph we visualize (especially graphics planes) it is possible to extract some basic properties. Let us see the most important one to know for univariate functions:
	
	\begin{enumerate}
		\item[P1.] The graph of a function is "\NewTerm{symmetrical about the $y$-axis}\index{graph symmetric about the $y$-axis}" if the change in from $x$ to  $-x$ in the function does not change the value of $y$ such that:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_symetry_y.jpg}
		\caption{Example of symmetry through the $y$-axis of a function}
		\end{figure}
		
		\item[P2.] The graph of a function is "\NewTerm{symmetrical about the $x$-axis}\index{graph symmetric about the $x$-axis}" if the change from $y$ to $-y$ does not change the value of $x$ such that:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_symetry_x.jpg}
		\caption{Example of symmetry through the $x$-axis of a function}
		\end{figure}
		
		\item[P3.] The graph of a function is "\NewTerm{symmetrical about the origin $\text{O}$}\index{graph symmetrical about the origin}" if the simultaneous change of $y$ to $-y$ and from $x$ to $-x$ gives the following result (that is to say that the change in the sign of one variable change the sign of the other):
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_symetry_o.jpg}
		\caption{Example of symmetry through the origin $\text{O}$ of a function}
		\end{figure}
		
		\item[P4.] Given a function $y=f(x)$, if we add a constant $c^{te} \geq 0$ to this function as:
		
		then the function $f(x)$ is shifted (or "translated") vertically upwards of a distance $c^{te}$ as presented in the figure below:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_positive_translated.jpg}
		\caption{Example of a positive vertical translation of a function}
		\end{figure}
		And conversely if $c^{te} \geq 0$ but:
		
		then the function $f(x)$  is obviously translated vertically downwards:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_negative_translated.jpg}
		\caption{Example of a negative vertical translation of a function}
		\end{figure}
		We can also consider horizontal translations of functions. Specifically, if we have still $c^{te}$, then $y=f(x)$ is translated horizontally to the right if we write:
		
		which graphically is represented by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_negative_horizontal_translated.jpg}
		\caption{Example of negative horizontal translation of a function}
		\end{figure}
		and conversely, translated horizontally to the left, if we write:
		
			as shown in the graph below:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_positive_horizontal_translated.jpg}
		\caption{Example of positive horizontal translation of a function}
		\end{figure}
		To stretch or compress vertically a function, we simply multiply $y=f(x)$ by a constant $c^{te}>1$ and respectively $0\leq c^{te}<1$ as:
		
		and don't forget that if a function is linear then we have the special property $f(\lambda x)=\lambda f(x)$.
		This is graphically represented for the case $c^{te}>1$ by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_upscaled.jpg}
		\caption{Example of vertical stretch of a function}
		\end{figure}
		and when $0\leq c^{te}<1$ by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_downscaled.jpg}
		\caption{Example of vertical compression of a function}
		\end{figure}
		To stretch or compress a function horizontally, by the same way, we just need to multiply the variable $x$ by a constant by a constant $c^{te}>1$ and respectively $0\leq c^{te}<1$ as:
		
		This is graphically represented for the case $c^{te}>1$ by:
		\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/function_property_upscaled_horizontal.jpg}
		\caption{Example of horizontal stretch of a function}
		\end{figure}
		and when $0\leq c^{te}<1$ by:
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/function_property_downscaled_horizontal.jpg}
			\caption{Example of horizontal downscale of a function}
		\end{figure}
	\end{enumerate}
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Translate, stretch, compress a function or apply it a symmetry is transforming it. The plot resulting from these transformations is named the "\NewTerm{transformed}\index{transformed graph}" from the initial plot.
	\end{tcolorbox}	
	
	\textbf{Definitions (\#\mydef):} We say that a function $f$ is (we simplify the definition using an univariate function):
	\begin{itemize}
		\item[D1.] A function is a "\NewTerm{constant function}\index{constant function}" on an interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\neq x_2$, we have $f(x_1)=f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D2.] A function is an "\NewTerm{increasing function}\index{increasing function}" or an "\NewTerm{increasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\leq f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D3.] A function is an "\NewTerm{decreasing function}\index{decreasing function}" or an "\NewTerm{decreasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\geq f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{monotonic function}\index{monotonic function}" or "\NewTerm{monotonic function in the broadest sense}" on an interval $I$ if it is increasing or decreasing in this interval.
		\end{tcolorbox}
		
		\item[D4.] A function is a "\NewTerm{strictly increasing function}\index{strictly increasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)< f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D5.] A function is an "\NewTerm{strictly decreasing function}\index{strictly decreasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)> f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{strictly monotonic function}" on an interval $I$ if it is strictly increasing or decreasing in this interval.
		\end{tcolorbox}
	\end{itemize}
	
	\subsubsection{Analytical Representation}
	The analytical method of representation is by far the most used and consists of representing any function in an "\NewTerm{analytic expression}\index{analytic expression}" or "\NewTerm{closed form}\index{closed form}" which is a symbolic and abstract mathematical notation of all known mathematical operations that must be applied in a certain order to numbers and letters expressing constants or variables that we seek to analyse.
	
	Note that by "all known mathematical operations", we consider not only the mathematical operations seen in the chapter of Arithmetics (addition, subtraction, root extraction, etc.) but also all the operations that will be defined later in this book.
	
	If the functional dependence $y=f(x)$ is such that $f$ is an analytic expression, then we say that the "\NewTerm{function $y$ of $x$}" is "given analytically ". 

	Here are some examples of simple analytical expressions:
	
	When we have determined the equation of the mediator, we have obtained an analytical expression of the visual straight line that characterize it as a function of the type:
	
	which we recall, is the analytical expression of the equation of a straight line, also named "\NewTerm{linear equation}\index{linear equation}" or "\NewTerm{affine function}\index{affine function}", on a plane where two points are known $P_1(x_1,y_1),P_2(x_2,y_2)$, the slope is given by the ratio of vertical growth on the horizontal growth as:
	
	A friendly and trivial application is to prove analytically that two non-vertical lines are parallel if and only if they have the same slope. Thus, given two lines with the equations:
	
	The lines intersect at a point $(x, y)$ if and only if values of $y$ are equal for a certain $x$, that is to say:
	
	The last equation can be solved with respect to $x$ if and only if $a_2-a_2\neq 0$. We have therefore proved that the lines $y_1,y_2$ intersect if and only if $a_1\neq a_2$. Therefore, they do not intersect (are parallel) if and only if $a_1=a_2$.
	
	In a quite simple way by applying the Pythagorean theorem, it is not difficult (\SeeChapter{see section Analytical Geometry page \pageref{conics}}) to determine that the equation of a circle with center $C (h, k)$ has for equation (it is of use in mathematics not explain $y$ for the equation of the circle therefore the equation of the latter is much more visually aesthetic and speaking):
	
	In these examples the functions are expressed analytically by a single formula (equality between two analytical expressions) which defines at the same time the "natural domain of definition" of the functions.
	
	\textbf{Definition (\#\mydef):} The "\NewTerm{natural domain of definition}\index{natural domain of definition}\label{natural domain of definition}" of a function given by an analytical expression is the set of $x$ values for which the expression on the right-hand side has a definite value.
	
	For example the function:
	
	is defined for all values of $x$ except the value $x=1$ where we have a singularity (division by zero).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	There are an infinite number of functions and we can not expose them all here, however we will meet more than a thousand on this entire book and should amply suffice to get an idea of their study.
	\end{tcolorbox}
	
	And we have the famous following "\NewTerm{table of variations}\index{table of variations}\label{table of variations}" that is also considered as an analytical tool and also used by some teachers to study the basics of the derivative $f'$ of a function $f$ (\SeeChapter{see section Differential and Integral Calculus page \pageref{differential calculus}}). For example with the function $x^3-3x^2+2$ (already seen in the previous mentioned section):

	\begin{minipage}{\linewidth}\centering
    \begin{variations}
     x      & \mI &    & 0 &    & 2 &    & \pI  \\
     \filet
     f'     & \ga +    & 0    &  -  &  0   & \dr+      \\
     \filet
     \m{f}  & ~  & \c  & \h{~} & \d & ~    &  \c       \\
     \end{variations}
	\end{minipage} 	
	
	Whose corresponding plot is:
	\begin{figure}[H]
		\centering
		\includegraphics{img/algebra/variation_plot_example.jpg}
		\caption[]{Plot of  function $x^3-3x^2+2$}
	\end{figure}
	
	\pagebreak
	\subsection{Functions}\label{functions}
	In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output.
	
	Functions of various kinds are the central objects of investigation in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. Others are given by a picture, named the "graph" of the function. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. A function could be described implicitly, for example as the inverse to another function or as a solution of a differential equation.
	
	First remember the definitions already given earlier during our study of graphical representation of functions:
	
	\textbf{Definitions (\#\mydef):} We say that a function $f$ is (we simplify the definition using an univariate function):
	\begin{itemize}
		\item[D1.] A function is a "\NewTerm{constant function}\index{constant function}" on an interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\neq x_2$, we have $f(x_1)=f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D2.] A function is an "\NewTerm{increasing function}\index{increasing function}" or an "\NewTerm{increasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\leq f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D3.] A function is an "\NewTerm{decreasing function}\index{decreasing function}" or an "\NewTerm{decreasing function in the broadest sense}" on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)\geq f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{monotonic function}\index{monotonic function}" or "\NewTerm{monotonic function in the broadest sense}" on an interval $I$ if it is increasing or decreasing in this interval.
		\end{tcolorbox}
		
		\item[D4.] A function is a "\NewTerm{strictly increasing function}\index{strictly increasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)< f(x_2)$. What we denote in a condensed manner by:
		
		
		\item[D5.] A function is an "\NewTerm{strictly decreasing function}\index{strictly decreasing function}"  on the interval $I$ if for each pair $(x_1,x_2)$ of elements of $I$ such that $x_1\leq x_2$, we have $f(x_1)> f(x_2)$. What we denote in a condensed manner by:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		A function is a "\NewTerm{strictly monotonic function}\index{strictly monotonic function}" on an interval $I$ if it is strictly increasing or decreasing in this interval.
		\end{tcolorbox}
	\end{itemize}
	And let us add now complementary definitions:
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] We say that $y$ is a function of $x$ and we will write $y=f(x),y=\varphi(x)$, etc., if for every value of the variable $x$ belonging to a certain domain of definition (set) $D$, corresponds a value of the variable $y$ in another target domain of definition (set) $E$. What we denote in various ways (the third one being the most recommended):
		
		The variable $x$ is named "\NewTerm{independent variable}\index{independent variable}" or "\NewTerm{input variable}" or even "\NewTerm{exogenous variable}\index{exogenous variable}" and $y$ the "\NewTerm{dependent variable}\index{dependent variable}" or "\NewTerm{endogenous variable}\index{endogenous variable}".
		
		The dependence between the variables $x$ and $y$ is named a "\NewTerm{functional dependency}\index{functional dependency}". The letter $f$, which in the symbolic notation of functional dependence, indicates that we need to apply some operations to $x$ to obtain the corresponding $y$ value.
		
		Sometimes we write:
		
		rather than:
		
		In the latter case the letter $y$ expresses at the same time the value of the function and the symbol of operations applied to $x$.
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		As we saw it during our study in the section Set Theory, an application (or function) may be injective, surjective or bijective: 
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.75]{img/analysis/functions_type.jpg}
			\caption{Quick summary of applications/functions types}
		\end{figure}
		It is therefore necessary that the reader for whom these concepts are unknown goes in priority read these definitions.
		\end{tcolorbox}
		
		\item[D2.] The set of $x$ values (inputs) for which the value of the function $y$ is given by the function $f (x)$ is named the "\NewTerm{range of existence}\index{range of existence}" of the function or "\NewTerm{domain of definition}\index{domain of definition}\label{domain of definition}" of the function and denoted in this book by the letter $D$.
		
		The set of outputs of $f(x)$ is named the "\NewTerm{image}\index{image}" or sometimes the "\NewTerm{codomain}\index{codomain}" and denoted in this book by the letter $E$. When study of the point of view of the knowledge of the output values only, the set of $x$ is named the "\NewTerm{pre-image}".
		
		\item[D3.] A function $f(x)$ is named a "\NewTerm{periodic function}\index{periodic function}" if there is a constant $c^{te}$ such that the function's value does not change when we add (or subtract we) that constant to the independent variable such as:
		
		which corresponds to a translation along the $x$-axis. The smallest constant satisfying this condition is named the "\NewTerm{period}\index{period}" of the function. It is frequently denoted by the letter $T$ in physics.
		
		The most common periodic functions know by students and engineers are the trigonometric functions (see section of the corresponding name page \pageref{trigonometry}):
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.4]{img/analysis/periodic_function.jpg}
			\caption{Example of periodic function with period and amplitude}
		\end{figure}
		 \begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The sum of two periodic functions with different periods is not necessarily periodic and there is no general formula to get the period of a function that is the sum of $n$ other functions!
		\end{tcolorbox}
		
		\item[D4.] In differential calculus (\SeeChapter{see section Differential and Integral Calculus page \pageref{differential calculus}}), the expression:
		
		with $h\neq 0$ is of particular interest. We name it a "\NewTerm{growth quotient}\index{growth quotient}" (we discuss this in much more detail in our study of differential and integral calculus).
		
		\item[D5.] We use certain properties of functions for easy graphical representation and analysis or mathematical simplifications. In particular, a function $f (x)$ is named "\NewTerm{even function}\index{even function}\label{even function}" if:
		
		for all $x$ in its definition domain.
		
		That is to say as we already seen previously, it is symmetric relatively with the $y$-axis:
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/function_property_symetry_y.jpg}
			\caption{Example of even function}
		\end{figure}
		A function $f (x)$ is named "\NewTerm{odd function}\index{odd function}\label{odd function}" if:
		
		for all $x$ in its definition domain.
		
		That is to say as we already seen previously, it is symmetric relatively with the origin:
		\begin{figure}[H]
			\centering	\includegraphics{img/analysis/function_property_symetry_o.jpg}
			\caption{Example of odd function}
		\end{figure}
		So, to summarize, an even function is a function that is independent of the sign of the variable and an odd function change of sign when we change the sign of the variable (the spiral of Cornus in the section Civil Engineering is a good practical example of odd function). This concept will be very useful to us to simplify some very useful developments in physics (such as Fourier transforms of odd or even functions for example, or the calculation of certain integrals!).
		\begin{theorem}
		Remember that this type theorem linking a general concept to a particular case and its opposite is often found in mathematics. We will see such examples in tensor calculus with the symmetric and antisymmetric tensor (\SeeChapter{see section Tensor Calculus page \pageref{symmetric tensor} and page \pageref{antisymmetric tensor}}) or in quantum physics with the Hermitian or non-Hermitian operators (\SeeChapter{see section Wave Quantum Physics page \pageref{hermitian operator} and page \pageref{non-hermitian operator}}).
		\end{theorem}
		\begin{dem}
		Let us write:
		
		Then:
		
		If we sum then we get:
		
		and by subtracting:
		
		So there is really and odd and even decomposition of any function!!!
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		Finally, it is important to note that:
		\begin{itemize}
			\item The product of two even functions is an even function
			\item The product of two odd functions is an even function
			\item The product of an even and odd function is an odd function
		\end{itemize}
		Let us see a short proof of the last property because we will need it in the chapter on Geometry.
		\begin{dem}
		Let $g(x)$ be an even function and $h(x)$ an odd function such as:
		
		Therefore:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		\item[D6.] In general, if $f (x)$ and $g (x)$ are arbitrary functions, we use the terminology and notations given in the following table:
		\begin{table}[H]	
			\begin{center}
				\begin{tabular}{|c|c|}
				\hline
				  \rowcolor[gray]{0.75}Terminology&Value of the function\\
				  \hline
				  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
				  Sum $f+g$ & $(f+g)(x)=f(x)+g(x)$ \\\hline
				  Difference $f-g$ & $(f-g)(x)=f(x)-g(x)$ \\\hline
				  Product $f \cdot g$ & $(f \cdot g)(x)=f(x)g(x)$ \\\hline
				  Quotient $\displaystyle\frac{f}{g}$&$\left(\displaystyle\frac{f}{g}\right)(x)=\displaystyle\frac{f(x)}{g(x)}$ \\\hline
				\end{tabular}
			\end{center}
			\caption{Terminology about functions}
		\end{table}
		The definition domains of $f+g,f-g,f\cdot g$ are the intersection $I$ of the definition domain of $f (x)$ and g $(x)$, that is to say, the numbers which are common to both domains of definition. The domain of definition of $g/g$ is meanwhile the subset $I$ of all $x$ such that  $g(x)\neq 0$.
		
		\item[D7.] Let $y$ be a function of $f$ of $u$ such that $y=f(u)$ and $u$ a function $g$ of $x$ such that $u=g(x)$, then $y$ depends on $x$ and we have what we name a "\NewTerm{composite function}\index{composite function}\label{composite function}" that we denote:
		
		The last equality should be read "\NewTerm{$f$ round $g$}" and not confuse with the "round" symbol with the notation of the dot product that we have seen during our study of the section Vector Calculus page \pageref{dot product}.
		
		The domain of definition of the composite function is either identical to the entire domain of definition of the function $u=g(x)$ or the part of the domain in which the values of $u$ are such that the corresponding values $f (u)$ belong to the domain of definition of this function.
		
		Obviously the principle of composite function can be applied not only once, but an arbitrary number of times such that $y=f(g(h(t)))$ and so on...
		
		In computing science a function may compose with itself a given number of times $n$, such that $f(f(f(f(f...)))))=f^n$ that must not be confuse with the notation $f^2(x)$.
		
		If $u$ does not depend on another variable (or it is not itself a composite function), then we say that $f(x)$ is an "\NewTerm{elementary function}\index{elementary function}".

		Obviously there are an infinite number of elementary functions but most can be classified into families whose expression is similar to one of the following:
		
		\begin{itemize}
			\item "\NewTerm{Linear functions}\index{linear function}":
			
			The are simply functions representing straight lines of slope $a$ passing through the origin of the axis.
			
			\item "\NewTerm{Affine functions}\index{affine function}":
			
			The are simply functions representing straight lines of slope $a$ passing through the origin of the axis or not (linear function with a translation).
						
			\item "\NewTerm{Power functions}\index{power function}":
			
			where $m\in\mathbb{R}$. Functions involving roots are often named "\NewTerm{radical functions}\index{radical functions}".
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/power_function.jpg}
				\caption{Different plots of simple power functions}
			\end{figure}
			
			\item "\NewTerm{Absolute value functions}\index{absolute value function}\label{absolute value plot}" (see section Arithmetic Operators page \pageref{absolute value} for the definition and the study of the "absolute value"):
			
			For example the plots with Maple 4.00b that we get with the command:\\
			
			\texttt{>plot([(x),(cos(x)),(x\string^2-3),(x\string^3-4*x\string^2+2*x)],x=-6..6,y=-4..3,\\
			thickness=3);}	
			
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/pre_absolute_plot_functions.jpg}
			\end{figure}
			
			and taking the absolute value:\\
			
			\texttt{>plot([abs(x),abs(cos(x)),abs(x\string^2-3),abs(x\string^3-4*x\string^2+2*x)]\\
			,x=-6..6,y=-0.5..3,thickness=3);}
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/post_absolute_plot_functions.jpg}
			\end{figure}
		
		\item "\NewTerm{Exponential functions}\index{exponential function}":
			
			where the famous $e^x$ is only a special case and also $a\in\mathbb{R}$.
			
			When $a\geq 0$ we have typically:
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/exponential_functions.jpg}
				\caption{Different plots of simple exponential functions $(1^2,2^x,3^x,4^x)$ with Maple 4.00b}
			\end{figure}
			where $m$ is a positive number different from $1$ (otherwise it is simple a linear function):
			
			If $a<0,x\in\mathbb{R}$ the function is not defined. Indeed for $(-1)^(0.5)=\left\lbrace \mathrm{i},	-\mathrm{i}	\right\rbrace$ therefore it is an application from $f:\mathrm{R}\mapsto\mathbb{C}^2$ and as far as we know there is no nice way to represent it visually and anyway this is not a function in the traditional way.
			
			\item "\NewTerm{Logarithmic functions}\index{logarithmic function}":
				
			with $a\in\mathbb{R}^{+}$ and that by construction of the logarithm (see further below) are of the type $f:\mathbb{R}^{+}\mapsto \mathbb{R}$.
			We have typically:
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/logarithm_functions.jpg}
				\caption{Different plots of logarithm $\ln(x)=\ln_e(x)$ in green and $\log_{10}(x)$ in red with Maple 4.00b}
			\end{figure}
			
			\item "\NewTerm{Periodic/Trigonometric functions}\index{period function}\index{trigonometric function}":
			
			We already defined previously what is a periodic function. For the trigonometric functions the reader can see below a plot of the main one but for more details it is strongly recommended to read the section Trigonometry page \pageref{trigonometry}:
			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.9]{img/analysis/trigonometric_functions.jpg}
				\caption{Different plots of trigonometric functions with Maple 4.00b}
			\end{figure}
			
			\item "\NewTerm{Polynomial functions}\index{polynomial function}":
			
			
			where as we already know $a_0,a_1,...,a_n$ are constant numbers named "\NewTerm{coefficients}\index{coefficients}" and $n$ is a positive integer that we name "\NewTerm{degree of the polynomial}\index{degree of a polynomial}". Obviously this function is defined for all values of $x$, that is to say, it is define on an infinite interval.
			
			If follows that functions the power functions of the type $x^m$ and linear functions of the type $f(x)=x$ are a subclass of polynomial for $m\in \mathbb{N}$.
			
			We have already study more deeply polynomials in the section Calculus with their main properties but let us give us again the plot of some of them as recall: 
			\begin{figure}[H]
				\centering
				\includegraphics{img/algebra/polynomials.jpg}
				\caption[Some polynomials plotted with R.3.2.1]{Some polynomials plotted with R.3.2.1 (see our \texttt{R} companion book)}
			\end{figure}
			We will see and study in this book some famous polynomials as: Legendre polynomials (\SeeChapter{see section Quantum Chemistry page \pageref{legendre polynomial}}), Bernoulli polynomials (\SeeChapter{see section Sequences and Series page \pageref{bernoulli polynomials}}), Bernstein polynomials (\SeeChapter{see section Numerical Methods page \pageref{bernstein polynomial}}), Hermite polynomials (\SeeChapter{see section Functional Analysis page \pageref{hermite polynomial}}), ...
			
			\item "\NewTerm{Rational fractions}\index{rational fractions}" are polynomials divisions (\SeeChapter{see section Calculus page \pageref{polynomials division}}):
			
			\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
			Obviously two rational fractions are equal, if one is obtained from the other by multiplying the numerator and denominator by the same polynomial.
			\end{tcolorbox}
			The rational function:
			
			is not defined at $x^2=5 \Leftrightarrow x=\pm \sqrt{5}$. It is asymptotic (see further below) to $\frac{x}{2}$ as $x$ approaches infinity:
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/rational_function.jpg}
				\caption[Example of rational function]{Example of rational function $f(x) = \frac{x^3-2x}{2(x^2-5)}$}
			\end{figure}
			The rational function:
			
			 is defined for all real numbers, but not for all complex numbers, since if x were a square root of -1 (i.e. the imaginary unit or its negative), then formal evaluation would lead to division by zero!
			 
			 A constant function such as is a rational function since constants are polynomials. Every polynomial function $f(x) = P(x)$ is a rational function with $Q(x) = 1$. The power functions $f(x)=x^m$ are also rational functions when $m\in\mathbb{N}$.
			 
			 \item "\NewTerm{Algebraic functions}\index{algebraic function}" are defined by the fact that the function $f(x)$ is the result of addition, subtraction, multiplication, division, of variables put to an integer or non-integer power. Therefore most of the functions defined previously can be included in this definition: linear functions, affine function, power function, polynomial function, rational functions.
			 
			 \item A "\NewTerm{piecewise function}\index{piecewise function}" is a function defined by different formulas on different parts of its domain. The absolute value is a famous example of a piecewise-defined function because the formula changes with the sign of $x$:
			 
			 
			 \item A "\NewTerm{step function}\index{step function}" $f:[a,b]\in \mathbb{R}$ is defined if and only if there exists a subdivision $(a_i)_{0\leq i \leq n}$ of $[a, b]$ such that $a_0=a$ and $a_n=b$ and $(\lambda_0,...,\lambda_n)\in \mathbb{R}^n$ such as:
			 
			\begin{figure}[H]
				\centering
				\includegraphics{img/analysis/step_function.jpg}
				\caption{Example of a step function}
			\end{figure}
			Such functions can be found in signal processing and also in statistics for survival analysis.
		\end{itemize}
		
		However, there are a very large number of other elementary functions that will meet in the individual sections of this book. Examples include the "Bessel functions" (\SeeChapter{see section Sequences and Series page \pageref{bessel functions}}), the "Lipschitz functions" (\SeeChapter{see section Topology page \pageref{lipschitz functions}}), the "Dirac functions" (\SeeChapter{see section Differential and Integral Calculus page \pageref{dirac function}}), the "distribution functions" (\SeeChapter{see section Statistics page \pageref{distribution function}}), the "Euler gamma function" (\SeeChapter{see section Differential and Integral Calculus page \pageref{gamma euler function}}), etc. The reader will notice that the Dirac function also belongs to the family of distribution functions.
	\end{enumerate}
	
	Here is a quite good summary (non exhaustive but good!):
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/functions.jpg}
		\caption[Visual representation of various functions]{Visual representation of various functions (source: ?)}
	\end{figure}
	
	\subsubsection{Limits and Continuity of Functions}\label{limits}
	We will now consider ordered variables of a special type, which we define by the relation "the variable tends to a limit." In what will follow, the concept of limit of a variable will play a fundamental role, being intimately related to the basic notions of mathematical analysis, derivatives, integrals, etc.
	
	\textbf{Definition (\#\mydef):} The number $a$ is named the "\NewTerm{limit}\index{limit}" of variable magnitude $x$, if for any arbitrarily small positive number $\varepsilon$ we have:
	
	If the number $a$ is the limit of the variable $x$, we say that "\NewTerm{$x$ tends to the limit $a$}".

	We can also define the concept of limit from geometrical considerations (this can help to better understand ... but not always ...):
	
	The constant number $a$ is the limit of the variable $x$, if for any given neighbourhood, no matter how small, of center $a$ and of radius $\varepsilon$, we can find a value $x$ such that all the points corresponding to the following values of the variable belong to this neighbourhood (notions that we defined earlier). We represent geometrically this as:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/limit_geometric_representation.jpg}
		\caption{Geometric concept of limit in $\mathbb{R}^1$}
	\end{figure}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} It should be trivial that the limit of a constant value is equal to this constant, since the inequality $|x-c^{te}|=|c^{te}-c^{te}|=0<\varepsilon $ is always satisfied for an arbitrary $\varepsilon>0$.\\
	
	\textbf{R2.}  Not all variable have limits. For example $y=\sin(x)$ as this trigonometric function fluctuates between $[-1,+1]$ from $[-\infty,+\infty]$.
	\end{tcolorbox}
	
	\textbf{Definition (\#\mydef):} A variable $x$ tends to infinity if for any positive chosen $M$, we indicate one value of $x$ from which all successive values of the variable $x$ (values in the neighbourhood of the previous chosen value) satisfy the inequality $|x|>M$. Formally:
	
	\begin{itemize}
		\item A variable $x$ "\NewTerm{tends to $+\infty$}" if for any positive chosen $M>0$, we indicate one value of $x$ from which all the successive values of the variables $x$ satisfies the inequality $M<x$.
	
		It is typically the type of consideration that we have for divergent sequences (divergent to infinity) where for a given term of value $M$ of the sequence all the other terms are greater ant $M$.
		
		
		\item A variable $x$ "\NewTerm{tends to $-\infty$}" if for any negative chosen $M<0$, we indicate one value of $x$ from which all the successive values of the variables $x$ satisfies the inequality $x<M$.
		
	\end{itemize}
	\textbf{Definition (\#\mydef):} Given $y=f(x)$ a function defined in a neighbourhood of $a$ or on some point of this neighbourhood. The function $y=f(x)$ tends to the limit $b$ (that is to say $y\rightarrow b$) when $x$ tends to $a$ (that is to say $x a$) if for any positive number $\varepsilon$ as small as possible, we can indicate  positive number $\delta$ such that all $x$ different from $a$ satisfying the inequality $|x-a|<\delta$ also satisfy $|f(x)-b|<\varepsilon$. Formally a function has a limit $b$ on $a$ when in a domain $E$ if:
	
	The inequality $|x-a|<\delta$ gives the possibility to have the distance from which we come with our $x$ without taking care of the direction (left or right) as we take for measurement of distance the absolute values. Indeed on a system of axis representing ordinates values, we can, for a given value, coming from the left or from the right (if necessary you can imagine a bus coming to a bus stop that can from the left or from the right only since the absolute distance from it to the bus stop is less than or equal to $\delta$).
	
	If $b$ is the limit of the function $f (x)$ when $x\rightarrow a$ we then write in this book in any case:
	
	Obviously the above definition is available when $a=\pm \infty$ or/and $b=\pm \infty$!	
	
	To define the direction from which we come from by applying the limit, we use a special notation (recall that this will give us the information of which side of the road comes our bus from...). Thus, if $f (x)$ tends towards the limit $b_1$ when $x$ approaches a number $a$ by taking only values smaller than $a$, then we write:
	
	(notice the small $-$ subscript) and we name $b_1$ the "\NewTerm{left limit}\index{left limit}" of the function $f (x)$ at point $a$ (because remember that the horizontal axis goes from left to right from $-\infty$ to $+\infty$, so small values compared to a given value, are on the left).
	
	If $x$ takes values greater than $a$, then we will write:
	
	(notice the small $+$ subscript) and we name $b_2$ the "\NewTerm{right limit}\index{right limit}" of the function $f (x)$ at point $a$.
	
	In the figure below we have for example:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/limits.jpg}
		\caption{Left and Right limit examples}
	\end{figure}
	It is not always easy (or even possible) to calculate limits of some functions. Let us see some typical examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us prove that:
		
	is true. For this purpose we have to prove that for any small $\varepsilon$ the inequality:
	
	will be satisfied as soon as $|x|>M$ where $M$ is defined by the choice of $\varepsilon$. The previous inequality is obviously equal to:
	
	which is satisfy if we have $x$:
	
	We admit that the example and the method can be discussed.... But in fact it is only an application of the Hospital rule (ratio of the derivatives) already proved in the section of Differential and Integral Calculus. The reader must also know that we will see also other techniques to determine limits further below with better examples.\\
	
	E2. Now using Taylor series and change of variables consider we want to calculate:
	
	The method is quite to intuitive. Indeed, first we do a change of variable:
	
	Now consider the Taylor series about $x=0$ for the function $f(x)=\sqrt{1+ax}$. We have:
	
	Which gives:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	as a Taylor expansion about $x=0$. Applying this to our limit we see that:
	
	E3. We want to calculate the limit of:
	
	How can we deal with something like this? An idea is the to remember that it also implicitly means:
	
	Hence:
	
	And to answer what is the value of $\theta$, we refer to the plot of the function $\tan(\theta)$ and we see then that the first corresponding value is $\pi/2$, therefore:
	
	\end{tcolorbox}
	
	The signification of the symbols $x\rightarrow -\infty$ and $x\rightarrow +\infty$ makes obvious the signification of the expressions:
	
	and:
	
	that we denote formally by:	
	
	We have defined the case where the function $f (x)$ tends to a certain limit $b$ when $x\rightarrow a_{+,-}$ or $x\rightarrow \pm \infty$. Now let us consider the case where the function tend to infinity when the variable $x$ change in a certain way.
	
	We then have typically and obviously:
	
	Or when we need to indicate the direction:
	
	If the function $f(x) \rightarrow +\infty$ when $a \rightarrow +\infty$ then we write:
	
	And as we have four possibilities for the sign, we write:
	
	that is to say the four following possibilities:
	
	And once again don't forget as we already mentioned before that some function such as for example $f(x)=\sin(x)$ don't have any finite limit when $x\rightarrow \pm \infty$. Then we say that the function is just "bounded" (\SeeChapter{see section Set Theory page \pageref{closed bounded interval}}).
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/limite_delucq.jpg}
	\end{figure}
	Now that we've roughly an overview of the concept of limit, we will give an extremely important definition that has a very important place of many areas of high-level mathematics, theoretical physics and computing science (numerical methods).
	
	\textbf{Definition (\#\mydef):} Given a function $f(x)$ and one of its subdomain (or whole one) $E$ (most of time $E \subseteq \mathbb{R}$ and $x_0\in E$, we say that we have a "\NewTerm{continuous function}\index{continuous function}" on $x_0$ if and only if:
	
	That is to say more formally (you have to be able to read the fact that we are going close in an infinitely small way of a limit an this allows the continuity):
	
	In other words: a function is continuous if for every point $x_0$ in the domain $E$, we can make the images of that point ($f(x_0)$) and another point ($f(x)$) arbitrarily close (of a distance $\varepsilon$) if we move the other point ($x$) close enough (distance $\delta$) to our given point.
	
	The latter relation will be generalized a little bit in the section of Topology and completed with the concept of... "uniform continuity"!
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1. }$f$ is "\NewTerm{continuous on the left}\index{continuous on the left}" or respectively "\NewTerm{continuous on the right}\index{continuous on the right}", if we add to the definition above the condition $x>x_0$, respectively $x<x_0$.\\
	
	\textbf{R2.} A continuous function with a continuous inverse function is named a "\NewTerm{homeomorphism}\index{homeomorphism}".\\
	
	\textbf{R3.} Instead of saying when necessary that a function is not continuous on $x_0$ or on a given domain, some practitioners prefer to say that the function has an "\NewTerm{oscillation}\index{oscillation}".
	\end{tcolorbox}	
	We have the following trivial corollaries:
	\begin{enumerate}
		\item[C1.] $f(x)$ is continuous on $x_0$ if and only if $f(x)$ is continuous on the left right and on the right left.
		
		\item[C2.] $f(x)$ is continuous on $E$ if and only if $f(x)$ is continuous on any point of $E$.
	\end{enumerate}
	
	\paragraph{Limit laws}\mbox{}\\\\
	We now take a look at the "\NewTerm{limit laws}\index{limit laws}", the individual properties limits in the univariate case. The proofs will be omitted as it is quite intuitive but any reader can request us the proof of one of them if needed!
	
	Let $f(x)$ and $g(x)$ be defined for all $x\neq a$ over some open interval containing $a$. Assume that $L$ and $M$ are real numbers such that:
	
	Let $c^{te}$ be a constant. Then, each of the following statements holds:		
	\begin{itemize}
		\item The sum law for limits gives:
		
		
		\item The difference law for limits gives:
		
		
		\item Constant multiple law for limits:
		
		
		\item Product law for limits:
		
		
		\item Quotient law for limits:
		
		for $M\neq 0$.
		
		\item Power law for limits:
		
		for every positive integer $n$.
		
		\item Root law for limits:
		
		for all $L$ if $n$ is odd and for $L\geq 0$ if $n$ is even.
	\end{itemize}
	
	\subsubsection{Asymptotes}
	The term "\NewTerm{asymptote}\index{asymptote}" is used in mathematics to precise possible properties of an infinite branch of curve which growth tends to an infinitesimal value.
	
	In analytic geometry, an asymptote of a curve is simply said to be a line such that the distance between the curve and the line approaches zero as they tend to infinity. In some contexts, such as algebraic geometry, an asymptote is defined as a line which is tangent to a curve at infinity.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The word asymptote is derived from the Greek and means "not falling together".
	\end{tcolorbox}	
	
	\textbf{Definitions (\#\mydef):}
	
	\begin{enumerate}
		\item[D1.] When the limit of a function $f(x)$ tends to a constant 
$c^{te}$ when $x \rightarrow \pm \infty$, then the graphical representation of this function leads us to draw a horizontal line that we name "\NewTerm{horizontal asymptote}\index{horizontal asymptote}" which equation is satisfies:
		
		
		\item[D2.] When the limit of a function $f(x)$ tends to  
$\pm \infty$ when $x \rightarrow a_{+,-}$, then the graphical representation of this function leads us to draw a vertical line that we name "\NewTerm{vertical asymptote}\index{vertical asymptote}" which equation is satisfies:
		
		Vertical asymptotes is the typical symptom of a division by zero in a fraction and has a very important place in physics. The syndrome is also named a "\NewTerm{singularity}\index{singularity}".
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The graph of the function:
		
		 has the straight line of $x=1$ and $y=0$ as horizontal asymptote:
		 \begin{figure}[H]
			\centering
			\includegraphics{img/analysis/asymptote_vertical_horizontal_example.jpg}
			\caption{Graphical representation of a  horizontal and vertical asymptote}
		\end{figure}
		\end{tcolorbox}
		
		\item[D3.] The straight line of equation is an "\NewTerm{oblique asymptote}\index{oblique asymptote}" of a curve of the function $f (x)$ if:
		
		the values of $a$ and $b$ can be easily found using the following relations:
		
		
		\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
		\bcbombe Caution! A curve may have two distinct oblique asymptotes in $+\infty$ and $-\infty$.
		\end{tcolorbox}
		
		To find a possible oblique asymptote, one must already be certain that the function $f(x)$ admits an infinite limit in $+\infty$ or $-\infty$ then only we look for the limits at $-\infty$ and $+\infty$ of  $f (x) / x$ and $f(x)-ax$.
		
		Three typical cases can be considered for oblique asymptotes:
		\begin{enumerate}
			\item The representative curve of $f(x)$ has for asymptotical direction the affine equation $y=ax$:
			
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The graph of the function:
			
			 has the straight line of $y=x$ as oblique asymptote:
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_oblique_affine_example.jpg}
				\caption{Graphical representation of an oblique affine asymptote}
			\end{figure}
			\end{tcolorbox}
			
			\item The representative curve of $f(x)$ has an infinite branch (this branch has not close form asymptote) and the only one thing we can say is that $x$-axis is the direction of this asymptote. Such an asymptote exists when:
			
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The functions $f(x)=\sqrt{x}$ (in red) or $\ln(x)$ (in green) have a limit $f(x)/x$ equal to $0$ and both have a "parabolic branch"  of direction following the $x$-axis:
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_parabolic_branche_example_x.jpg}
				\caption[]{Graphical representation of an parabolic branch example following $x$-axis}
			\end{figure}
			\end{tcolorbox}
			
			\item The representative curve of $f(x)$ has an infinite branch (this branch has not close form asymptote) and the only one thing we can say is that $y$-axis is the direction of this asymptote (we then also speak of "parabolic branch"):
			
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The function $f(x)=x^2$ has an infinite $f(x)/x$ limit and therefore has a parabolic branch of direction following the $y$-axis.
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_parabolic_branche_example_y.jpg}
				\caption[]{Graphical representation of a parabolic branch example following $y$-axis}
			\end{figure}
			\end{tcolorbox}
			
			\item A function $f(x)$ is say to have a "\NewTerm{curvilinear asymptote}\index{curvilinear asymptote}" if it satisfies:
			
			for $n>1 $where for recall $P_n(x)$ is a polynomial of degree $n$.
			\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
			\textbf{{\Large \ding{45}}Example:}\\\\
			The function :
			
			has a curvilinear asymptote that is:
			
			Indeed:
			
			 \begin{figure}[H]
				\centering
				\includegraphics{img/analysis/asymptote_curvilinear_example.jpg}
				\caption[]{Graphical representation of curvilinear asymptote}
			\end{figure}
			\end{tcolorbox}
		\end{enumerate}
	\end{enumerate}
	There are many techniques for finding limits that apply in various conditions. It's important to know all these techniques, but it's also important to know when to apply which technique. Some basic techniques who doesn't involve derivation are:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/finding_limits.jpg}
		\caption[Basic techniques for finding limits]{Basic techniques for finding limits (source: Khan Academy)}
	\end{figure}
	
	
	\pagebreak
	\subsubsection{Concavity/Convexity of a function}
	We will define now a property which at first sight may seem of no interest as it is so trivial but which we shall find in the section of Statistics for the demonstration of an important relation named "Jensen inequality" and which is of major importance Finance and Insurance for the valuation of options and premiums (\SeeChapter{see section Economy page \pageref{finance convex function}}) and especially for the application of the Jensen's inequality (\SeeChapter{see section Statistics page \pageref{jensen inequality}}).

	Consider the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/concavity_convexity.jpg}
		\caption[]{Graphical representation of curvilinear asymptote}
	\end{figure}
	\textbf{Definition (\#\mydef):} In mathematics, a real function of a real variable is say to a "\NewTerm{convex function}\index{convex function}\label{convex function}" if, viewed from below, its graph is convex (in bump); we mean that if $A$ and $B$ are two points of the graph of the function, the segment $[AB]$ is entirely situated above the graph. It is the same to say that the "\NewTerm{epigaph}" (the set of points above the graph) is a concave set. Conversely, a function whose graph, as seen from below, is seen as a cave, is say to be a "\NewTerm{concave function}\index{concave function}\label{concave function}". It is the same to say that the "\NewTerm{hypograph}" (the set of points below) is a convex set.
	
	By specifying by the values of the function what are the points $A$ and $B$ above, we get often an equivalent definition of the convexity of a function: a function defined on a real interval $I$ is convex when, for any $x_1$ and $x_2$ of $I$ and all $t$ in $[0,1]$ we have:
	
	When the inequality is strict, then we obviously speak of a "\NewTerm{strictly convex function}".
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	Without proof, just by looking to the above chart, we will assume quite obvious that convexity implies $f''(x)\geq 0$ for all $x$. Just as before, strict convexity occurs when
the inequality is strict.
	\end{tcolorbox}

	By extension (common sense from my point of view), a function $f$ is concave if $-f$ is convex (which is trivial with the pay-off function - see section Economy page \pageref{finance convex function} - profile of options from seller or buyer point of view ).
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider $f(x)=x^{2}$ . The first derivative of $f(x)$ is given by $\frac{\mathrm{d}}{d\mathrm{d} x} f=2 x$ and its second derivative by $\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}} f=2$. Since this is  always strictly greater than $0,$ we have proven that $f(x)=x^{2}$ is strictly convex.\\
	
	E2. $f(x)=\log (x)$ . The first derivative is $\frac{\mathrm{d}}{\mathrm{d} x} f=\frac{1}{x}$ and its second derivative is given by $\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}} f=-\frac{1}{x^{2}}$. Since this is negative for all $x>0$, we have proven that $\log(x)$ is a concave function over $\mathbb{R}_{+}$ .
	\end{tcolorbox}
	
	\subsubsection{Euler theorem for homogeneous functions}
	We have to introduce now a definition and a theorem that will be very important for quite advanced concepts in our study of Physics.
		
	\textbf{Definition (\#\mydef):} A "\NewTerm{homogeneous function}\index{homogeneous function}\label{homogeneous function}" is one with multiplicative scaling behaviour: if all its arguments are multiplied by a factor, then its value is multiplied by some power of this factor. For example, a homogeneous real-valued function of two variables $x$ and $y$ is a real-valued function that satisfies the condition $f(t x,t y)=t ^{k}f(x,y)$ for some constant $k$ and all real numbers $t\in \mathbb{R}^*$. The constant $k$ is named  the "\NewTerm{degree of homogeneity}".
	
	More generally, if $f:\mathbb{R}^n\mapsto \mathbb{R}$ is a function between two vector spaces over a field $F$, and $k$ is an integer, then $f$ is said to be homogeneous of degree $k$ if:
	
	or more commonly written:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The function $f(x,y)=x^{2}+y^{2}$ is homogeneous of degree $2$. Indeed:
	
	\end{tcolorbox}
	
	Now let us suppose $f:\mathbb{R}^n\mapsto \mathbb{R}$ is continuously differentiable on $\mathbb{R}$. We know that a function is homogeneous of degree $k$ if:
	
	Differentiating both sides with respect to $t$, we get:
	
	by the total exact differential chain rule of $f$. 
	
	So the "\NewTerm{Euler theorem for homogeneous functions}\index{Euler theorem for homogeneous functions}\label{Euler theorem for homogeneous functions}" can be summarized as following:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Remember that we have proved in the section of Differential and Integral Calculus (see page \pageref{total exact differential}) that:
	
	Therefore:
	
	\end{tcolorbox}
	
	Then if we choose to set with the special case $k=1$ (it's the case that will interest us the most in Physics) then the above becomes:
	
	or explicitly for $n=2$:
	
	if $f(tx)$ is homogeneous of degree $1$.
	
	This is an important result we will need in Lagrangian Mechanics that will be useful for the study of the Lagrangian of a free particle in Special Relativity and in Quantum Cosmology.
	
	
			
	\pagebreak
	\subsection{Logarithms}\label{logarithms}
	We hesitated to put the definition of logarithms in the section Calculus. After a moment of reflection, we decided it was better to put it in this section because to understand it well, we must be aware of the concept of limits, of definition domain and of the power function. We hope that our choice will suit you best.
	
	Given the power (bijective) function of any base where $a \in \mathbb{R}_{+}^{*}/1$ (we exclude $1$ otherwise it is not bijective) and denoted for recall by:
	
	for which it corresponds to each real number $x$, exactly one positive number $a^x$ (the image set of the function is in $\mathbb{R}$) such as the powers calculations rules are applicable (\SeeChapter{see section Calculus page \pageref{power rules calculations}}).
	
	We know that for such a function that if $a>1$, then $f (x)$ is an increasing and positive (monotone) in $\mathbb{R}$, and if $0<a<1$, then $f(x)$ is positive and decreasing (monotone) in $\mathbb{R}$.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} If $a>1$, when $x$ decreases to negative values, the graph of $f (x)$ approaches the $x$-axis. Thus, the $x$ axis is a horizontal asymptote. When $x$ increases in positive values, the graph rises quickly. This type of change is characteristic of the "\NewTerm{law of exponential of growth}\index{law of exponential of growth}" and $f(x)$ is sometimes named "\NewTerm{growing function}\index{growing function}"... If $0<a<1$, when $x$ increases, the graph tends asymptotically to the $x$-axis. This type of variation is known as an "\NewTerm{exponential decay}\index{exponential decay}".\\
	
	\textbf{R2.} By studying $a^x$, we exclude the case where $a\leq 0$ and $a=1$. Notice that if $a<0$, then $a^x$ is not a real number for many values of $x$ (we recall that the whole image set is forced to $\mathbb{R}$ in our previous definition). If $a=0$, the $a^0=0$ is not defined. Finally, if $a=1$, then $a^x=1$ for all $x$ and the graph of $f(x)$ is a horizontal line.
	\end{tcolorbox}
	As the power function $f (x)$ is bijective then there exists an inverse function $f^{-1}(x)$ and is named "\NewTerm{logarithm function}\index{logarithm function}" of base $a$ and is denoted by:
	
	and therefore:
	
	if and only if $y=a^x$.
	
	More generally it is defined by:
	
	
	Considering $\log_a(x)$ as an exponent, we have the following properties:	
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{|c|c|}
			  \hline
			  \rowcolor[gray]{0.75}Properties & Justification \\ \hline
			  $\log_a1=0$ & $a^0=1$ \\ \hline
			  $\log_aa=a$ & $a^1=a$ \\ \hline
			  $\log_aa^x$ & $a^x=a^x$ \\ \hline
			  $a^{\log_a(x)}=x$ & $a^{\log_a(x)}=a^y=x$ \\
			  \hline
		\end{tabular}
		\end{center}
		\caption{Properties of the logarithm in base $a$}
	\end{table}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The word "logarithm" means "number of logos", "logos" meaning "reason" or "ratio".\\
	
	\textbf{R2.} The logarithm and power functions are defined by their bases (the number $a$). When using a power of $10$ as a base ($10, 100, 1000, ...$) then we speak of "\NewTerm{common system}\index{common system}" because they have for $\log$ successive integers.\\
	
	\textbf{R3.} The integer part of the logarithm is named the "\NewTerm{characteristic}"\index{characteristic of a logarithm}.
	\end{tcolorbox}
	There are two types of logarithms that we find almost exclusively in mathematics and physics: the logarithm of base $10$, logarithm of base $e$ (the latter often named "\NewTerm{natural logarithm}\index{natural logarithm}") and logarithm of base $2$ for information theory.
	
	First the on in base $10$ (the most used on graphical representations):
	
	abusively noted:
	
	and the base (Eulerian) $e$:
	
	historically noted:
	
	the "$n$" meaning "Napierian".
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Historically, it is John Napier (1550-1617) whose name was Latinized "Napier" that we own the study of logarithms and the name of "natural logarithms" which aimed to facilitate greatly the time for manual calculations.
	\end{tcolorbox}
	In English for the logarithm function in base-$10$ logarithmic we need to calculate:
	
	ask the following question: at what power $n\in \mathbb{R}$ should we raise $10$ to get $x$?
	
	Formally, this consist to solve the equation:
	
	or written in another way:
	
	with $x$ being known and therefore in base $10$:
	
	The logarithm in base $10$ is used a lot in graphical representations in the scientific perspective when we look at amplitudes variations. For example with Maple 4.00b  we have for two sine function  having respectively for their respective mean the same amplitude variation of $50\%$ visible below that do not highlights necessarily this fact trivially:
	
	\texttt{>plot({10+0.5*10*sin(x),100+100*0.5*sin(x)},x=1..10);}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/two_sinus_for_comparison_without_logarithm_scale.jpg}
		\caption[]{Plot with Maple 4.00b with two sine functions having same amplitude change compared to their average}
	\end{figure}
	While in logarithmic scale, this gives:
	
	\texttt{>with(plots):\\
	>logplot({10+0.5*10*sin(x),100+100*0.5*sin(x)},x=1..10);}
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/two_sinus_for_comparison_with_logarithm_scale.jpg}
		\caption[]{Same plot with Maple 4.00b but with the $y$-axis in logarithm (base $10$) scale}
	\end{figure}
	For the logarithmic function in Eulerian base $e$ it is necessary to calculate:
	
	to ask ourselves the following question: at what power $n\in \mathbb{R}$ we must raise the number $e$ to get $x$?
	
	Formally this consists to solve the equation:
	
	with $x$ being known and therefore:
	
	Technically, we say that the exponential function (see below for details):
	
	is the inverse bijection of the $\ln (x)$ function.
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/bijection_ln_x_exp_x.jpg}
		\caption{Graphical representation of the correspondence between the natural logarithm and the exponential}
	\end{figure}
	But what is that "Eulerian" number also named "\NewTerm{Euler number}\index{Euler number}\label{Euler number}"? Why do we find so often in physics and mathematics? Let us first determine the origin of its value:
	
	with $\alpha \in \mathbb{N}$ and when $\alpha \rightarrow +\infty$.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The second term of the equality is typically the type of expression that we find in compound interest in finance (\SeeChapter{see section Economy page \pageref{compound interest}}) or in any other type of identical increase factor. And what interests us in this case is when this type of increase tends to infinity.
	\end{tcolorbox}
	The interest we have to pose the problem as in this way is that if we do tend $\alpha \rightarrow +\infty$ the function written above tends to $e$ and this function has the special property of being calculable more or less easily for historical reasons using Newton's binomial.
	
	So according to the development of the Newton binomial (\SeeChapter{see section Calculus page \pageref{binomial coefficient development}}) we can write:
	
	This development is similar to the Taylor expansion (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) of some given functions for particular cases of development values (hence the reason why we find this eulerian number in many places that we will later).
	
	By performing some algebraic transformations that should now be obvious to the reader, we find:
	
	We see in this last equality that the function $\left(1+\frac{1}{\alpha}\right)^\alpha$ is increasing when $\alpha$ increases. Indeed, when we move from $\alpha$ to the value $\alpha+1$ each term of this sum increases:
	
	Let us prove now that the variable $\left(1+\frac{1}{\alpha}\right)^\alpha$ is bounded. By seeing that:
	
	So we get by analogy with the extended expression of Newton binomial determined just previously the following order relation:
	
	On the other hand:
	
	We then can write the inequality:
	
	The underlined terms constitute a geometric sequence of reason $q=1/2$ (\SeeChapter{see section Sequences and Series page \pageref{geometric sequence}}) and whose first term is $1$. If follows using the result obtained in the section of Sequences ans Series, that we can write:
	
	Therefore, we have:
	
	We have therefore proved that the function $\left(1+\frac{1}{\alpha}\right)^\alpha$ is bounded.
	
	The limit:
	
	tends to this limited value that is the number $e$ whose value is:
	
	The prior previous relation is also know in the following form after an obvious change of variable:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	As we have proved it in the section Numbers, this number is irrational.
	\end{tcolorbox}
	We can then define the "\NewTerm{natural exponential function}\index{natural exponential function}\label{natural exponential function}" (reciprocal of the natural logarithm function) by:
	
	also sometimes denoted by:
	
	The number $e$ and the function that determines it are very useful. We find them in all areas of mathematics and physics and thus in almost all the chapters of this book.
	
	As we have proved it in the section of Differential and Integral Calculus the functions $e^x$ has for remarkable property that its derivative is equal to itself:
	
	and this is used a lot for the resolution of differential equations in physics and finance.
	
	Logarithms have several properties. Here are the most important one in our point of view (we are referring to a given base $X$) and that are very useful in physics, electronics, chemistry and so on...
	
	Let us begin. First:
	
	If we put $X^m=a$ and $X^n=b$ we get:
	
	If we have the special case when $a=b$ then:
	
	Now let us try to express:
	
	in another way. For this we put first:
	
	which leads us to the development:
	
	Now let us try to express:
	
	with $n\in \mathbb{N}^{*}$ in another way. For this we put first:
	
	which leads us to the development:
	
	There is  also another relation used a lot of time in physics in respect to the change of logarithm basis. The first relation is trivial and follows from the algebraic properties of logarithms:
	
	the second one:
	
	is a bit less trivial and requires perhaps a proof (we used it for our study of continued fractions in the section Number Theory).
	\begin{dem}
	We first use the equivalent equations (of the first relation above):
	
	and we proceed as follows:
	
	What finally brings us to:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\pagebreak
	\subsection{Convolutions}\label{convolution}
	A convolution is a mathematical operation on two functions to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the point-wise multiplication of the two functions as a function of the amount that one of the original functions is translated.
	
	There are different type of convolutions and as always, we will focus in this book only on the one that are actually used in other sections of this books.
	
	\subsubsection{Continuous and Discrete Linear Convolution Product}
	\textbf{Definition (\#\mydef):} The "\NewTerm{continuous convolution}\index{continuous convolution}\label{continuous convolution}" of two continuous signals $x(t)$ and $h(t)$ is defined as:
	
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The convolution is also sometimes denoted with different symbols:
	
	or:
	
	\end{tcolorbox}
	We will now prove some properties of the convolutions on focusing only on those use in the other sections of this book!
	\begin{itemize}
		\item[P1.] Convolution is commutative:
		
		\begin{dem}
		By making the change of variable $\lambda=t-\tau$, in one form of the definition of convolution:
		
		it becomes:
		
		proving that convolution is commutative.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P2.] Convolution is associative:
		
		\begin{dem}
		The proof is easier to understand if we consider a limited integral (but you can change the bounds to the infinity one and you will fall back on the general result):
		
		 The proof may not be obvious for many readers. So we recommend to see the equivalent proof for the discrete version further below.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P3.] Convolution is distributive:
		
		\begin{dem}
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P4.] Relation with differentiation:
		
		\begin{dem}
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{itemize}
	We will assume as obvious that these properties also apply to the discrete convolution that we will introduce now!
	
	Typically, $y(t)$ is the output of a system characterized by its impulse response function $h(t)$ with input $x(t)$.
	
	\textbf{Definition (\#\mydef):} The "\NewTerm{discrete convolution}\index{discrete convolution}" of two discrete signals $x[n]$ and $h[n]$ is defined as:
	
	If $h[m]$ is finite, e.g.:
	
	the convolution becomes
	
	If the system in question were a causal system in time domain:
	
	the above would become:
	
	This is also written:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider:
	
	We get visually (it's not obvious!):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/analysis/discrete_convolution.jpg}
		\caption{Discrete convolution example}
	\end{figure}
	The details steps are given by first considering that:
	
	and the fact that:
	And we have for recall:
	
	And in tabular form:
	\begin{table}[H]
	\centering
		\begin{tabular}{lccccc}
		$n$: & $0$ & $1$ & $2$ & $3$ & $4$ \\
		$m=0$ & ${\color{blue}{2}}\cdot {\color{red}{3}}=6$ & ${\color{blue}{1}}\cdot {\color{red}{3}}=3$ & ${\color{blue}{3}}\cdot {\color{red}{3}}=9$ & $0$ & $0$ \\
		$m=1$ & $0$ & ${\color{blue}{2}}\cdot {\color{red}{2}}=4$ & ${\color{blue}{1}}\cdot {\color{red}{2}}=2$ & ${\color{blue}{3}}\cdot {\color{red}{2}}=6$ & $0$ \\
		$m=2$ & $0$ & $0$ & ${\color{blue}{2}}\cdot {\color{red}{1}}=2$ & ${\color{blue}{1}}\cdot {\color{red}{1}}=1$ & ${\color{blue}{3}}\cdot {\color{red}{1}}=3$ \\ \hhline{|=|=|=|=|=|=|}
		$c[n]$ & $6$ & $7$ & $13$ & $7$ & $3$
		\end{tabular}
	\end{table}
	The $c[n>4]$ being all equal to zero.\\
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	
	The final result will then be written:
	
	Such a calculations can obviously be done very simply with softwares like \texttt{R} for example (see the corresponding companion book).\\
	
	E2. A well known example is the convolution of two gaussians (\SeeChapter{see section Statistics page \pageref{sum of two random normal variables}}) that also result in a... gaussian. And obviously the discrete version of the convolution also give a gaussian as illustrated below with \texttt{R} (see the corresponding companion book for more details):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.55]{img/analysis/discrete_convolution_gaussians_R.jpg}
	\end{figure}
	where we have as the reader have noticed above, the following convolution $\mathcal{N}(5,20)*\mathcal{N}(10,3)$ and we see obviously that the result gives a gaussian of $\mathcal{N}(5+10=15,\sqrt{20^2+3^2}=15)$.
	\end{tcolorbox}
	Let us now prove that the discrete convolution is also associative for the discrete convolution (as promised earlier!):
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	However, in image processing, we often consider convolution in spatial domain where causality does not apply.
	
	If $h[m]=h[-m]$ is symmetric (almost always true in image processing),
	then replacing $m$ by $-m$ we get:
	
	We see that now the convolution is the same as the {\em correlation} of the two functions. 
	
	If the input $x[m]$ is finite (always true in reality), i.e.:
	
	its index $n+m$ in the convolution has to satisfy the following for $x$ to be in the valid non-zero range:
		
	or correspondingly, the index $n$ of the output $y[n]$ has to satisfy:
	
	When the variable index $m$ in the convolution is equal to $k$, the 
	index of output $y[n]$ reaches its lower bound $n=-k$; when $m=-k$, 
	the index of $y[n]$ reaches its upper bound $n=N+k-1$. In other words,
	there are $N+2k$ valid (non-zero) elements in the output:
	
	
	Assume the size of the input signal $x[n]$ is $N$ ($n=0,\cdots,n=N-1$) and the size of $h$ is $M=2k+1$ (usually an odd number), then the size of the resulting convolution $y=x*h$ is $N+M-1=N+2k$. However, as it is usually desirable for the output $y$ to have the same size as the input $x$, we can drop $k$ components at each end of $y$. When the size of $h$ is even,we can drop $k$ components at one end and $k-1$ from the other of $y$.
	
	The code segment for this $1$D convolution $y=x*h$ is given below. 
	
	In particular, if the elements of the kernel are all the same (an average operator or a low-pass filter), then we can speed up the convolution process while sliding the kernel over the input signal by taking care of only the two ends of the kernel.
	
	\subsubsection{Matrix Convolution}\label{matrix convolution}
	In image processing, all of the discussions above for one-dimensional convolution are generalized into two dimensions!
	
	"\NewTerm{Matrix convolution}\index{matrix convolution}" is the treatment of a matrix by another one which is named the "\NewTerm{kernel}". Most of the time, the convolution matrix filter uses a first matrix which is the image to be treated. The image is a bi-dimensional collection of pixels in rectangular coordinates. The used kernel $h$ of always \underline{odd} $k\times k$ dimensions  depends on the effect you want!
	
	
	For example, if we have the following matrices:
	
	Then (don't forget that the columns and row of the kernel matrix are flipped!):
	
	That can be illustrated as following ($h$ has already been double-flipped in the figure below):
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{img/analysis/matrix_convolution.jpg}
		\caption{Matrix Convolution}
	\end{figure}
	The full example can be run and reproduced with a free software like R:
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.0\textwidth]{img/analysis/matrix_convolution_r.jpg}
	\end{figure}
	Or for people who may prefer a slow implementation in C++:
	\begin{lstlisting}[language={C++}, caption={C++ for matrix convolution}]
	// find center position of kernel (half of kernel size)
	kCenterX = kCols / 2;
	kCenterY = kRows / 2;

	for(i=0; i < rows; ++i)              // rows
	{
	    for(j=0; j < cols; ++j)          // columns
	    {
	        for(m=0; m < kRows; ++m)     // kernel rows
	        {
	            mm = kRows - 1 - m;      // row index of flipped kernel
	
	            for(n=0; n < kCols; ++n) // kernel columns
	            {
	                nn = kCols - 1 - n;  // column index of flipped kernel
	
	                // index of input signal, used for checking boundary
	                ii = i + (kCenterY - mm);
	                jj = j + (kCenterX - nn);
	
	                // ignore input samples which are out of bound
	                if( ii >= 0 && ii < rows && jj >= 0 && jj < cols )
	                    out[i][j] += in[ii][jj] * kernel[mm][nn];
	            }
	        }
	    }
	}
	\end{lstlisting}

	\pagebreak
	\subsection{Integral Transforms}
	An "\NewTerm{integral transform}\index{integral transform}" is an operator that maps functions from one space to another. Formally:
	
	Now the practical motivation for an integral transform is to reduce the complexity of the problem i.e the mathematical operations will be much easier to handle in the image space (typically the resolution of differential equations!).
	
	However, as much as it is fun to do work in the image space, one has to be able to interpret the results in the original space. To do so requires the study of the operator $K$. Usually one knows a priori the nature of the function $f$ by the nature of the problem one is dealing. Hence the study of integral transforms is the study of the operator $\mathcal{T}$. Two properties come very easily:
	
	To ensure invertibility, one has to show that the kernel space only contains the null function.

	The Fourier and Laplace transforms are for example continuous (integral) transforms of continuous functions (even if there exist a discrete version of the Fourier transform!).

	The Laplace transform maps a function $f(t)$ to a function $\mathcal{F}(s)$ of the complex variable $s$, where:
	
	Since the derivative:
	
	maps to $s\mathcal{F}(s)$, the Laplace transform $\mathcal{L}$ of a linear differential equation is an algebraic equation. Thus, the Laplace transform is useful for, among other things, solving linear differential equations.
	
	If we set the real part of the complex variable $s$ to zero, $\sigma=0$, the result is the Fourier transform $\mathcal{F}(\mathrm{i}\omega)$ which is essentially the frequency domain representation of $f(t)$ (note that this is true only if for that value of $\sigma$ the formula to obtain the Laplace transform of $f(t)$ exists, i.e., it does not go to infinity).
	
	The $\mathcal{Z}$-transform is essentially a discrete version of the Laplace transform and, thus, can be useful in solving difference equations, the discrete version of differential equations. The $\mathcal{Z}$-transform maps a sequence $f[n]$ to a continuous function $F(z)$ of the complex variable $z=re^{\mathrm{i}\Omega}$.
	
	If we set the magnitude of $z$ to unity, $r=1$, the result is the Discrete Time Fourier Transform (DTFT) $\mathcal{F}(\mathrm{i}\Omega)$ which is essentially the frequency domain representation of $f[n]$.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The three integral transformations mentioned above (Fourier, Laplace, $\mathcal{Z}$) are only a sample of what exists in practice. Let us also mention the Hartley transform, the Mellin transform, the Weierstrass transform, the Hankel transform (Fourier-Bessel), the Abel transform, the Hilbert transform, the Gauss-Weierstrass transform, etc.\\
	
	\textbf{R2.} For information we will calculate cases and prove properties of the Fourier, Laplace, $\mathcal{Z}$ and Hilbert transforms only used in concrete applications in the industry and mainly useful in other chapters of this book! Indeed a rather general presentation would require a few hundred pages and it is not the objective of this book, as you already know it, to prove mathematical properties not associated with concrete cases.
	\end{tcolorbox} 
	
	\subsubsection{Fourier Transform} \label{fourier transform analysis}
	So we have already introduced Fourier Transforms in the section of Sequences and Series (page \pageref{fourier transform}), we will come back here more in details on this topic in (we wish) are more structured way...
	
	\paragraph{Continuous Time Fourier Transform}\mbox{}\\\\	
	The Fourier expansion coefficient $X[k]$ of a continuous periodic  signal $x_T(t)=x_T(t+T)$ is:
	
	and the Fourier expansion of the signal is:
	
	which can also be written as:
	
	where $X(k\omega_0)$ is defined as:
	
	
	When the period of $x_T(t)$ approaches infinity $T \rightarrow +\infty $, the 
	periodic signal $x_T(t)$ becomes a non-periodic signal $x(t)$ and the following 
	will result:
	\begin{itemize}
	
	\item Interval between two neighbouring frequency components becomes zero:
	
	
	\item Discrete frequency becomes continuous frequency:
	
	
	\item Summation of the Fourier expansion becomes an integral:
	
	the second equal sign is due to the general fact:
	
	
	\item Time integral over $T$ becomes over the entire time axis:
	
	\end{itemize}
	
	In summary, when the signal is non-periodic $x(t)=\lim_{T\rightarrow +\infty}x_T(t)$, the Fourier expansion becomes Fourier transform. The forward transform (analysis) is:
	
	and the inverse transform (synthesis) is:
	
	
	Comparing Fourier coefficient of a periodic signal $x_T(t)$ with Fourier spectrum of a non-periodic signal $x(t)$:
	
		
	The spectrum of a time signal can be denoted by $X(\omega)$ or $X(f)$ to emphasize the fact that the spectrum represents how the energy contained in the signal is distributed as a function of frequency $\omega$ or $f$. Moreover, if $X(f)$ is used, the factor $1/2\pi$ in front of the inverse transform is dropped so that the transform pair takes a more symmetric form. On the other hand, as Fourier transform can be considered as a special case  of Laplace transform when the real part $\sigma$ of the complex argument $s=\sigma+\mathrm{i}\omega=\mathrm{i}\omega$ is zero:
	
	it is also natural to denote the spectrum of $x(t)$ by $X(\mathrm{i}\omega)$.
	
	Ok this done let us see now some important example for physics (especially quantum physics) and signal processing!
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider the unit impulse function (Dirac function):
	
	Therefore:
	
	and if $a=0$ we have then:
	
	E2. If the spectrum of a signal $x(t)$ is a delta function in frequency domain $X(\mathrm{i}\omega)=2\pi\;\delta(\omega)$, the signal can be found to be:
	
	i.e.:
	
	E3. We consider:
	
	The spectrum is:
	
	This is the sinc function with a parameter $a$.\\
	
	Note that the height of the main peak is $2a$ and it gets taller and narrower as	$a$ gets larger.
	
	\end{tcolorbox}
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	 Also note:
	
	When $a$ approaches infinity, $x(t)=1$ for all $t$, and the spectrum becomes:
	
	Recall that the Fourier coefficient of $x(t)=1$ is:
	
	which represents the energy contained in the signal at $k=0$ (DC component at zero frequency), and the spectrum $X(\mathrm{i}\omega)=X[k]/\omega$ is the energy density or distribution which is infinity at zero frequency.\\
	
	The integral in the above transform is an important formula to be used frequently later:
	
	which can also be written as:
	
	Switching $t$ and $f$ in the equation above, we also have:
	
	representing a superposition of an infinite number of cosine functions of all
	frequencies, which cancel each other any where along the time axis except at
	$t=0$ where they add up to infinity, an impulse. \\
	
	E3. Let us now consider:
	
	The spectrum of the cosine function is:
		
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	The spectrum of the sine function:
	
	can be similarly obtained to be:
	
	Again, these spectra represent the energy density distribution of the sinusoids, while the corresponding Fourier coefficients:
	
	and:
	
	represent the energy contained at frequency $\omega=\omega_0$.
	\end{tcolorbox}
	
	
	
	
	\subparagraph{Properties of Fourier Transform}\mbox{}\\\\	
	The properties of the Fourier transform are summarized below. For some of them already proved in the section of Sequences and Series we will not give the poof. But for others we will do it again! 
	
	In the following, we assume 
	$\;\;{\cal F}(x(t))=X(\mathrm{i}\omega)$ and ${\cal F}(y(t))=Y(\mathrm{i}\omega)$.
	
	\begin{itemize}
	
	\item[P1.] Linearity:
	
	
	\item[P2.] Time shift:
	
	\begin{dem} Let $t'=t\pm t_0$, i.e., $t = t' \mp t_0$, we have:
	\begin{eqnarray}
	{\cal F}(x(t \pm t_0))&=&\int\limits_{-\infty}^{+\infty} x(t\pm t_0)
		e^{-\mathrm{i}\omega t} \mathrm{d}t
		=\int\limits_{-\infty}^{+\infty} x(t')e^{-\mathrm{i}\omega(t'\mp t_0)} \mathrm{d}t'
		\nonumber \\
		&=& e^{\pm \mathrm{i}\omega t_0}
		\int\limits_{-\infty}^{+\infty} x(t')e^{-\mathrm{i}\omega t'} \mathrm{d}t'=X(\mathrm{i}\omega)e^{\pm \mathrm{i}\omega t_0}
		\nonumber
	\end{eqnarray}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P3.] Frequency shift:
	
	\begin{dem} Let $\omega'=\omega\pm \omega_0$, i.e., $\omega = \omega'\mp\omega_0$,
	we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P4.] Time reversal:
	
	\begin{dem}
	
	Replacing $t$ by $-t'$, we get:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P5.] Even and Odd Signals and Spectra:
	
	If the signal $x(t)$ is an even (or odd) function of time, its spectrum $X(\mathrm{i}\omega)$ is an even (or odd) function of frequency:
	
	and:
	
	\begin{dem} If $x(t)=x(-t)$ is even, then according to the time reversal property, we have:
	
	i.e., the spectrum $X(\mathrm{i}\omega)=X(-\omega)$ is also even. Similarly, if $x(t)=-x(-t)$ is odd, we have:
	
	i.e., the spectrum $X(\mathrm{i}\omega)=-X(-\omega)$ is also odd.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P6.] Time and frequency scaling:
	
	\begin{dem}
	Let $u=at$, i.e., $t=u/a$, where $a>0$ is a scaling factor, we have:
	
	Note that when $a<1$, time function $x(at)$ is stretched, and $X(\mathrm{i}\omega/a)$ is compressed; when $a>1$, $x(at)$ is compressed and $X(\mathrm{i}\omega/a)$ is stretched.	This is a general feature of Fourier transform, i.e., compressing one of the $x(t)$ and $X(\mathrm{i}\omega)$ will stretch the other and vice versa. In particular, when $a\rightarrow 0$, $x(at)$ is stretched to approach a constant, and $X(\mathrm{i}\omega/a)/a$ is compressed with its value increased to approach an impulse; on the other	hand, when $a \rightarrow +\infty$, $ax(at)$ is compressed with its value increased to approach an impulse and $X(\mathrm{i}\omega/a)$ is stretched to approach a constant.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P7.] Complex Conjugation:
	
	
	\begin{dem} Taking the complex conjugate of the inverse Fourier transform, we get:
	
	Replacing $\omega$ by $-\omega'$ we get the desired result:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We further consider two special cases:
	\begin{itemize}
	\item If $x(t)=x^*(t)$ is real, then:
	
	i.e., the real part of the spectrum is even (with respect to frequency $\omega$), and the imaginary part is odd:
	
	\item If $x(t)=-x^*(t)$ is imaginary, then:
	
	i.e., the real part of the spectrum is odd, and the imaginary part is even:
	
	\end{itemize}
	
	If the time signal $x(t)$ is one of the four combinations shown in the table (real even, real odd, imaginary even, and imaginary odd), then its spectrum $X(\mathrm{i}\omega)$ is given in the corresponding table entry:
	\vskip0.2in
	\begin{table}[H]
		\centering
		\begin{tabular}{c||c|c} \hline
			& if $x(t)$ is real		& if $x(t)$ is imaginary	\\ 
			& $X_r$ even, $X_i$ odd	& $X_r$ odd, $X_i$ even \\ \hline \hline
		if $x(t)$ is Even	&			&		\\
		$X_r$ and $X_i$ even	& $X_i=0$, $X=X_r$ even & $X_r=0$, $X=X_i$ even	\\ \hline
		if $x(t)$ is Odd	& 			&		\\
		$X_r$ and $X_i$ odd	& $X_r=0$, $X=X_i$ odd	& $X_i=0$, $X=X_r$ odd	\\ \hline
		\end{tabular}
	\end{table}
	Note that if a real or imaginary part in the table is required to be both even 
	and odd at the same time, it has to be zero.
	
	These properties are summarized below:
	\vskip 0.1in
	\begin{table}[H]
		\centering
		\begin{tabular}{l|l|l} \hline
		  & $x(t)=x_r(t)+\mathrm{i}x_i(t)$	& $X(\mathrm{i}\omega)=X_r(\mathrm{i}\omega)+\mathrm{i}X_i(\mathrm{i}\omega)$	\\ \hline
		1 & real $x(t)=x_r(t)$ 		& even $X_r(\mathrm{i}\omega)$, odd $X_i(\mathrm{i}\omega)$ \\
		2 & real and even $x(-t)=x_r(t)$ 	& real and even $X_r(\mathrm{i}\omega)$ \\
		3 & real and odd $x(-t)=-x_r(t)$ 	& imaginary and odd $X_i(\mathrm{i}\omega)$ \\
		4 & imaginary $x(t)=x_i(t)$  	& odd $X_r(\mathrm{i}\omega)$, even $X_i(\mathrm{i}\omega)$ \\ 
		5 & imaginary and even $x(-t)=x_i(t)$ 	& imaginary and even $X_i(\mathrm{i}\omega)$ \\
		6 & imaginary and odd $x(-t)=-x_i(t)$ 	& real and odd $X_r(\mathrm{i}\omega)$ \\ \hline
		\end{tabular}
	\end{table}
	
	As any signal can be expressed as the sum of its even and odd components, the first three items above indicate that the spectrum of the even part of a real signal is real and even, and the spectrum of the odd part of the signal is  imaginary and odd. 
	
	\item[P8.] Symmetry (or Duality):
	
	
	Or in a more symmetric form:
	
	\begin{dem} As ${\cal F}(x(t))=X(\mathrm{i}\omega)$, we have:
	
	Letting $t'=-t$, we get:
	
	Interchanging $t'$ and $\omega$ we get:
	
	or:
	
	In particular, if the signal is even:
	
	then we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	For example, the spectrum of an even square wave is a sinc function, and the spectrum of a sinc function is an even square wave. 
	
	\item[P9.] Multiplication theorem:
	
	
	\begin{dem} 
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P10.] Parseval's equation\index{Parseval's equation} for the Fourier transform (for more details see page \pageref{Parseval theorem}):
	
	In the special case when $y(t)=x(t)$, the above becomes the Parseval's equation:
	
	where:
	
	is the energy density function, commonly named "\NewTerm{power spectrum}\index{power spectrum}\label{power spectrum}", representing how the signal's energy is distributed along the frequency axes. The total energy contained in the signal is obtained by integrating $S(\mathrm{i}\omega)$ over the entire frequency axes.
	
	The Parseval's equation also indicates that the energy or information contained in the signal is reserved, i.e., the signal is represented equivalently in either the time or frequency domain with no energy gained or lost!
	
	The latter relation is more commonly written:
	
	or more simply:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have already proved in the section of Sequences and Series that the Fourier transform of the square pulse was given by (see page \pageref{fourier transform pulse square}):
	
	Hence:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The energy spectrum of the pulse we have just calculated shows a great similarity with the Fraunhofer diffraction pattern due to a narrow slit (\SeeChapter{see section Wave Optics page \pageref{fraunhofer diffraction}}). In reality, it is more than a similarity because it is possible to prove in physics that any diffraction pattern is the Fourier transform of the object that is the cause!
	\end{tcolorbox}
	
	
	\item[P11.] Correlation:
	
	The "\NewTerm{cross-correlation}\index{cross-correlation}" of two real signals $x(t)$ and $y(t)$ is defined as (notice that it is strictly equivalent to the definition of the continuous convolution seen just earlier above at page \pageref{continuous convolution}):
	
	Specially, when $x(t)=y(t)$, the above becomes the "\NewTerm{auto-correlation}\index{auto-correlation}" of signal $x(t)$:
	
	Assuming ${\cal F}(x(t))=X(\mathrm{i}\omega)$, we have ${\cal F}(x(t-\tau))=X(\mathrm{i}\omega)e^{-\mathrm{i}\omega\tau}$ and according to multiplication theorem, $R_x(\tau)$ can be written as:
	
	i.e.:
	
	that is, the auto-correlation and the energy density function of a signal $x(t)$ are a Fourier transform pair.
	
	\item[P12.] Convolution Theorems:
	
	The "\NewTerm{convolution theorem}\index{convolution theorem}\label{convolution theorem}" states that convolution in time domain corresponds to multiplication in frequency domain and vice versa:
	
	
	Let us start with the proof of ($a$)!
	
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	And now let us continue with the proof of ($b$)!

	\begin{dem}
	
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P13.]  Time Derivative\label{fourier transform time derivative}:
	
	\begin{dem} 
	Differentiating the inverse Fourier transform $X(\mathrm{i}\omega)$ with respect to $t$ we get:
	
	Repeating this process we get:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P14.] Time Integration\label{fourier transform time integration}:
	
	First consider the Fourier transform of the following two signals:
	
	
	According to the time derivative property above:
	
	we get:
	
	and:
	
	Why do the two different functions have the same transform?
	
	In general, any two function $f(t)$ and $g(t)=f(t)+c^{te}$ with a constant difference $c^{te}$ have the same derivative $\mathrm{d}\;f(t)/\mathrm{d}t$, and therefore they have the same transform according the above method. This problem is obviously caused by the fact that the constant difference $c^{te}$ is lost in the derivative operation.
	
	To recover this constant difference in time domain, a delta function 
	needs to be added in frequency domain. Specifically, as function $\mathrm{sgn}(t)$ does not have DC component, its transform does not contain a delta:
	
	To find the transform of $u(t)$, consider:
	
	and:
	
	The added impulse term $\pi \delta(\omega)$ directly reflects the constant $c=1/2$ in time domain.
	
	Now we show that the Fourier transform of a time integration is:
	
	
	\begin{dem}
	
	First consider the convolution of $x(t)$ and $u(t)$:
	
	Due to the convolution theorem, we have:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P15.] Frequency Derivative:
	
	\begin{dem} We differentiate the Fourier transform of $x(t)$ with
	respect to $\omega$ to get:
	
	i.e.:
	
	Multiplying both sides by $\mathrm{i}$, we get:
	
	Repeating this process we get:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\end{itemize}
	
	\pagebreak
	\subparagraph{Usual Fourier transforms}\label{usual Fourier transforms}\mbox{}\\\\
	There are in math and physics many Fourier transforms of signals we see quite frequently (but not exclusively). Furthermore, all Fourier transforms proved below will be used in the various chapters on Physics, Engineering, Atomistic, Social Mathematics, etc of this book. So, as in any formula booklet, we propose you the most Fourier transforms but with... the proofs!
	\begin{enumerate}
	
	\item Impulse:
	
	As shown above:
	
	or:
	
	It is therefore immediate that the inverse Fourier transform of the complex exponential is the Dirac delta:
	
	
	\item Unit Step:
	
	As shown above:
	
	
	\item Constant:
	
	As shown above:
	
	This is a useful formula.
	
	\item Complex exponential:
	
	The spectrum of a complex exponential can be found from the above due to the frequency shift property:
	
	Let us write this more explicitly:
	
	Now let us try to solve this using the physicist way... Using what we have just seen before (the inverse Fourier transform of the Dirac pulse), we have:
	
	it comes obviously after rearranging (multiplying both sides by $2\pi$):
	
	
	\item Sinusoids:
	
	
	Therefore:
	
	Similarly, we have:
	
	
	\item Exponential decay (right-sided):
	
	Therefore:
	
	
	\item Exponential decay (left-sided):
	
	Due to the time reversal property, we also have (for $a>0$):
	
	or:
	
	
	\item Exponential decay (two-sided):
	
	As the two-sided exponential decay is the sum of the right and left-sided 
	exponential decays, its spectrum of $x(t)$ is the sum of their spectra due 
	to linearity:
	
	
	\item Comb function:
	
	The comb function is defined as:
	
	Its Fourier series coefficient is:
	
	and its spectrum is:
	
	We see that the spectrum of an impulse train with time interval $T$ is also an impulse train with frequency interval $\omega_0=2\pi/T$. Also, according to the definition of the Fourier transform, we have:
	
	Therefore we have this equation:
	
	which can be compared with the equation in continuous case:
	
	
	
	\item Square wave (for another detailed derivation see page \pageref{fourier transform pulse square}):
	
	A square wave or rectangular function of width $a$ can be considered as the  difference between two unit step functions:
	
	and due to linearity, its Fourier spectrum is the difference between 
	the two corresponding spectra:
	
	
	\item Sinc function:
	
	The spectrum of an ideal low-pass filter is:
	
	and its impulse response can be found by inverse Fourier transform:
	
	
	\item Triangle function:
	
	As $x(t)$ is an even function, its Fourier transform is:
	
	Alternatively, as the triangle function is the convolution of two square functions
	($a=1/2$), its Fourier transform can be more conveniently obtained according to the
	convolution theorem as: 
	
	
	\item Gaussian function:
	
	The Fourier transform of a Gaussian or bell-shaped function $x(t)=e^{-\pi t^2}$ is:
	
	Here we have used the identity:
	
	We see that the Fourier transform of a bell-shaped function is also a bell-shaped function:
	
	Note that the area underneath either $x(t)$ or $X(\mathrm{i}\omega)$ is unity. Moreover, due to the property of time and frequency scaling, we have:
	
	(Note that if $a=1/\sqrt{2\pi \sigma^2}$, then $a\;x(at)$ above is a normal 	distribution with variance $\sigma^2$ and mean $\mu=0$.) If we let $a \rightarrow \infty$, $x(t)$ becomes narrower and taller and  approaches $\delta(t)$, and its spectrum $e^{-\pi (f/a)^2}$ becomes wider and approaches constant $1$. On the other hand, if we rewrite the above as:
	
	and let $a \rightarrow 0$, $x(t)$ approaches $1$ and $X(\mathrm{i}\omega)$ approaches $\delta(\omega)$.
	
	\end{enumerate}
	
	Now that we have seen quite a number of properties and usual Fourier Transform, let us go back to our heat equation\index{heat equation} determined in the section of Thermodynamics (see page \pageref{heat equation}) in the form:
	
	And let us also be interested in the case where:
	
	To simplify the writing, we will write the differential equation in the following form:
	
	Let us take the Fourier transform relatively to $x$ of this equality. Let us recall that for this purpose we have proved in the section of Sequences and Series and just earlier at page \pageref{fourier transform time derivative} that (time derivative property):
	
	Let us put to simplify the notations:
	
	As we take the Fourier transform with respect to $x$, we can take out the partial derivative of the integral of the Fourier transform such as:
	
	Our differential equation is then reduced to:
	
	Thus explicitly:
	
	We see then immediately with the simplified version that a particular solution is:
	
	The constant has to be determined by the initial condition:
	
	Therefore:
	
	Then it comes by doing the inverse Fourier transform in $x$:
	
	Now let's make a small change of notation by putting:
	
	Then we have:
	
	We then fall much more quickly on the same integral that we obtained in the section of Thermodynamics in our study of the heat equation, the finishing work being the same, the reader can refer to it!

	We had also proved in the section Thermodynamics that we obtained:
	
	We then notice that the Fourier transform is not only a tool for analysing a frequency domain signal but also for solving some differential equations more quickly.
	
	But as many times in mathematics, we must be careful using such a tool. They may be some trap and subtilities. Let us see on famous example!

	We know that the set of solutions of the differential equation:
	
	we $y$ is a continuous function from $\mathbb{R} \mapsto \mathbb{C}$ is made of the set of constant functions!
	
	Let us try to solve this equation using the Fourier transform. By taking the Fourier transform on the left and on the right we get:
	
	An error would be to believe that we can divide left and right by $\omega $ and thus get:
	
	In this case, taking the inverse transform would get $ y = 0 $ as the only solution to the equation, then we would lose all other solutions.
	
	In fact, what we must remember is that the Fourier transform is defined on the space of "temperate distributions" and that therefore, as long as we decide to use this integral transform to solve the differential equation  above, we also decide, implicitly, to look for solutions in this space. Now in the space of temperate distributions, the equation $\omega\mathcal{F}(y)=0$ possesses an infinity of solutions that are given by (without proof) $c\cdot \delta$ where $c$ is a complex number and $\delta$ is the Dirac distribution. As a result:
	
	and taking the inverse transform we get indeed:
	
	We therefore conclude that when we solve differential equations with the Fourier transform it must be remembered that in the space of temperate distributions the usual algebraic calculation rules are to be handled with care. This remark is obviously valid only for people who know the Fourier transform but have only a vague idea of what is a temperate distribution (which is normally the case for engineers....
			
	\paragraph{Discrete Time Fourier Transform}\mbox{}\\\\
	A discrete-time signal can be considered as a continuous signal $x(t)$ 
	sampled at a rate $F=1/t_0$ or $\Omega=2\pi/t_0$, where $t_0$ is the 
	sampling period (time interval between two consecutive samples). The
	corresponding sampling function (comb function) is:
	
	The sampling process can be represented by:
	
	where $x[m]=x(mt_0)$ is the value of $x(t)$ at $t=mt_0$. The Fourier transform of this discrete signal (treated as a special case of continuous signal) is:
	
	This is the forward Fourier transform (analysis) of a discrete signal $x_s(t)$. The spectrum $X(\mathrm{i}\omega)$ is periodic with period $\Omega=2\pi F=2\pi/t_0$:
	
	as :
	
	
	To get back the time signal $x[m]$ from its spectrum:
	
	we multiply the equation by $e^{\mathrm{i}\omega nt_0}/\Omega$ and integrate both sides with respect to $\omega$ over the period $\Omega=2\pi F=2\pi/t_0$	to obtain the inverse Fourier transform (synthesis):
	
	Note that here we used:
	
	which can be compared this with:
	
	To summarize, the spectrum of a given discrete signal:
	
	can be found by the "\NewTerm{forward discrete Fourier transform}\index{forward discrete Fourier transform}\index{discrete Fourier transform}" to be:
	
	and the signal can be expressed by inverse Fourier transform:
	
	It is interesting to compare this discrete time Fourier transform pair with the Fourier series expansion (the Fourier transform of a periodic signal): 
	
	
	with discrete spectrum:
	
	We see symmetry between these two different forms of Fourier transform. If the  signal $x(t)=x(t+T)$ is periodic, its spectrum $X(\mathrm{i}\omega)$ is discrete, the coefficients of the Fourier series with interval $\omega_0=2\pi/T$. On the other hand, if $x(t)$ is discrete with interval $t_0=2\pi/\Omega$, its spectrum $X(\mathrm{i}\omega)=X(\mathrm{i}\omega+\Omega)$ is periodic.
	
	In particular, if the unit of time is so chosen that the sampling period is $t_0=1$, then $\Omega=2\pi/t_0=2\pi$, and the forward Fourier transform of a discrete signal becomes:
	
	The inverse transform becomes:
	
	The spectrum $X(\mathrm{i}\omega)=X(\mathrm{i}\omega+2\pi)$ is periodic.
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	The spectrum of a time signal (continuous or discrete) can be denoted by $X(\mathrm{i}\omega)$ or $X(f)$ to emphasize the fact that the spectrum represents how the energy contained in the signal is distributed as a function of frequency $\omega$ or $f$. Moreover, if $X(f)$ is used, the factor $1/2\pi$ in front of the inverse transform is dropped so that the transform pair takes a more symmetric form. On the other hand, as Fourier transform of discrete signal can be considered as a special case of Z transform when the real part of $s=\sigma+\mathrm{i}\omega$ is zero, i.e., $z=e^s=e^{\mathrm{i}\omega}$:
	
	it is also natural to denote the spectrum of $x[n]$ by $X(e^{\mathrm{i}\omega})$.
	\end{tcolorbox}
	
	
	\paragraph{Properties of Discrete Fourier Transform}\mbox{}\\\\
	As a special case of general Fourier transform, the discrete time transform  shares all properties (and their proofs) of the Fourier transform discussed above, except now some of these properties may take different forms. In the following, we always assume ${\cal F}[x[m]]=X(e^{\mathrm{i}\omega})$ and ${\cal F}[y[m]]=Y(e^{\mathrm{i}\omega})$. 
	
	\begin{enumerate}
	\item[P1.] Linearity:
	
	
	\item[P2.] Time Shifting:
	
	\begin{dem}
	
	If we let $m'=m-m_0$, the above becomes:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	
	\item[P3.] Time Reversal:
	
	
	
	\item[P4.] Frequency Shifting:
	
	
	\item[P5.] Differencing:
	
	Differencing is the discrete-time counterpart of differentiation.
	
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	
	\item[P6.] Differentiation in frequency:
	
	
	\begin{dem}
	Differentiating the definition of discrete Fourier transform with respect to 	$\omega$, we get:
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\item[P7.] Convolution Theorems:
	
	The convolution theorem states that convolution in time domain corresponds to multiplication in frequency domain and vice versa:
	
	
	Recall that the convolution of periodic signals $x_T(t)$ and $y_T(t)$ is:
	
	Here the convolution of periodic spectra $X(f)$ and $Y(f)$ is similarly defined as:
	
	
	Proof of ($a$): 
	
	
	Proof of ($b$):
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	
	\item[P8.] Parseval's relation\index{Parseval's relation} for the discrete Fourier transform:
	
	\end{enumerate}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. The discrete Fourier transform of the Dirac delta:
	
	E2. The spectrum of:
	
	is:
	
	If the signal is two-sided:
	
	Due to the time reversal property, its spectrum is:
	
	E3. Consider a LTI system (Linear Time-Invariant) with impulse response:
	
	and input:
	
	The output $y[n]$ can be found in either time domain by convolution or in frequency domain by multiplication. In time domain, we have:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	When $a=b$, we have:
	
	In frequency domain, we first find the spectra of both $x[n]$ and $h[n]$ to be:
	
	and the spectrum of the output is:
	
	To find $y(n)$ in time domain by inverse transform of $Y(e^{\mathrm{i}\omega})$, we use partial fraction expansion to rewrite the above as:
	
	By equating the coefficients of $e^{-\mathrm{i}\omega}$ and the constants, we get:
	
	which can be solved to get:
	
	In this form, $Y(\mathrm{i}\omega)$ can be easily inverse transformed to yield:
	
	same as the result from convolution. Again when $a=b$, we have:
	
	But since:
	
	by the frequency differentiation property, we have:
	
	and the output in time domain is obtained as:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	Note that the time-shifting property is used due to the factor $e^{\mathrm{i}\omega}$. Also note that $u[n+1]$ (starting at $n=-1$) is replaced by $u[n]$ (starting at $n=0$) as $n+1=0$ when $n=-1$.\\
	
	E4. The impulse response of a discrete LTI system is:
	
	where $|a|<1$ so that the system is stable. The output $y[m]$ of the system with an input:
	
	can be found in three different ways.
	\begin{itemize}
		\item Time domain convolution: 
		The output is the convolution of $x[m]$ and $h[m]$:
		
		
		\item The eigenequation method:\\ 
	
		We first get the frequency response function from $h[m]$:
		
		which is the eigenvalue of the system when the input is a complex exponential $e^{\mathrm{i}n\omega}$. Now the system's response to: 
		
		can be found to be:
		
	\end{itemize}
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{itemize}
		\item Frequency domain multiplication:
		
		If we find the spectra of both $h[m]$ and $x[m]$ in the frequency domain, the spectrum of $y[m]$ can be found by multiplication. We already know:
		
		We next find the spectrum of $x[m]$:
		
		Now the spectrum of the output $y[m]$ can be found:
		
		and the output $y[m]$ is obtained by inverse Fourier transform:
		\begin{eqnarray}
		y[m] &=& \frac{1}{2\pi} \int\limits_0^{2\pi} \left[\frac{\pi}{1-ae^{-\mathrm{i} \omega}}
		 \sum_{k=-\infty}^{+\infty} \left[\delta(\omega-2k\pi-\frac{2\pi}{N})+\delta(\omega-2k\pi-\frac{2\pi}{N})\right]\right]
			e^{\mathrm{i}m\omega} \mathrm{d}\omega
			\nonumber \\
		 &=& \frac{1}{2}e^{\mathrm{i}2\pi m/N}\frac{1}{1-ae^{-\mathrm{i}2\pi /N}}
			+\frac{1}{2}e^{-\mathrm{i}2\pi m/N}\frac{1}{1-ae^{\mathrm{i}2\pi /N}}	
			\nonumber
		\end{eqnarray}
	\end{itemize}
	
	The physical meaning of this result will be clear if we write $H(2\pi/N)$ in polar form:
	
	and the output becomes:
	
	That is, the output of the system is also a sinusoidal signal of the same 
	frequency as the input, but with different magnitude $r$ and a phase angle
	$\theta$. For example, if $N=4$, we have:
	
	and the output is:
	
	\end{tcolorbox}
	
	
	
	\StickyNote[2.5cm]{\LARGE To finish depending on donations}[6.5cm]
	
	\pagebreak
	\subsubsection{Laplace Transform}\label{Laplace transform}
	The Laplace transform (LT), as we have already mention it, is a generalization of the Fourier transform (TF), however, although it is so called in his honour because he used it in his work on the theory of probability, seems to have been originally discovered by Leonhard Euler. The Laplace transform also appears in all branches of mathematical physics (mechanical engineering, electronics, quantitative finance, etc.) and is used extensively in order to solve  differential equations that arise in many modelling situations of real life.
	
	As for the Fourier transform, the Laplace transformation allows us to get rid of differentiations. The transform can do this because it has the wonderful property of converting the operation of differentiation into the far simpler one of multiplication. That is, it transforms a differential equation into an algebraic equation. This process is analogous to how logarithms transform multiplication into the simpler operation of
addition (because $\log (xy)=\log(x)+\log(y)$).

	A power series about an origin is any series that can be written in the following form:
	
	where $a_n$ are  numbers and $n$ is a non-negative integer. One can think of $a_n = a(n)$ as a function of $n$ for each non-negative integer $n = 0, 1, 2, \ldots$. In order to give birth to Laplace transformation technique, we  make some associations. The discrete variable $n$ is converted into a real variable $t$. The coefficient term $a_n$ is written as $f(t)$. The term $x^n$ can equivalently be written as $e^{(\ln (x^t))}$. Finally, summation notation can be replaced by its continuous analogue, that is, integration. By doing so, we have following:
	 
For convergence, it is obviously important to have following condition for the above integral (yes think for this about the original sum!):
	 
	Therefore:
	 
	Since $0<x<1 $ so it implies that:
	
	Thus $\ln(x)$ has to be negative for the integral to converge, in this regard, we suppose $\ln(x)=-s$ where $s>0$. Thus, the final integral takes the form:
	
	In this way, we can say that Laplace Transform is simply stretching a discrete (infinite series)  into a continuous (integration) analogue. 
	
	Let us recall that if $s=\mathrm{i}\omega$ (the real part of $s$ is purely imaginary), Laplace transform becomes Fourier transform! In general, any continuous time signal $x(t)$ can be Laplace transformed to get:
	
	provided the integral converges, i.e., the function $X(s)$ exists. This general form of "\NewTerm{bilateral Laplace transform}\index{bilateral Laplace transform}\index{Laplace transform}\label{bilateral Laplace transform}" is related to the Fourier transform:
	
	i.e., Laplace transform of a generic function is Fourier transform of the same function multiplied by $e^{-\sigma t}$. This exponential factor has the effect of forcing the signals to converge. This is why the Laplace transform can be applied to a broader class of signals than the Fourier transform, including exponentially growing signals shown in the following two cases:

	\begin{itemize}
		\item Right sided:
		
		The Fourier transform does not exist as the signal grows exponentially when $t\rightarrow +\infty$, i.e., the transform integral does not converge (not integrable). However, its Laplace transform exists if $\Re[s]=\sigma>a$ (i.e., $\sigma-a>0$), as the modified signal $x(t)e^{-\sigma t}=e^{-(\sigma-a)t}u(t)$ will converge.
		
		\item Left sided:
		 
		Again the Fourier transform does not exist as the signal grows exponentially when $t\rightarrow -\infty$, i.e., the transform integral does not converge. But if $\Re[s]=\sigma<a$ (i.e., $\sigma-a<0$), the modified signal $x(t)e^{-\sigma t}=e^{-(\sigma-a)t}u(-t)$ will converge and its Laplace transform exists.
	\end{itemize}

	In Fourier transform, both the signal $x(t)$ in time domain and its spectrum $X(\mathrm{i}\omega)$ in frequency domain are a one-dimensional (1D) complex function. However, the Laplace transform $X(s)$ of the 1D signal $x(t)$ is a complex function defined over a two-dimensional {\em complex plane}, called the s-plane,	spanned by the two variables $\sigma$ (for the horizontal real axis) and $\omega$ (for the vertical imaginary axis). 
	
	In particular, if this 2D function $X(s)=X(\sigma+\mathrm{i}\omega)$ is evaluated along the imaginary axis $\Re[s]=\sigma=0$, it becomes a 1D function $X(\mathrm{i}\omega)$, the	Fourier transform of $x(t)$. Graphically, the Fourier transform, the spectrum of the signal, can be found as the cross section of the 2D function $X(s)=X(\sigma+\mathrm{i}\omega)$ along the line $\Re[s]=\sigma=0$.
	
	Before we go further another way to introduce the Laplace may be useful and help the reader. But if you have understand the above introduction you can go over this alternate presentation.
	
	The Fourier transform works quite well only when the argument of the integral does not diverge, where the kernel (the multiplication by $e^{-\mathrm{i}\omega t}$) is purely complex, and its integral sweeps the set of reals (bilateral infinity $]-\infty,+\infty[$).

	Typically, mathematicians, that like to generalize stuff...., have generalized the Fourier transform to functions kernel that are not purely imaginary and whose integral could be one-sided $[0,+\infty[$. Thus, the Laplace transform converges for a larger set of functions than the Fourier transform.
	
	Indeed, let us see a simple example with a diverging function:
	
	This last integral does not converge (at least as far as I know ...) whatever the value of $\alpha$ different from zero!
	
	Taking a generalized version of the Fourier transform and denoting it (we already know that it is the bilateral Laplace transform):
	
	with (we know that already!):
	
	where in general, the convergence of the integral is not guaranteed for all $s$. We then name the "\NewTerm{abscissa of absolute convergence}" of the Laplace transform the set of values that $\sigma$ must take for the integral to converge.
	
	Let us take again the non convergent Fourier transform seen just above but where the function is defined only for the positive times only (unilateral Laplace transform):
	
	The result converges if $\omega\geq 0$ (which is always the case in physics) for $t>0$ and if and only if $\sigma>1$. Then in this case we have:
	
	
	\paragraph{Inverse Laplace Transform}\mbox{}\\\\
	The inverse Laplace transform can be obtained from the corresponding Fourier	transform:
	
	The inverse Fourier transform of the above is:
	
	Multiplying both sides by $e^{\sigma t}$, we get the inverse Laplace transform:
	
	As we want to represent the inverse transform in terms of $s$ (instead of $\omega$), we realize that:
	
	We also realize that the integral in the inverse transform is along a vertical line in the $s$-plane from $\mathrm{i}\omega=-\mathrm{i}\infty$ to $\mathrm{i}\omega=+\mathrm{i}\infty$ while the other variable $\sigma$ is fixed. The "\NewTerm{inverse Laplace transform}\index{inverse Laplace transform}" above can therefore be written as:
	
	Now we have the Laplace transform pair:
	
	
	The forward (bilateral) and inverse Laplace transform pair can also be represented as:
	
	
	\pagebreak
	\paragraph{Region of Convergence}\mbox{}\\\\
	An essential issue of Laplace transform of $x(t)$ is whether the transform $X(s)$ even exists, and under what condition it exists. To see this, consider the following examples.
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	The Fourier transform of a signal $x(t)=e^{-at}u(t)$ is:
	
	This integral does not converge unless $a>0$. In other words, only when the signal $x(t)$ decays (instead of grows) exponentially, will its Fourier transform  $X(\mathrm{i}\omega)$ exist:
	
	
	Now consider Laplace transform of the same signal:
	
	Similar to the Fourier transform, for this integral to converge, i.e., for Laplace transform $X(s)$ to exist, it is necessary for $\sigma=\Re[s]$ to satisfy:
	
	in which case the Laplace transform is:
	
	As a special case where $a=0$, $x(t)=u(t)$ and we have:
	
	When $ \Re[s]=\sigma=0$, Laplace transform $X(s)$ becomes Fourier transform $X(\mathrm{i}\omega)$. \\
	
	E2. The non-causal version of the signal above is $x(t)=-e^{-at}u(-t)$ and its Laplace transform is:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Only when:
	
	will this integral converge and Laplace transform $X(s)$ exists
	
	Again as a special case when $a=0$, $x(t)=-u(-t)$ we have
	
	Comparing the two examples above we see that two different signals can have identical Laplace transform $X(s)$ but $s$ may have to satisfy different conditions for $X(s)$ to exist. In general, the set of all $s$ values satisfying the conditions for the integral of Laplace transform to converge is called the "\NewTerm{region of convergence}\index{region of convergence}" (ROC) in the complex s-plane. In the first case above, the ROC is $\Re[s]>0$, and in the second case, the ROC is $\Re[s]<0$.\\
	
	E3. Consider the signal:
	
	The Laplace transform of this signal is:
	
	Following example 1, we see that the conditions for the three integrals to converge are, respectively:
	
	i.e., when $\Re[s]>-1$, $X(s)$ exists and can be written as:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E4. Let us consider:
	
	As the Laplace integration converges independent of $s$, the ROC is the entire $s$-plane. In particular, when $T=0$, we have:
	
	\end{tcolorbox}
	\paragraph{Zeros and Poles of Laplace Transform}\mbox{}\\\\
	All Laplace transforms in the above examples are rational, i.e., they can be written as a ratio of polynomials of variable $s$ in the general form:
	
	where $N(s)$ is the numerator polynomial of order $M$ with roots $s_{z_k}, (k=1,2, \cdots, M)$, and $D(s)$ is the denominator polynomial of order $N$ with roots $s_{p_k}, (k=1,2, \cdots, N)$. In general, we assume the order of the numerator polynomial is lower than that of the denominator polynomial, i.e.,  $M < N$. If this is not the case, we can always expand $X(s)$ into multiple terms so that $M<N$ is true for each of terms.	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider:
	
	As the order of the numerator $M=2$ is higher than that of the denominator $N=1$, 
	we expand it into the following terms
	
	and get
	
	Equating the coefficients for terms $s^k$ $(k=0, 1, \cdots, M)$ on both sides, we get
	
	Solving this equation system, we get coefficients
	
	and
	
	Alternatively, the same result can be obtained by carrying out a long-hand division
	
	\end{tcolorbox}
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Any complex value $s_z$ of $s$ for which $H(s)|_{s=s_z}=H(s_z)=0$ is a "\NewTerm{zero}" of $H(s)$.

		\item[D2.]  Any complex value $s_p$ of $s$ for which $H(s)|_{s=s_p}=H(s_p)=\infty$ is a "\NewTerm{pole}\index{pole}" of $H(s)$.
	\end{enumerate}
	Obviously, all roots of the numerator polynomial $N(s)$ are zeros of $H(s)$ and all roots of the denominator polynomial $D(s)$ are poles of $H(s)$. Moreover, if the order of $D(s)$ exceeds the order of $N(s)$ (i.e., $N>M$), then $H(\infty)=0$, i.e., there is a zero at infinity. On the other hand, if the order of $N(s)$ exceeds that of $D(s)$ (i.e., $M>N$), then $H(\infty)=\infty$, i.e, there is a pole at infinity. On the $s$-plane zeros and poles can be indicated by $o$ and $x$ respectively. Most essential behaviour properties of an LTI system can be obtained graphically from the ROC and the zeros and poles of its transfer function $H(s)$ on the $s$-plane.
	
	\paragraph{Properties of region of convergence}\mbox{}\\\\
	Whether the Laplace transform $X(s)$ of a function $x(t)$ exists depends on whether or not the transform integral converges:
	
	which in turn depends on the duration and magnitude of $x(t)$ as well as the real part of $s$ $\Re[s]=\sigma$. When $x(t)$ is right sided (i.e., $x(t)=0$ for $t<t_0$), it may have infinite duration for $t>0$, and the larger $\sigma$ the more quickly $x(t)e^{-\sigma t}$ decays as $t \rightarrow +\infty$. On the other hand, if $x(t)$ is left sided (i.e., $x(t)$ for $t<t_0$), it may have infinite duration for $t<0$, and the smaller $\sigma$ the more quickly $x(t)e^{-\sigma t}$ decays as $t \rightarrow -\infty$. The imaginary part of $s$ $Im[s]=\mathrm{i}\omega$ determines the frequency of a sinusoid which is bounded and has no effect on the convergence of the integral. Based on these observations, we can get the following properties for the ROC:	
	\begin{itemize}
	
	\item If $x(t)$ is absolutely integrable and of finite duration, then the ROC is the entire $s$-plane (the Laplace transform integral is finite, i.e., $X(s)$ exists, for any $s$).
	
	\item The ROC of $X(s)$ consists of strips parallel to the $\mathrm{i}\omega$-axis in the $s$-plane.
	
	\item If $x(t)$ is right sided and $\Re[s]=\sigma_0$ is in the ROC, then any $s$ to	the right of $\sigma_0$ (i.e., $\Re[s]>\sigma_0$) is also in the ROC, i.e., ROC is a right sided half plane.
	
	\item If $x(t)$ is left sided and $\Re[s]=\sigma_0$ is in the ROC, then any $s$ to the left of $\sigma_0$ (i.e., $\Re[s]<\sigma_0$) is also in the ROC, i.e., ROC is a left sided half plane.
	
	\item If $x(t)$ is two-sided, then the ROC is the intersection of the two one-sided ROCs corresponding to the two one-sided parts of $x(t)$. This intersection can be either a vertical strip or an empty set.
	
	\item If $X(s)$ is rational, then its ROC does not contain any poles (by definition $X(s)|_{s=s_p}=\infty$ dose not exist). The ROC is bounded by the poles or extends to infinity.
	
	\item If $X(s)$ is a rational Laplace transform of a right sided function $x(t)$, then the ROC is the half plane to the right of the rightmost pole, and if $X(s)$ is a rational Laplace transform of a left sided function $x(t)$, then the ROC is the half plane to the left of the leftmost pole.
	
	\item A signal $x(t)$ is absolutely integrable, i.e., its Fourier transform $X(\mathrm{i}\omega)$ exists, if and only if the ROC of the corresponding Laplace transform $X(s)$ contains the imaginary axis $\Re[s]=0$ or $s=\mathrm{i}\omega$.
	
	\end{itemize}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Consider the Laplace transform of a two-sided signal:
		$x(t)=e^{-b|t|}$:
	
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The Laplace transform of the two components can be obtained from the two examples discussed earlier above. Then we get:
	
	and let $b=-a$,  we then have:
	
	Combining the two components, we have:
	
	Whether $X(s)$ exists or not depends on $b$. If $b>0$, i.e., $x(t)$ decays exponentially as $|t| \rightarrow +\infty$, then the ROC is the strip between $-b$ and $b$ and $X(s)$ exists. But if $b<0$, i.e., $x(t)$ grows exponentially as  $|t| \rightarrow +\infty$, then the ROC is an empty set and $X(s)$ does not exist.\\
	
	E2. Given the following Laplace transform, find the corresponding signal:
	
	Given the two poles $s_{p_1}=-1$ and $s_{p_2}$ of the expression, there are three associated ROCs: 
	\begin{itemize}
		\item The half plane to the right of the rightmost pole $s_{p_2}=-1$, with the corresponding right sided time function:
		
		\item The half plane to the left of the leftmost pole $s_{p_1}=-2$, with the corresponding left sided time function:
		
		\item The vertical strip between the two poles $-1 < \Re[s] < -2$, with the corresponding two sided time function:
		 
	\end{itemize}
	In particular, note that only the first ROC includes the $\mathrm{i}\omega$-axis and the corresponding time function has a Fourier transform. Fourier transform of the other two functions do not exist.
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Properties of Laplace Transform}\label{properties of Laplace Transform}\mbox{}\\\\ 
	The Laplace transform has a set of properties in parallel with that of the Fourier transform. The difference is that we need to pay special attention to the ROCs. In the following, we always assume:
	
	and:
	
	
	\begin{enumerate}
		\item[P1.] Linearity:
		
		While it is obvious that the ROC of the linear combination of $x(t)$ and $y(t)$ should be the intersection of the their individual ROCs $R_x \cap R_y$ in which both	$X(s)$ and $Y(s)$ exist, we note that in some cases when zero-pole cancellation occurs, the ROC of the linear combination could be larger than $R_x \cap R_y$, as  shown in the example below.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Assume:
		
		and:
		
		then:
		
		\end{tcolorbox}
		
		\item[P2.] Time Shifting:
		
		
		\item[P3.] Shifting in $s$-Domain:
		
		Note that the ROC is shifted by $s_0$, i.e., it is shifted vertically by $Im[s_0]$ (with no effect to ROC) and horizontally by $\Re[s_0]$. If $R_x$ is $\Re[s]>0$, the new ROC is $\Re[s+s_0]>0$, i.e., $\Re[s]>-\Re[s_0]$.
		
		\item[P4.] Time Scaling:
		
		Note that the ROC is horizontally scaled by $1/a$, which could be either positive ($a>0$) or negative ($a<0$) in which case both the signal $x(t)$ and the ROC of its Laplace transform are horizontally flipped. 
		
		\item[P5.] Conjugation:
		
		\begin{dem} 
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item[P6.] Convolution:
		
		Note that the ROC of the convolution could be larger than the intersection of $R_x$ and $R_y$, due to the possible pole-zero cancellation caused by the convolution, similar to the linearity property.
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Assume:
		
		then:
		
		\end{tcolorbox}
		
		\item[P7.] Differentiation in Time Domain:
		
		This can be proven by differentiating the inverse Laplace transform:
		
		Again, multiplying $X(s)$ by $s$ may cause pole-zero cancellation and therefore the resulting ROC may be larger than $R_x$. For example, when $x(t)=u(t)$ and $X(s)=1/s$ with $\Re[s]>0$ as the ROC, $\mathrm{d} x(t)/\mathrm{d}t=\delta(t)$ and with $sX(s)=1$ whose ROC is the entire $s$-plane. In general, we have:
		
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		The ROC of ${\cal L}[\delta(t)]=1$ is the entire s-plane, and we have:
		
		and more generally:
		
		\end{tcolorbox}
		
		\item[P8.] Differentiation in $s$-Domain:
		
		This can be proven by differentiating the Laplace transform:
		
		Repeat this process we get:
		
		
		\item[P9.] Integration in Time Domain:
		
		This can be proven by realizing that:
		
		and therefore by convolution property we have:
		
		Also note that as the ROC of ${\cal L}[u(t)]=1/s$ is the right half plane $\Re[s]>0$, the ROC of $X(s)/s$ is the intersection of the two individual ROCs $R_x \cap \{\Re[s]>0\}$, except if pole-zero cancellation occurs (when $x(t)=\mathrm{d}\delta(t)/\mathrm{d}t$ with $X(s)=s$) in which case the ROC is the entire $s$-plane.
	\end{enumerate}
	
	\paragraph{Usual Laplace transforms}\mbox{}\\\\
	As we always do in this book, let us see some common usual Laplace Transforms that are useful in some physics and finance well known problems (and also used as example in the MATLAB™ companion book):
	\begin{enumerate}
		\item $\delta(t)$, $\delta(t-\tau)$
		
		
		Moreover, due to time shifting property, we have:
		
		
		\item $u(t)$ (Heaviside function\index{Heaviside function}), $t\;u(t)$, $t^n\;u(t)$
		
		Due to the property of time domain integration, we have:
		
		Let us have another more explicit approach as the previous one may be a bit hard.... The function $u(t)$ (for recall it is the Heaviside function defined as $H(t)=0,\forall t<0$) for the usage of the unilateral Laplace transform as it non-null only for $t\geq 0$. Then we have:
		
		This integral converse only if $\sigma \geq 0$ Hence:
		
		
		Applying the $s$-domain differentiation property to the above, we have:
		
		Or more explicitly (it sometimes good to see different ways...!) using integration by part:
		
		By the first term we also see that we must have $\sigma>0$. Which gives us:
		
		
		And in general (useful especially in Mechanical Engineering even if so far we were able to avoid its usage):
		
		Indeed, using integration by parts:
		
		From the first term we also see that we must have $\sigma >0$. Hence it remains:
		
		In then comes under the condition $\sigma>0$:
		
		
		\item $e^{-at}u(t)$, $te^{-at}u(t)$
		
		Applying the $s$-domain shifting property to:
		
		we have:
		
		If you don't like using the shift property for that proof, here is a more explicit and long way to get the same result (we change the notation a bit to avoid any confusion):
		
		We see that the integral will converge if and only if:
		
		Therefore:
		

		Applying the same property to:
		
		we have:
		
		
		\item $e^{-\mathrm{i}\omega_0 t}u(t)$, $\sin(\omega_0 t)u(t)$, $\cos(\omega_0 t)u(t)$ 
		
		We could make it by integration by parts but it's quite long. The tick is letting $a=\pm \mathrm{i}\omega_0$ in:
		
		we get:
		
		and therefore:
		
		and:
		
		
		\item $t\;\cos(\omega_0 t)u(t)$, $t\;\sin(\omega_0 t)u(t)$ 
		
		Letting $a=\pm \mathrm{i}\omega_0$ in:
		
		we get:
		
		Furthermore we have:
		
		and:
		
		
		\item $e^{-at}\cos(\omega_0 t) u(t)$,  $e^{-at}\sin(\omega_0 t) u(t)$  
		
		Applying $s$-domain shifting property to:
		
		and:
		
		we get, respectively:
		
		and:
		
		
		\item \label{Laplace pair for finance and telegrapher equation}Let us see an important transform pair that we will find invaluable when we get to transients in transmission lines and in advanced quantitative finance\footnote{ Notice that the entire development that will follow is inspired from the excellent book: \textit{Transients for Electrical Engineers} \pageref{nahin2018transients}.}. Specifically, if we define the "\NewTerm{Gauss error function}\index{Gauss error function}\index{error function}":
		
		then we will derive here the pair of unilateral Laplace transform:
		
		\begin{dem}
		We will start by computing the Laplace transform of:
		
		How do we know we have to start from this? Because this function appears in the book \textit{Analytical Theory of Heat} (1822) of Joseph Fourier where he already did the job before us but in a different painful way\footnote{You can found it in the English translation of 1878 \cite{fourier2003analytical} at the page 384 for $\mathbb{R}^3$.}!
		
		In any case what we have is:
		
		Next, let us make the change of variable:
		
		and so:
		
		Therefore:
		
		With this our unilateral Laplace transforms becomes:
		
		Or:
		
		where:
		
		Now, concentrate on the integral, alone. Multiplying out the exponent
	of the integrand, we have:
		
		where:
		
		The integral $I(b)$, despite its perhaps complicated appearance, can be evaluated as follows.
		
		Differentiating  with respect to $b$ we get:
		
		Now, make the change of variable:
		
		which leads to:
		
		and so:
		
		Thus:
		
		which the primitive gives:
		
		where $c^{te}$ is some constant. We can determine it easily with (\SeeChapter{see section Statistics page \pageref{Gauss integral}}):
		
		the reader may have indeed recognized the Gauss integral here.
		
		So:
		
		And putting into:
		
		We get:
		
		And putting also back into:
		
		Which give us indeed the pair\label{laplace pair for telegrapher equation} (that will be useful to us for the study of the Telegrapher equation):
		
		Which is equivalent:
		
		Finally, recall that we have proved during our studies of main Laplace transform properties the (\SeeChapter{see section Analysis page \pageref{properties of Laplace Transform}}) following pair (integration in time domain):
		
		Therefore:
		
		In the integral above let us make the following change of variable:
		
		Therefore:
		
		and so:
		
		In the first integral in the square brackets we recognize the half on the Gauss integral. Therefore:
		
		In the second one we recognize the Gauss error function (see page \pageref{error function}), therefore:
		
		Therefore our pair:
		
		can be rewritten:
		
		and so, at last, we have the claimed pair of the beginning!
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
	
	\paragraph{Solving differential systems}\mbox{}\\\\
	 Let us see well known simple academic companion example of the application of the Laplace transform to deal with differential equations.
	 
	 Let us consider that a voltage $x(t)$ is applied as the input to a resistor $R$, a capacitor $C$ and an inductor $L$ connected in series (the case without the use of the Laplace transform is introduced in details in the section of Electrical Engineering page \pageref{rlc circuit}). The output $y(t)$ is the voltage across one of the three elements. The system can be described by a differential equation in time domain:
	
	or an algebraic equation in $s$-domain:
	
	and the overall impedance of the circuit is defined as the ratio between voltage $V(s)$ and the current $I(s)$:
	
	which is composed of the individual impedance of the three elements
	

	\begin{table}[H]
		\centering
		\begin{tabular}{l|c|c|c} \hline
			& resistor $R$ & capacitor $C$ & inductor $L$  \\ \hline
		time domain & $i=\frac{v}{R}$ & $i=\frac{1}{C}\frac{\mathrm{d}v}{\mathrm{d}t}$ & $v=\frac{1}{L}\frac{\mathrm{d}i}{\mathrm{d}t}$ 
		\\ \hline 
		$s$-domain    & $V_R=IR$ & $V_C=I/Cs$ & $V_L=IsL$	\\ \hline
		impedance $Z=V/I$   &    $R$   &   $1/sC$   &   $sL$    \\ \hline
		\end{tabular}
	\end{table}	
	If the output is the voltage across one of the three elements ($V_L$, $V_R$, or $V_C$), the transfer function $H(s)$ can be easily obtained by treating the series circuit as a voltage divider: 
	\begin{itemize}
		\item Output is voltage across the capacitor $v_C(t)$
		
		\item Output is voltage across the resistor $v_R(t)$
		
		\item Output is voltage across the inductor $v_L(t)$
		
	\end{itemize}
	If we define
	
	the common denominator of the transfer functions can be written in standard (canonical) form:
	
	with two roots
	
	and the transfer functions above can be written in standard forms:
	\begin{itemize}
		\item 
		
		with two poles $p_1, p_2$ and no zeros. 
		\item 
		
		with two poles $p_1, p_2$ and one zero at the origin. 
		\item 
		
		with two poles $p_1, p_2$ and two repeated zeros at the origin. 
	\end{itemize}
	The three transfer functions behave like low-pass, band-pass and high-pass filter, respectively. Moreover, when the common real part $-\zeta \omega_n$ of the two complex conjugate poles is small (i.e., $0<\zeta < 0.5$), there will be a narrow pass-band around $\omega=\omega_n$ for all three transfer functions. The magnitude and phase angle of the corresponding frequency response function $|H(\mathrm{i}\omega)|$ can be 
	qualitatively determined in the s-plane, as to be discussed later.
	
	\paragraph{Unilateral Laplace Transform}\mbox{}\\\\
	The Laplace transform so far discussed is the "bilateral Laplace transform" as it can be applied to left sided signals as well as right sided ones. Now we will consider the "\NewTerm{unilateral Laplace transform}\index{unilateral Laplace transform}\index{Laplace transform}" of an arbitrary signal $x(t)$ defined as:
	
	where for recall $\cal U$ and $u(t)$ are another typical notation of the Heaviside function.
	
	This definition always assumes $x(t)=0$ for $t<0$. When the unilateral Laplace transform is applied to find the transfer function $H(s)={\cal UL}[h(t)]$ of an LTI system, it is always assumed to be causal. And the ROC is always right sided in $s$-plane.
	
	By definition, the unilateral Laplace transform of any signal $x(t)=x(t)u(t)$ is identical to its bilateral Laplace transform. However, when $x(t) \ne x(t)u(t)$, the two Laplace transforms are different. 
	
	\label{properties of unilateral Laplace Transform}Unilateral Laplace Transform shares all the properties of bilateral Laplace transform, except some of the properties are expressed in different forms. Here we only consider the differentiation in time domain:
	
	\begin{dem}
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We can further get Laplace transform of higher order derivatives
	
	and in general:
	

	\StickyNote[2.5cm]{\LARGE To finish depending on donations}[6.5cm]
	
	\pagebreak
	\subsubsection{$\mathcal{Z}$-Transform}
	Laplace transforms (whose Fourier transform is for recall a special case) is applicable only for functions say ... "continuous" to make it simple.... But since the advent of computing the vast majority of functions (signals) are samples at a time interval $T_s$ such that the functions are actually discrete.

	From then on it becomes useful to define a transformation similar to the Laplace transform but in the discrete case! Thus, the Laplace transform becomes a special case of the $\mathcal{Z}$-transform when the sampling period $T_s$ tends to $0$.

	The discrete Fourier transform of a discrete signal $x[n]$ is defined for recall as:
	
	provided $x[n]$ is absolutely summable:
	
	Obviously some signals may not satisfy this condition and their Fourier transform do not exist. To overcome this difficulty, we can multiply the given $x[n]$ by an exponential function $e^{-\sigma n}$ so that $x[n]$ may be forced to be summable for certain values of the real parameter $\sigma$. Now the discrete time Fourier transform becomes:
	
	The result of this summation is a function of a complex variable defined as:
	
	This is the "\NewTerm{forward (bilateral) $\mathcal{Z}$-transform}\index{forward $\mathcal{Z}$-transform}" of the discrete signal $x[n]$:
	
	
	There is another way to introduce the $\mathcal{Z}$-transform that result to the same result but with different notation and that is more explicit. As it may help the reader to better understand, let us show that but by limiting ourselves to positive times only (the case of interest in engineering is the unilateral $\mathcal{Z}$-transform!).

So we know that a continuous function $f(t)t$ can be discretized (sampled) in a way to take at each time step $nT_s$ with $n\in \mathbb{N}$ the value of the function of interest, multiplied by a Dirac pulse on this same point such that:
	
	Now let us calculate the Laplace transform of this discretized function at the limit and using the fact that the $f(nT_e)$ are henceforth independent from the time and the linearity property of the Laplace transform:
	
	As we have shown above in our study of the usual Laplace transforms, we have:
	
	Then we have:
	
	That is usage as we have just seen before to write as following and define as the "\NewTerm{unilateral $\mathcal{Z}$-transform}" (on which we will come back later below):
	
	with:
	
	That's it for the second approach!
	
	Now, given the $\mathcal{Z}$-transform $X(z)$, the original time signal can be obtained by the inverse $\mathcal{Z}$-transform, which can be derived from the corresponding discrete Fourier transform. As shown above, we have:
	
	Now $x[n]e^{-\sigma n}$ can be obtained by the inverse Fourier transform:
	
	Multiplying both sides by $e^{\sigma n}$, we get:
	
	To represent the inverse transform in terms of $z$ (instead of $\omega$), we note:
	
	i.e.:
	
	and the "\NewTerm{inverse $\mathcal{Z}$-transform}\index{inverse $\mathcal{Z}$-transform}" can be obtained as:
	
	Note that the integral with respect to $\omega$ from $0$ to $2\pi$ becomes an integral with respect to $z=e^{\sigma+\mathrm{i}\omega}$ in the complex $z$-plane, along a circle with a fixed radius $e^\sigma$ and a varying angle $\omega$ from $0$ to $2\pi$. Now we have the $\mathcal{Z}$-transform pair:
	
	
	The forward and inverse $\mathcal{Z}$-transform pair can also be represented as:
	
	In particular, if we let $\sigma=0$, i.e., $z=e^{j\omega}$, then the $\mathcal{Z}$-transform becomes the discrete-time Fourier transform:
	
	This is the reason why sometimes the discrete Fourier spectrum is expressed as a function of $e^{\mathrm{i}\omega}$.
	
	
	Different from the discrete-time Fourier transform which converts a 1-D signal $x[n]$ in time domain to a 1-D complex spectrum $X(e^{\mathrm{i}\omega})$ in frequency domain, the $\mathcal{Z}$-transform $X(s)$ converts the 1-D signal $x[n]$ to a complex function defined over a 2-D complex plane, named the  "\NewTerm{$z$-plane}\index{$z$-plane}", represented in polar form by radius $|z|=|e^{\sigma+\mathrm{i}\omega}|=e^\sigma$ and angle 
	$\angle z=\angle(e^{\sigma+\mathrm{i}\omega})=\omega$. 
	
	In particular, when this 2-D function $X(z)=X(e^{\sigma+\mathrm{i}\omega})$ is evaluated along the unit circle $|z|=e^0=1$ corresponding to $\sigma=0$, it becomes a 1-D periodic function $X(e^{\mathrm{i}\omega})$, the discrete Fourier transform of $x[n]$. 
	
	\pagebreak
	\paragraph{Transfer function of LTI system}\mbox{}\\\\
	The output $y[n]$ of a discrete LTI (Linear Time-Invariant) system with input $x[n]$ can be found by convolution (see page \pageref{convolution}):
	
	where $h[n]$ is the impulse response function of the system. In 	particular, if the input is a complex exponential:
	
	then the output $y[n]$ can be found to be:
	
	This is the eigenequation with the complex exponential $x[n]=z^n=e^{sn}$ being the eigenfunction of any discrete LTI system, corresponding to its eigenvalue defined as:
	
	which is the $\mathcal{Z}$-transform of its impulse response $h[n]$, called the "\NewTerm{transfer function}\index{transfer function}" of the LTI system. In particular, when $\sigma=0$, i.e., $z=e^s=e^{\mathrm{i}\omega}$, the transfer function $H(z)$ becomes the frequency response function, the Fourier transform of the impulse response:
	
	
	\paragraph{Conformal mapping between $s$-plane to $z$-plane}\mbox{}\\\\
	The $s$-plane and the $z$-plane are related by a conformal mapping (ie. conserves the angles) specified by the analytic complex function:
	
	where :
	
	Even if, as far as i know, the is no important application of mapping, it is more a mathematical curiosity the may help some students to understand how the $s$-plane and $z$-plane are related together.
	
	The mapping is continuous, i.e., neighbouring points in $s$-plane are mapped to neighbouring points in $z$-plane and vice versa. Consider the mapping of these specific features: 
	\begin{itemize}
		\item The origin $s=0$ of $s$-plane is mapped to $z=e^0=1$ on the real axis in $z$-plane.
		\item Each vertical line $\Re[s]=\sigma_0$ in $s$-plane is mapped to a circle $|z|=e^{\sigma_0}$ centered about the origin in $z$-plane. In particular,
			\begin{itemize}
			\item Leftmost vertical line $\Re[s]=\sigma=-\infty$ is mapped as the origin $|z|=e^{-\infty}=0$
			\item Imaginary axis $\Re[s]=0$ is mapped as the unit circle $|z|=e^0=1$
			\item Rightmost vertical line $\Re[s]=\sigma={+\infty}$ is mapped as a circle of infinite radius $|z|=e^{{+\infty}}={+\infty}$.
			\end{itemize}
		\item Each horizontal line $\Im[s]=\mathrm{i}\omega_0$ in $s$-plane is mapped to $\angle{z}=\omega_0$, a ray from the origin in $z$-plane of angle $\omega_0$ with respect to the positive horizontal direction. 
		\item A right angle formed by a pair vertical and horizontal lines in $s$-plane is conserved by the mapping, as the corresponding circle and ray in $z$-plane also form a right angle (in fact any angle is conserved, an important property of the conformal mapping).
	\end{itemize}
	The infinite range $-\infty < \omega < {+\infty}$ for frequency $\omega$ along a vertical line $\Re[s]=\sigma_0$ in $s$-plane is mapped repeatedly to a finite range $0 \le \omega < 2\pi$ around a circle $|z|=e^{\sigma_0}$ in $z$-plane, corresponding to the conversion of a continuous signal $x(t)$ with non-periodic spectrum $X(\mathrm{i}\omega)$ for $-\infty < \omega < {+\infty}$ to a discrete signal $x[n]$ with periodic spectrum $X(e^{\mathrm{i}\omega})$ for $0 \le \omega < 2\pi$.
	
	\paragraph{Region of Convergence}\mbox{}\\\\
	Whether the $\mathcal{Z}$-transform $X(z)$ of a signal $x[n]$ exists depends on the complex variable $z=e^s$ as well as the signal itself. $X(z)$ exists if and only if the argument $z$ is inside the region of convergence (ROC) in the $z$-plane, which is composed of all $z$ values for the summation of the $\mathcal{Z}$-transform to converge. The ROC of the $\mathcal{Z}$-transform is determined by $|z|=|e^s|=e^{\sigma}$ (a circle), the magnitude of variable $z$, while the ROC for the Laplace transform is determined by $\sigma=\Re[s]$, (a vertical line), the real part of $s$. 
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Let us see our first $\mathcal{Z}$-transform with at the same time its corresponding region of convergence. For this purpose, let us first recall the geometric series (see page \pageref{sum of powers}):
	
	for $|x|<1$. The $\mathcal{Z}$-transform of the following right sided signal $x[n]=a^n u[n]$ is:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	If we put $a=1$, we simply get the $\mathcal{Z}$-transform of the Heaviside function (as a simple geometric series: $1+z^{-1}+z^{-2}+\ldots$\\
	
	For this summation to converge, i.e., for $X(z)$ to exist, it is necessary to have $| az^{-1} |<1$, i.e., the ROC is $|z| > |a|$. As a special case when $a=1$, $x[n]=u[n]$ and we have:
	
	E2. Let us see now a simple trivial example of an inverse of the following given $\mathcal{Z}$-transform:
	
	Comparing this with the definition of $\mathcal{Z}$-transform:
	
	we get:
	
	In general, we can use the time shifting property:
	
	to inverse transform the $X(z)$ given above to $x[n]$ directly.\\
	
	E3.  Let us determine the obvious inverse transform of:
	
	As we know that:
	
	which converges if the ROC is $|z|>|a|$, i.e., $|az^{-1}|<1$ then we get:
	
	\end{tcolorbox}
	
	\pagebreak
	\paragraph{Zeros and Poles of $\mathcal{Z}$-Transform}\mbox{}\\\\
	All $\mathcal{Z}$-transforms in the above examples are rational, i.e., they can be written as a ratio of polynomials of variable $z$ in the general form:
	
	where $N(z)$ is the numerator polynomial of order $M$ with roots $z_{z_k}, (k=1,2, \cdots, M)$, and $D(z)$ is the denominator polynomial of order $N$ with roots $z_{p_k}, (k=1,2, \cdots, N)$. In general, we assume the order of the numerator polynomial is lower than that of the denominator polynomial, i.e.,  $M < N$. If this is not the case, we can always expand $X(z)$ into multiple terms so that $M<N$ is true for each of terms.
	
	The "\NewTerm{zeros}\index{zeros}" and "\NewTerm{poles}\index{poles}" of a rational $X(z)=N(z)/D(z)$ are defined as:
	
	\textbf{Definitions (\#\mydef):} 
	\begin{enumerate}
		\item[D1.]  Each of the roots of the numerator polynomial $z_z$ for which:
		
		is a "\NewTerm{zero}" of $X(z)$.
	
	  	If the order of $D(z)$ exceeds that of $N(z)$ (i.e., $N>M$), then $X({+\infty})=0$, i.e., there is a zero at infinity:
	  	
	
		\item[D2.] Each of the roots of the denominator polynomial $z_p$ for which :
		
		is a "\NewTerm{pole}" of $X(z)$.
	
		If the order of $N(z)$ exceeds that of $D(z)$ (i.e., $M>N$), then $X({+\infty})={+\infty}$, i.e, there is a pole at infinity: 
	  
	\end{enumerate}
	Most essential behaviour properties of an LTI system can be obtained graphically from the ROC and the zeros and poles of its transfer function $H(z)$ on the $z$-plane.
	
	\paragraph{Properties of $\mathcal{Z}$-Transform}\mbox{}\\\\
	The $\mathcal{Z}$-transform has a set of properties in parallel with that of the Fourier transform (and Laplace transform). The difference is that we need to pay special attention to the ROCs. In the following, we always assume:
	
	and:
	
	
	\begin{itemize}
		\item Linearity:
		
		While it is obvious that the ROC of the linear combination of $x[n]$ and $y[n]$ should be the intersection of the their individual ROCs $R_x \cap R_y$ in which both $X(z)$ and $Y(z)$ exist, note that in some cases the ROC of the linear combination could be larger than $R_x \cap R_y$. For example, for both $x[n]=a^n u[n]$ and $y[n]=a^n u[n-1]$, the ROC is $|z|>|a|$, but the ROC of their difference $a^n u[n]-a^n u[n-1]=\delta[n]$ is the entire $z$-plane.
		
		\item Time shifting:
		
		\begin{dem}
		
		Define $m=n-n_0$, we have $n=m+n_0$ and:
		
		The new ROC is the same as the old one except the possible addition/deletion of the origin or infinity as the shift may change the duration of the signal.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		A slightly different approach lead us to a different result that is more explicit and by the way more useful especially in business applications (we change the notation a bit to be in conformity with the tradition in the financial field). First let us consider the special case (shift of one unit only in the positive direction\footnote{This is then a "unilateral $\mathcal{Z}$-transform" and as we will see it on page , the properties then differ slightly from the bilateral version.}):
		
		Repeating exactly the same type of procedure we get quite simply in the general way for $m>0$:
		
		
		\item Time Expansion (Scaling):
		
		
		The discrete signal $x[n]$ cannot be continuously scaled in time as $n$ has to be an integer (for a non-integer $n$ $x[n]$ is zero). Therefore $x[n/k]$ is defined as:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		If $x[n]$ is ramp:
		\begin{table}[H]
		\centering
		\begin{tabular}{c|cccccc} \hline
		 $n$ & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline 
		 $x[n]$ & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline 
		\end{tabular}
		\end{table}
		
		then the expanded version $x[n/2]$ is :
		\begin{table}[H]
		\centering
		\begin{tabular}{c|cccccc} \hline
		 $n$ & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
		 $n/2$ & 0.5 & 1 & 1.5 & 2 & 2.5 & 3 \\ \hline
		 $m$ &	     & 1 &     & 2 &     & 3 \\ \hline
		 $x[n/2]$ & 0 & 1 & 0 & 2 & 0 & 3 \\ \hline 
		\end{tabular}
		\end{table}
		
		where $m$ is the integer part of $n/k$.
		\end{tcolorbox} 
		
		\begin{dem}
		 The $\mathcal{Z}$-transform of such an expanded signal is:
		
		Note that the change of the summation index from $n$ to $m$ has no effect as the terms skipped are all zeros.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item Convolution:
		
		The ROC of the convolution could be larger than the intersection of $R_x$ and $R_y$, due to the possible pole-zero cancellation caused by the convolution.
		
		\item Time Difference:
		
		\begin{dem}
		
		Note that due to the additional zero $z=1$ and pole $z=0$, the resulting ROC is the same as $R_x$ except the possible deletion of $z=0$ caused by the added pole and/or addition of $z=1$ caused by the added zero which may cancel an existing pole.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item Time Accumulation:
			
		\begin{dem}
		The accumulation of $x[n]$ can be written as its convolution with $u[n]$:
		
		Applying the convolution property, we get:
		
		as ${\cal Z}[u[n]]=1/(1-z^{-1})$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item Time Reversal:
		
		\begin{dem}
		
		where $m=-n$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item Scaling in $z$-domain:
		
		
		\begin{dem}
		
		In particular, if $a=e^{j\omega_0}$, the above becomes:
		
		The multiplication by $e^{-\mathrm{i}\omega_0}$ to $z$ corresponds to a rotation by  angle $\omega_0$ in the $z$-plane, i.e., a frequency shift by $\omega_0$. The rotation is either clockwise ($\omega_0>0$) or counter clockwise ($\omega_0<0$) corresponding to, respectively, either a left-shift or a right shift in frequency domain. The property is essentially the same as the frequency shifting property of discrete Fourier transform.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright} 
		\end{dem}
		
		\item Conjugation:
		
		\begin{dem}
		Complex conjugate of the $\mathcal{Z}$-transform of $x[n]$ is
		
		Replacing $z$ by $z^*$, we get the desired result.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		
		\item Differentiation in $z$-domain:
		
		\begin{dem}
		
		i.e.:
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Taking derivative with respect to $z$ of the right side of:
		
		we get:
		
		Due to the  property of differentiation in $z$-domain, we have:
		
		Note that for a different ROC $|z|<|a|$, we have:
		
		\end{tcolorbox} 
	\end{itemize}
	
	\paragraph{Usual $\mathcal{Z}$-Transforms}\mbox{}\\\\
	\begin{enumerate}

		\item $\delta[n]$, $\delta[n-m]$
		
		Due to the time shifting property, we also have:
		
		
		\item $u[n]$, $a^n u[n]$, $n a^n u[n]$
		
		
		Due to the scaling in $z$-domain property, we have:
		
		Or for those that don't like using this property, we have simply using the previous result:
		
		Applying the property of differentiation in $z$-domain to the above, we have:
		
		
		\item $u[n-k]$

		Using the time shifting property that is for recall given by:
		
		we have:
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		A useful case for a simple business application that we will use later is:
		
		\end{tcolorbox}
		
		\item $e^{\pm jn\omega_0}u[n]$, $\cos[n\omega_0]u[n]$, $\sin[n\omega_0]u[n]$
		
		Applying the scaling in $z$-domain property to ${\cal Z}[u[n]]=1/(1-z^{-1})$, we have:
		
		and similarly, we have:
		
		Moreover, we have:
		
		Similarly we have:
		
		
		\item $r^n \cos[n\omega_0]u[n]$, $r^n \sin[n\omega_0]u[n]$
		
		Applying the $z$-domain scaling property to the above, we have:
		
		and:
		
	
	\end{enumerate}
	
	
	\paragraph{Unilateral $\mathcal{Z}$-Transform}\mbox{}\\\\
	The "\NewTerm{unilateral $\mathcal{Z}$-transform}\index{unilateral $\mathcal{Z}$-transform}" of an arbitrary signal $x[n]$ is defined as:
	
	where for recall $\cal U$ and $u[n]$ are another typical notation of the Heaviside function.
	
	Some of the properties of the unilateral $\mathcal{Z}$-transform that differ slightly from the bilateral $\mathcal{Z}$-transform are listed below:
	\begin{enumerate}
		\item[P1.] Time Advance:
		
		where we have assumed $m=n+1$.
		
		\item[P2.] Time Delay:
		
		where $m=n-1$. Similarly, we have:
		
		where $m=n-2$. In general, we have:
		
		
		\item[P3.] Convolution:
		
		If both $x[n]$ and $y[n]$ are causal, i.e., $x[n]=y[n]=0$ for $n<0$, the unilateral and bilateral $\mathcal{Z}$ transforms are identical.
		
		\item[P4.] Time Difference:
		
		\begin{dem}
		
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem} 
		
		
		\item[P5.] Time Accumulation:
		
		
		\item[P6.] Initial Value Theorem:
		
		If $x[n]=x[n]u[n]$, i.e., $x[n]=0$ for $n<0$, then:
		
		\begin{dem}
		
		All terms with $n>0$ become zero as $z^{-n}=1/z^n \rightarrow 0$ as 
		$z \rightarrow {+\infty}$, except the first one which is always $x[0]$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}		
		
		\item[P7.] Final Value Theorem:
		
		If $x[n]=x[n]u[n]$, i.e., $x[n]=0$ for $n<0$, then:
		
		\begin{dem}

		i.e.:
		
		Letting $z\rightarrow 1$ in the above, we get:
		
		where $x[-1]=0$.
		\begin{flushright}
			$\blacksquare$  Q.E.D.
		\end{flushright}
		\end{dem}
	\end{enumerate}
	
	
	\StickyNote[2.5cm]{\LARGE To finish depending on donations}[6.5cm]
	
	\pagebreak
	\subsubsection{Hilbert Transform}\label{hilbert transform}
	The information about the Hilbert transform is often scattered in most textbooks about signal processing. Their authors frequently use mathematical formulas without explaining them thoroughly to the reader at the opposite of the Fourier, Laplace or $\mathcal{Z}$-transform. Our purpose is to make a more stringent presentation of the Hilbert transform but still with the signal processing application in mind.
	
	\textbf{Definition (\#\mydef):} The "\NewTerm{Hilbert transform}\index{Hilbert transform}" of a function $f(t)$ is defined for all $t$ by:
	
	when the integral exists.
	
	It is normally not possible to calculated the Hilbert transform as an ordinary improper integral because of the pole at $\tau=t$. However, the $P$ in front of the integral denotes the "\NewTerm{Cauchy principal value}\index{Cauchy principal value}" which expanding the class of function for which the integral definition above exist. It can be defined as following:
	
	\textbf{Definition (\#\mydef):} The "\NewTerm{Cauchy principal value}\index{Cauchy principal value}" is a method for assigning values to certain improper integrals (\SeeChapter{see section Differential and Integral Calculus page \pageref{improper integral}}) which would otherwise be undefined. Depending on the type of singularity in the integrand $f$, the Cauchy principal value is defined according to the following rules:
	\begin{itemize}
		\item For a singularity at the finite number $b$:
		
		where $b$ is a point at which the behaviour of the function $f$ is such that:
		
	
		\item For a singularity at infinity:
		
		where:
		
	\end{itemize}
	In some cases it is necessary to deal simultaneously with singularities both at a finite number $b$ and at infinity. This is usually done by a limit of the form:
	
	The Cauchy principal value can also be defined in terms of contour integrals of a complex-valued function $f(z)$; $z = x + \mathrm{i}y$, with a pole on a contour $C$. Define $C(\varepsilon)$ to be the same contour where the portion inside the disk of radius $\varepsilon$ around the pole has been removed. Provided the function $f(z)$ is integrable over $C(\varepsilon)$ no matter how small $\varepsilon$ becomes, then the Cauchy principal value is the limit:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Consider the ill-defined expression:
	
	As the primitive of $1/x$ is equal to $\ln(x)$ there is obviously a problem... So the idea is to write instead:
	
	but as the $1/x$ function is odd, we can write this:
	
	that is obviously equal to $0$! So $0$ is the principal value of the initial integral.
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Sadly different authors use different notations for the Cauchy principal value of a function $f$, among others:
	
	\end{tcolorbox}

	\StickyNote[2.5cm]{\LARGE To finish depending on donations}[6.5cm]
	
	\pagebreak
	\subsection{Functional dot product (inner product)}\label{functional dot product}
	The "\NewTerm{functional dot product}\index{function dot product}\index{orthogonality of functions}\index{functions orthogonality}" (very strong analogy with the dot product in seen in the section Vector Calculus) may seem unnecessary when examined for the first time outside of an application context or only as generalization purpose, but in fact it has many practical applications. We will make such direct use in the section of Wave Quantum Physics and Quantum Chemistry, or even more important in the context of trigonometric polynomials through the Fourier series and transforms (\SeeChapter{see section Sequences and Series page \pageref{fourier series}}) that we find everywhere in contemporary physics and computer science.
	
	However, if the reader has not travelled the section of Vector Calculus and the part treating the vector dot product, we would highly recommend reading it otherwise what follows may be a little incomprehensible.
	
	We put ourselves in the space $\mathcal{C}([a,b],\mathbb{R})$ of continuous functions in the interval $[a, b]$ into $\mathbb{R}$ with the inner product defined by (we find here again the specific notation of the dot product in its functional - ie integral - version as we had mentioned during our definition of the vector dot product in the section of Vector Calculus):
	
	
	A family of orthogonal polynomials, as we can make the analogy with the dot product in the section Vector Calculus, is therefore a polynomial family $(p_0,...,p_n,...)$ such as:
	
	if $j \ne k$. We recall that an orthogonal family is a free family. We also saw in the section of Vector Calculus that in the space $\mathcal{C}([a,b],\mathbb{C})$ the only possible coherent choice was:
	
	We name the two previous relations "\NewTerm{$L^2$-dot product}\index{dot product!$L^2$-dot product}".
	Remember now that if we want to show something is an inner product, we need to show three things for all $f,g\in \mathbb{R}^2$ and $\alpha\in \mathbb{R}$:
	\begin{enumerate}
		\item Symmetry: $\langle f|g\rangle=\langle g|f\rangle$ (or, if the field is the complex numbers $\mathbb{C}^n$, $\langle f|g\rangle=\overline{\langle g|f\rangle}$, i.e. "conjugate symmetry.)
	
		\item Linearity: $\langle \alpha f|g\rangle=\alpha \langle f|g\rangle$. Notice this also implies $\langle f|\alpha g\rangle=\alpha \langle f|g\rangle$ ($\bar{\alpha}$ in the complex case) by symmetry
	
		\item Positive-definite: $\langle f,f\rangle\geq 0$ with equality if and only if $f=0$, the zero function
	\end{enumerate}
	The first two properties follow directly from the definition of an integral. For the third property, it's quite obvious (the integral of $f^2$ can be zero only if $f$ is zero).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If such a dot product exist, we can then the Cauchy-Schwarz inequality also applies to it. We name it the "\NewTerm{Cauchy-Schwarz inequality for integrals}\index{Cauchy-Schwarz inequality for integrals}".
	\end{tcolorbox}
	
	Therefore we can build the "\NewTerm{$L^2$-norm}\index{norm!$L^2$-norm}":
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We will see further below that the definition above is not the most general one as especially physicists and engineers say that functions are orthogonal under a more constraint situation!
	\end{tcolorbox}
	The development that will follow will remind us the Gram-Schmidt procedure (\SeeChapter{see section Vector Calculus page \pageref{gram-schmidt procedure}}) to build an orthogonal family.
	
	\begin{theorem}
	Given $(p_0,...,p_n,...)$ a family of linearly independent polynomial defined on $[a,b]$ and $V$ the vector space defined by this family. The family $(y_0,...,y_n,...)$ defined by recurrence in the following way:
	
	and $y_0=p_0$ is orthogonal and generates $V$.
	\end{theorem}
	\begin{dem}
	Let us show by induction on $n$ that $(y_0,y...,y_n,...)$ is an orthogonal family which generates the same space as $(p_0,...,p_n,...)$. The assertion holds for $n=0$. Let us suppose that the assertion holds for $n\geq 0$, for $0\leq k\leq n$ we have:
	
	$(y_0,...,y_n,...)$ is therefore orthogonal. Finally, the equality:	
	
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\addcontentsline{toc}{paragraph}{Orthogonality of trigonometric functions}
	Let us see a first example very important is signal processing and statistics that is relatively to frequency analysis:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider the very important example in modern physics that is the set of continuous $2\pi$-periodic function denoted $P_{2\pi}$ that forms a vector space (\SeeChapter{see section Set Theory page \pageref{vector space}}).\\
	
	We define the dot product of two functions of this set by:
	
	The aim of this definition is to build an abstract functional basis $P_{2\pi}$ on which we can break down any $2\pi$-periodic function!!!\\
	
	The simplest idea is then to use the trigonometric functions sine and cosine:
	
	The relations below show that the basis chosen above are orthogonal and therefore form a free family, plus it's a generating family of the vector space $P_{2\pi}$ because as we have seen in our study of Fourier series (\SeeChapter{see section Sequences and Series page \pageref{fourier series}}), we have the following values:
	
	where $\delta_{km}$ is the kronecker symbol (\SeeChapter{see section Tensor Calculus page \pageref{kronecker symbol}}).\\
	
	Therefore it is also an orthogonal basis but not orthonormal. If we want to normalized the vectors of the basis we just need obviously to take:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If the reader remembers that for a random variable $X$ defined on $\mathbb{R}$, the mean was calculated as (\SeeChapter{see section Statistics page \pageref{expected mean continuous variable}}):
	
	The we can assimilate:
	
	where:
	
	to the expected mean of the function $g(x)$! Analogy sometimes very useful in practice!
	\end{tcolorbox}
	\addcontentsline{toc}{paragraph}{Orthogonality of complex exponential functions}
	Let us see now another example that is an extension of the previous one and that has also a great importance in signal processing but also in quantum physics and quantum chemistry:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us consider a basis of complex functions of the form $r e^{\mathrm{i}n\varphi}$ with $n\in\mathbb{Z}$. We therefore can write:
	
	We get:
	
	It is obvious that if we take for basis functions of the type:
	
	then we have an orthonormal basis (and not just an orthogonal one).
	\end{tcolorbox}
	\addcontentsline{toc}{paragraph}{Orthogonality of Bessel functions}\label{orthogonality of bessel functions}
	Another example that will be useful for us in the section of Wave Mechanics for our study of the ideal circular membrane of a drum.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We have proved in the section Sequences and Series that the Bessel function\index{Bessel function} $J_p(x)$ satisfies the following differential equation (Bessel's equation):
	
	which can be written as:
	
	The variable $p$ need not be an integer as we will see it in the section of Mechanical Engineering with the study case of the self-buckling column.\\
	
	It turns out to be useful to define a new variable $t$ by $x = a t$, where $a$ is a constant which we will take to be a zero of $J_p$, i.e. $J_p(a) = 0$. Let us define:
	
	which implies:
	
	and substituting into (\ref{eq}) gives:
	
	since $x \mathrm{d}/\mathrm{d}x$ is equivalent to $t \mathrm{d}/\mathrm{d}t$.
	We can also write down the equation obtained by picking another zero, $b$. Defining:
	
	which implies:
	
	we have then:
	
	To derive the orthogonality relation, we multiply (\ref{eq1}) by $v$, and (\ref{eq2}) by $u$. Subtracting and dividing by $t$ gives:
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	The first two terms in (\ref{combine}) can be combined as:
	
	since the extra terms present in (\ref{totalderiv}), but not in (\ref{combine}), when the derivatives are expanded out are equal and opposite and so cancel. Hence we have:
	
	We next integrate this over the range of $t$ from $0$ to $1$ ($0$ since the Bessel function is not defined for $t<0$ and to $1$ since it's the place where there is a zero by construction for recall!), which gives:
	
	The integrated term vanishes at the lower limit because $t=0$, and it also vanishes at the upper limit because $u(1) = v(1) = 0$, see (\ref{u10}) and (\ref{v10}). Hence, if $a \ne b$, (\ref{int01}) gives:
	
	which, using (\ref{ut}) and (\ref{v10}), can be written
	
	This is the desired orthogonality equation. Remember we require that $a$ and $ b$ are distinct zeroes of $J_p$, so both Bessel functions in (\ref{orthog}) vanish at the upper limit.
	\end{tcolorbox}
	
	\pagebreak
	\addcontentsline{toc}{paragraph}{Orthogonality of Hermite polynomial}
	Another that will be useful for us in the section of Wave Quantum Physics during our study of the harmonic oscillator:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	We will introduce in the section of Wave Quantum Physics the following "physicist Hermite polynomial\index{Hermite polynomials}\label{hermite polynomial}":
	
	Therefore (see the plot in the section of Wave Quantum Physics):
	
	where we notice almost immediately that (useful for further below):
	
	And we need to prove that they are orthogonal (or even better: orthonormal!).\\
	
	For this purpose we introduce the weight function $w(x)=e^{-x^2}$ therefore:
	
	So we get (we use the Gauss integral as proved in the section Statistics page \pageref{Gauss integral}):
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	Finally, using Kronecker symbol (\SeeChapter{see section Tensor Calculus page \pageref{kronecker symbol}}):
	
	\end{tcolorbox}
	
	\addcontentsline{toc}{paragraph}{Orthogonality of Laguerre polynomial}\label{orthogonality of Laguerre polynomial}
	Now let us see a last example useful for our study of the non-rigid rotator in Quantum Chemistry (radial part solution of the hydrogenoid Schrödinger equation):
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If $L_{m}(x)$ and $L_{n}(x)$ are Laguerre's polynomials (\SeeChapter{see section page \pageref{Laguerre polynomials}}) $m,n$ being positive integers) then:
	
	where
	
	Proof: The generating function for Laguerre's polynomial gives:
	
	We now multiply both sides of the above by $e^{-x}$ and integrate both sides from $0$ to $+\infty$ with respect to $x$, which gives:
	
	Therefore:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
	When $m\neq n$, equating coefficients of $t^{n} s^{m}$ on both sides of the above relation gives:
	
	When $m=n$, equating coefficients of $t^{n} s^{n}$ from both sides of the prior previous relation gives:
	
	Combining we get:
	
	where:
	
	\end{tcolorbox}
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	\addcontentsline{toc}{paragraph}{Orthogonality of Legendre polynomial}We have already proved in the section of Calculus at page \pageref{legendre polynomials} that the Legendre polynomials (useful for our study of Quantum Chemistry) are orthogonal.
	\end{tcolorbox}
	From what we have seen above we deduce that:
	
	is in fact generalized by:
	
	where $w(x)$ is the "\NewTerm{weight function}\index{weight function}". 
	
	So the engineer, physicist, mathematician must always be careful when he see in a textbook a sentence of the type: \textit{these functions are orthogonal}. Indeed the author/redactor should instead read: \textit{these functions are orthogonal with a given weight}.
	
	\subsubsection{Cauchy-Schwarz inequality for integrals}\label{Cauchy-Schwarz inequality for integrals}
	Let us now provide a detailed proof of what we have mentioned earlier above: the 	"\NewTerm{Cauchy-Schwarz inequality for integrals}\index{Cauchy-Schwarz inequality for integrals}\label{Cauchy-Schwarz inequality for integrals}". As this will be an important result with quite important application in Statistics and Quantum Physics.
	
	\begin{theorem}
	Let $f$ and $g$ be real functions which are continuous on the closed interval $[a,b]$. Then:
	
	\end{theorem}
	\begin{dem}
	For the proof we use the following clever trick! We put $\forall x\in\mathbb{R}$:
	
	Hence using relative sizes of definite integrals:
	
	Using the linear combination of integrals property:
	
	Let us put:
	
	where:
	
	The quadratic equation $Ax^2+2Bx+C$ is non-negative for all $x$. It follows (using the same reasoning as in Cauchy's Inequality) that the discriminant $(2B)^2-4AC$ of this polynomial must be non-positive.
	
	Thus:
	
	and hence the result!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{100} & \pbox{20cm}{\score{4}{5} \\ {\tiny 47 votes,  71.49\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Complex Analysis}\label{complex analysis}

	\lettrine[lines=4]{\color{BrickRed}B}efore starting this section on the study of differential and integral calculus in the generalized case of complex numbers, I should point out that I used many illustrations inspired by the PDF of E.~Hairer (with his permission). This text also contains many sentences and developments taken, homogenized and simplified from the same PDF (at the risk to make some purist readers climbing to the curtains...) according to the notations and educational objectives of the rest of this book.

	The subject of the complex analysis is the study of functions $\mathbb{C} \mapsto \mathbb{C}$ and their differentiability (which is different from that in $\mathbb{R}^2$). The "holomorphic functions" (that is to say differentiable in a subset of $\mathbb{C}$) have as we will see it later surprising and elegant properties that can be reused in the situation of the special case of functions in $\mathbb{R}^2$ (remember that $\mathbb{C}$ is a generalization of $\mathbb{R}^2$ ) that have important applications in advanced physics (we will use the results of this section for our study of quantum physics and also for some applications of fluid mechanics and also for advanced models in financial options pricing).

	Before we start let us first explain the interest of Complex Analysis in a simplified way!

	We studied in the chapter Algebra a part of the Differential and Integral Calculus with some useful and important theorems in physics and engineering. However, staying in $\mathbb{R}$ or $\mathbb{R}^2$ the list of theorems runs out somehow and we end up finding much relevant tools in practice that allows to simplify the integration calculation that we can sometimes found in industrial applications. So, when we remember that $\mathbb{R} \subset \mathbb{C}$ (thus the set of complex numbers generalizes the set of real numbers) and that we can also build a correspondence $\mathbb{R}^2 \mapsto \mathbb{C}$ as we shall see, then new theorems appear with very interesting results that can be exploit for the integrals in $\mathbb{R}$ or $\mathbb{R}^2$!! It is because of this reason that the engineer needs to know Complex Analysis!
	
	After studying this particular field of mathematics, it is common to say that the shortest path between two truths of the real domain often requires to pass trough the complex domain...

	\subsection{Linear Applications}

	A good introduction to complex analysis and its representation is to look at first (for educational purposes mainly) the special case of complex linear applications. Let us see this!

Let $U \subset \mathbb{C}$ be a set and $V \subset \mathbb{C}$ another set. A function that associates to each $z \in U$ an $w \in V$ such that:
	
is a "\NewTerm{complex function}\index{complex function}":
	
What is important is to remember (\SeeChapter{see section Numbers page \pageref{complex numbers}}) that we can identify:
	
and:
	
We have thus two functions of two real variables $x, y$:
	
which are the coordinates of the point $w$.

\textbf{Definition (\#\mydef):} An application is named "\NewTerm{$\mathbb{C}$-linear}\index{$\mathbb{C}$-linear function}" if for example a function of the type:
	
where $c$ is a fixed complex number and $z$ an arbitrary complex number, satisfying:
	

That is to say that $f(z)$ must me additive and homogeneous or just briefly when this two properties are satisfied we say that $f(z)$ is a "\NewTerm{linear map}\index{linear map}".

We have seen and proved in the section on Numbers during our study of complex numbers, that the multiplication of two complex numbers could be equivalent to an orthogonal rotation followed by a scaling and that this same multiplication could be represented in matrix form! Or the transcription into a matrix form involves as we saw in the section on Linear Algebra automatically  the property of linearity!

So the reader can easily check that a matrix of rotation/scaling is an example of an $\mathbb{C}$-linear application (on request we can detail) that we will now write:
	
Which can be typically represented as follows (we can clearly see a rotation and a scaling which conserve the angles and proportions):

\begin{figure}[H]
\centering
\includegraphics[scale=0.75]{img/analysis/clinear_application.eps}
\caption{$\mathbb{C}$-linear application example}
\end{figure}

It is the fact that the proportions and the angles are kept that makes a complex function $\mathbb{C}$-linear. Otherwise, we would say that the function is $\mathbb{R}$-linear.

So a matrix equation is $\mathbb{C}$-linear if and only if it is of the form:
	
Let us see some examples of quite remarkable $\mathbb{C}$ non-linear functions.

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
E1. Consider the function:
	
In real coordinates this gives:
		
So let's look what this function do with the points of the complex plane which are coincident with the vertical lines of this same plane (which take us to write $x=a$). Then we have:
	
	and eliminating y, we find the equation of a parabola or rather a family of  parabolas (for several values of $b$) which are open to the left of the pictured complex plane.\\

	Here is a picture representation of the complex plane on which we have drawn a cat head:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_image_cat.eps}
		\end{center}	
		\caption{Complex representation of the image of the example function}
	\end{figure}
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
and if we look at the corresponding pre-image complex plane  then we have two heads of cats that appear:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_pre_image_cat.eps}
		\end{center}	
		\caption{Pre-image representation of the example function}
	\end{figure}
The appearance of these two heads of cats is that this function has 2 possible pre-images for each image point (so it is a surjective function - see section Set Theory page \pageref{surjective application}).\\

Here is a nice Maple 17.00 script by Carl Ebehart to check this (shame that this can not be done in an easier way in Maple):\\

\texttt{> complextools[gridimage] := proc(p)\\
local llhc, width, height, xres, yres, clrs, V, H, i,j,k,l,pz,x,y,z,f,g,xtcs,ytcs,opts,margs;\\
llhc := [-1, -1];\\
width := 2;height := 2;\\
xres := .25;yres := .25;\\
xtcs := 1; ytcs := 1;\\
clrs := [red, black];\\
opts := NULL;\\
opts := op(select(type,[args],`=`));\\
margs:= remove(type, [args] ,`=`) ;\\
if nops(margs) >1 and margs[2] <> `` then llhc := margs[2] fi:\\
if nops(margs) >2 and margs[3] <> `` then width := margs[3] fi:\\
if nops(margs) >3 and margs[4] <> `` then height := margs[4] fi:\\
if nops(margs) >4 and margs[5] <> `` then xres := margs[5] fi:\\
if nops(margs) >5 and margs[6] <> `` then yres := margs[6] fi:\\
if nops(margs) >6 and margs[7] <> `` then xtcs := margs[7] fi:\\
if nops(margs) >7 and margs[8] <> `` then ytcs := margs[8] fi:}
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\texttt{if nops(margs) >8 and margs[9] <> `` then clrs := margs[9] fi:\\
z:= x + I*y;\\
pz := evalc(p(z));\\
f := unapply(evalc( Re(pz)),x,y); g := unapply(evalc(Im(pz)),x,y);\\
V:= plot( [
seq([seq(op([[f(llhc[1] + i*xres ,llhc[2]+(j-1)*yres/ytcs),g(llhc[1] + i*xres ,llhc[2]+(j-1)*yres/ytcs)], [f(llhc[1] + i*xres , llhc[2] +j*yres/ytcs),g(llhc[1] + i*xres , llhc[2] +j*yres/ytcs)]]),
j=1..ytcs*height/yres)], i = 0 .. width/xres)
],color=clrs[1]);\\
H := plot( [
seq([seq(op([[f(llhc[1]+(j-1)*xres/xtcs,llhc[2] + i*yres),
g(llhc[1]+(j-1)*xres/xtcs,llhc[2] + i*yres)], 
[f(llhc[1] +j*xres/xtcs, llhc[2] + i*yres),
g(llhc[1] +j*xres/xtcs, llhc[2] + i*yres)]]),
j=1..xtcs*width/xres)], i = 0 .. height/yres)
],color=clrs[2]);\\
plots[display]([V,H],scaling=constrained,opts);
end:\\
with(complextools);}\\

\texttt{>plots[display]([seq(plots[display]([gridimage(z->z), gridimage(z->z\string^2)]),i=10)],insequence=true);}
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_maple_transform.eps}
		\end{center}	
		\caption{Practical Maple 17.00 example of simple $C$-linear application}
	\end{figure}
	\end{tcolorbox}
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
E2. Another interesting feature is the "\NewTerm{Cayley transformation}\index{Cayley transformation}" used in some areas of physics and defined as:
	
having as domain definition: $\mathbb{C}/\left\lbrace 1\right\rbrace$.\\

	We notice that this is an involutive function since:
	
	and as we have proved in the section of Proofs Theory that any involution function is both injective and surjective, then the Cayley transform is a bijective function.\\

This function transforms the imaginary axis $\mi y$ in unit circle (and vice versa as it is involutive). Let us see that:
	
where:
	
satisfies:
	
That is to say:
	
This is the equation of a circle as proven in the section of Analytical Geometric.
	\end{tcolorbox}

\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	E3. As another example of function, consider the "\NewTerm{Joukovski transformation}\index{Joukovski transformation}" defined by:
	
If the definition domain is built in polar coordinates look at how a circle or ellipse transforms with this function:

	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/joukovski_pre_image.eps}
		\end{center}	
		\caption{Transformation into polar coordinates of an ellipse with the example function}
	\end{figure}
Then the image plane will be:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.5]{img/analysis/joukovski_image.eps}
		\end{center}	
		\caption{Result of the Joukovski transformation in polar coordinates}
	\end{figure}
It thus transforms the circles respectively centered at $0$ and the rays passing through $0$ into a family cofocal ellipses and hyperbole . To prove this fact, we use the complex polar coordinates (Euler formula) seen in the section on Numbers (\SeeChapter{see chapter Arithmetics page \pageref{euler formula}}):
	\end{tcolorbox}

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	
and:
	
Then we have:
	
therefore:
	
and we immediately see that (\SeeChapter{see section Trigonometry page \pageref{remarkable trigonometric identities}}):
	
has the form of the equation of an ellipse (\SeeChapter{see section Analytical Geometry page \pageref{analytical expression ellipse}}) and we also have:
	
	which is the equation of a hyperbola (\SeeChapter{see section Analytical Geometry page \pageref{hyperbola}}).\\

	This function is useful in case we cleverly place a circle through the point $z=1$ (as in the case of the first figure) the plan represented in polar coordinates with a dotted line might looks like an airplane wing. This allowed a time ago in aerodynamics (but the technique is obsolete today)  to transpose the study of a vector field of an airplane wing profile to the study of a circle profile and to do after the Joukovski transformation.
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
Indeed, let us see a part of this still with Maple 4.00:\\

\texttt{> assume(x,real,y,real);}\\
\texttt{> z:=x+I*y;}\\
\texttt{> F:=1/2*(z+1/z);}\\
\texttt{> u:=Re(F);}\\
\texttt{> u:=evalc(u);}\\
\texttt{> v:=Im(F);}
\texttt{> v:=evalc(v);}\\
\texttt{> with(plots):with(plottools):}\\
\texttt{> p1:=disk([0,0],1,color=black):}\\
\texttt{> p2:=implicitplot({seq(v=b8,b=-10..10)},x=-4..4,y=-2..2,color=black):}\\
\texttt{> display([p2,p1],scaling=constrained);}\\

We thus get:

	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.5]{img/analysis/joukovski_application.eps}
		\end{center}	
		\caption{Important application example of the Joukovski function}
	\end{figure}

	\end{tcolorbox}
	Let us see a last example that shows an electric dipole with its electric field and potential lines (\SeeChapter{see section Electrostatics page \pageref{equipotentials}}) can bee seen as the emergence of the $\mathbb{C}$-linear function $1/z$:

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
\textbf{{\Large \ding{45}}Examples:}\\\\
E4. Always with Maple 4.00b we write:\\

\texttt{>assume(x,real,y,real);}\\
\texttt{> z:=x+I*y;}\\
\texttt{> F:=1/z;}\\
\texttt{> u:=Re(F);u:=evalc(u);}\\
\texttt{> v:=Im(F);v:=evalc(v);}\\
\texttt{> with(plots):}\\
\texttt{> p1:=implicitplot({seq(u=a,a=-5..5)},x=-1..1,y=-1..1,numpoints=1000):}\\
\texttt{> p2:=implicitplot({seq(v=b,b=-5..5)},x=-1..1,y=-1..1,numpoints=1000,}\\
\texttt{color=green):}\\
\texttt{> display([p1,p2],scaling=constrained);}\\
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.5]{img/analysis/dipole.eps}
		\end{center}	
		\caption{Another important application of a complex application}
	\end{figure}
	\end{tcolorbox}
	
	\subsection{Holomorphic Functions}\label{holomorphic functions}
	The definition of the derivative with respect to a complex variable is naturally formally identical to the derivative with respect to a real variable.
	
	We then have, if the function $f(z)$ is differentiable in $z_0$:
	
	and we say (abusively in this book) that function is "\NewTerm{holomorphic}\index{holomorphic function}" (while in $\mathbb{R}$ we say "differentiable") or "\NewTerm{analytical}\index{analytical function}" in its domain or in a subset of it if it is differentiable at any point.
	
	In other words a holomorphic functions is a complex-valued function of one or more complex variables that is complex differentiable in a neighbourhood of every point in its domain. The existence of a complex derivative in a neighbourhood is a very strong condition, for it implies that any holomorphic function is actually infinitely differentiable and equal to its own Taylor series.
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} A complex function is derived like a real function, we just have to put $z$ as being $x$... at the condition of what we will see in what follows is respected!\\
	
	\textbf{R2.} In fact if the function is holomorphic in a subset of the complex plane, we will see a little further below in our study of the convergence of power series that this is than always an open subset.
	\end{tcolorbox}
	Equivalently, we say that the function $f$ is $\mathbb{C}$-differentiable if the following limit exists in  $\mathbb{C}$:
	
	Let us now present and prove a central theorem for complex analysis named "\NewTerm{Cauchy-Riemann theorem}\index{Cauchy-Riemann theorem}"!
	
	If the function:
	
	is $\mathbb{C}$-differentiable on $z_0=x_0+\mathrm{i}y_0$, then we have:
	
	which is somewhat the equivalent to the Schwarz theorem (limited to $\mathbb{R}$) proved in the section of Differential and Integral Calculus. The above two relations are named "Cauchy conditions". So these are the two conditions that must verify a complex function to be differentiable on $z_0$. Thus, it is possible to use these relations to examine the points where the function is not analytic.
	
	\begin{theorem}
	If these conditions are satisfied (what will prove right below), then we deduce that $u$ and $v$ must both harmonic functions of $x$ and $y$.
	\end{theorem}
	\begin{dem}
	As:
	
	by choosing:
	
	with $x \in \mathbb{R}$ we get:
	
	and as $x$ approaches a small value $\mathrm{d}x$, we have (\SeeChapter{see section Differential and Integral Calculus page \pageref{differential calculus}}):
	
	by choosing:
	
	with $y \in \mathrm{R}$, we get:
	
	and when $y$ tends to a small value we have (\SeeChapter{see section Differential and Integral Calculus page \pageref{differential calculus}}):
	
	So now we have:
	
	But remember we proved in the section of Integral and Differential Calculus the following theorem:
	
	Therefore:
	
	Therefore using directly Schwartz theorem:
	
	Which can be written:
	
	A trivial solution is obviously to have:
	
	Therefore the right to write:
	
	By identifying real and imaginary parts, we finish the proof!
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	So for $f$ to be differentiable in the complex domain $\mathbb{C}$ (holomorphic) at a point, it is sufficient that it be differentiable as a function of two real variables ($\mathbb{R}^2$-differentiable on $(x_0,y_0)$) and that its partial first derivatives at this point satisfy the Cauchy-Riemann equations.
	
	But, for it to be $\mathbb{C}$-differentiable, Cauchy-Riemann's equations must valid at all points of the complex plane (we sometimes speaks about "\NewTerm{complete functions}\index{complete functions}") and not only in a subdomain thereof! Otherwise, it contains therefore "\NewTerm{singularities}\index{singularities}" and we then speak of "\NewTerm{meromorphic function}\index{meromorphic function}\label{meromorphic function}" (which is therefore a holomorphic function excepted on singularities points).
	
	The Gamma function studied in the section of Differential and Integral Calculus (see page \pageref{gamma euler function}) is such a well-known function:
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/analysis/gamma_meromorphic.jpg}
		\caption[Gamma function is meromorphic in the whole complex plane]{Gamma function is meromorphic in the whole complex plane (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Geometrically, we will prove later that a holomorphic function has a possible interpretation in the sense that it is a conformal transformation (angles conservation).
	\end{tcolorbox}
	
	Notice therefore that if $f (z)$ is $\mathbb{C}$-differentiable it can be developed as Taylor series (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}):
	
	Note an important thing too. If we rewrite:
	
	as following:
	
	Then we say that $f$ is "\NewTerm{irrotational}\index{irrotational}" (\SeeChapter{see section Vector Calculus page \pageref{irrotational}}) since the first relation can be seen as:
	
	which is an important analogy! Finally, the second equation:
	
	also let us say by analogy (but it stops at a simple analogy!) that the bivariate function $f$ is non-divergent (\SeeChapter{see section Vector Calculus page \pageref{divergence vector field}}) what is good mnemonic way to remember this equation.
	
	Let's also show something else in evidence. If we take the two Cauchy-Riemann equations:
	
	and that we derivate them once again we get:
	
	and that we sum these two relations, we get then:
	
	It is the same with v. Then we have:
	
	And we know very well this form of equations (Maxwell-Poisson equation in the section of Electrodynamics and Newton-Poisson equation in the section of Astronomy...). This is a wave equation also named "\NewTerm{Laplace equation}\index{Laplace equation}" (nothing to do with that of the same name seen in our study of the hydrostatic!) and given by the scalar Laplacian (\SeeChapter{see section Vector Calculus page \pageref{scalar laplacian}}):
	
	Then it is traditional to say that $u$ is harmonic and of course we can get the same result with $v$! Well obviously ... we knew it, since we have already studied in the section Numbers that the real and imaginary parts of a complex number could be put in trigonometric form.

	Thanks to this discovery, Riemann opened the application of holomorphic functions in many problems of physics, since these equations are satisfied by the gravitational potential (Newton-Poisson equation in the section of Astronomy page \pageref{newton-poisson equation}) by electric and magnetic fields (Maxwell-Poisson equation in the section of Electrodynamics page \pageref{maxwell-poisson equation}) by heat balance (no examples yet in this book) and by movements without rotational of certain fluids (no examples either in this book yet).

	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The potential of a dipole can be described by the following holomorphic function:
	
	The figure below:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/holomorphic_dipole_plot.jpg}
		\caption{Plane representation of a well known holomorphic function...}
	\end{figure}
	shows level-curves (iso-curves) of the given harmonic functions $u (x, y)$ and $v (x, y)$ as real and complex parts of the function $f (z)$ of this example.
	\end{tcolorbox}
	
	
	\pagebreak
	\subsubsection{Orthogonality of real and imaginary iso-curves}
	We will now prove a friendly property that have the functions that satisfy the Cauchy conditions (i.e. that are analytic functions!). Indeed, remember that we have already seen above the function:
	
	which gave the following diagram:
	\begin{figure}[H]
		\begin{center}
			\includegraphics[scale=0.75]{img/analysis/c_linear_image_cat.eps}
		\end{center}	
		\caption{Reminder of plane representation of a complex function seen earlier}
	\end{figure}
	
	\begin{theorem}
	Well he functions satisfying the conditions Cauchy have the simple following geometrical property following: the lines whose real part of the function is constant $\mathcal{R}(f(z))=c^{te}$ and lines whose imaginary part is constant $\mathcal{I}(f(z))=c^{te}$  are orthogonal to each other (think to the trigonometric form of complex numbers it helps to better visualize!).
	
	In other words, the analytical complex functions are transformation functions of an area of the plane into a new plane where the angles are preserved. Then we say that the function is a "\NewTerm{complete transformation}\index{complete transformation}".
	\end{theorem}
	
	\begin{dem}
	For the proof remember that we have proved in section of Vector Calculus that  gradient of a function $f$ of $\mathbb{R}^2$ is given by:
	
	and as part of our study of isolines in the section of Differential Geometry that the tangent vector to isolines of the function $f$ will always be parallel to the vector of the plane:
	
	and that the latter two vectors are perpendicular, such that:
	
	Now assimilate the tangent (parallel) vector $\vec{t}_u$ to the real isolines:
	
	with:
	
	and the normal vector to the imaginary isolines:
	
	with the gradient $v$ of components:
	
	Using the Cauchy conditions proved above, we have for this last relation:
	
	By comparing:
	
	we therefore see that $\vec{t}_u$ and $\vec{\nabla}(v)$ are parallel (collinear). And since $\vec{t}_u$ is colinear the real isolines and that $\vec{\nabla}(v)$  is perpendicular to the imaginary isolines we finished our proof.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	The reader may take as an example the function:
	
	mathematically and schematically detailed earlier above! But to change a little bit, consider an example that will accompany us throughout the rest of this section and that is the following holomorphic function:
	
	That gives us with Maple 4.00b:
	
	\texttt{
	>assume(x,real,y,real);\\
	> z:=1/(1+(x+I*y)\string^2);\\
	> F:=1/z;\\
	> u:=Re(F);\\
	> u:=evalc(u);\\
	> v:=Im(F);\\
	> v:=evalc(v);\\
	> with(plots):\\
	> p1:=implicitplot({seq(u=a,a=-5..5)},x=-5..5,y=-5..5,numpoints=1000):\\
	> p2:=implicitplot({seq(v=b,b=-5..5)},x=-5..5,y=-5..5,numpoints=1000,color=green):\\
	> display([p1,p2]);
	}
	
	which gives:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/holomorphic_isoclines.jpg}
		\end{center}	
		\caption{Representation of an important holomorphic function with its isolines}
	\end{figure}
	
	\subsection{Complex Logarithm}
	We need for all functions built into $\mathbb{R}$ found their equivalent in $\mathbb{C}$ while knowing that if we reduce the case of $\mathbb{R}$ to $\mathbb{C}$ we must get back on our feet!
	
	To do this, let us start with the most classical and academic function which is the logarithm and also the only one function for which we will need the complex version in other sections of this book. As always we will focus only on the properties that we will need later for practical applications and nothing more!
	
	In the same way that we built the logarithm as being by definition by the inverse function of the natural exponential $e^x$ in the section of Functional Analysis, we first start from:
	
	where $z$ is a complex number and we will define the complex logarithm that must be reduced to the natural logarithm if $z$ has no imaginary part!
	
	So by definition the complex logarithm will be:
	
	and in this entire book, the complex logarithm will be differentiated by the real logarithm by a capital L for the first letter!
	
	Let us write $z$ and $w$ in the Euler form as viewed in the section Numbers:
	
	Then we have:
	
	By correspondence, we find immediately
	
	with $k \in \mathbb{Z}$. Therefore we get:
	
	Therefore:
	
	or more explicitly:
	
	So if $w$ has no imaginary part, we fall back on our feet since $\text{arg} (w)$ becomes zero.
	
	A big difference is highlighted between the logarithm of the complex and real numbers: the complex numbers logarithms can take several values because of the argument!!
	
	For a function to have an inverse, it must map distinct values to distinct values, i.e., be injective. But the complex exponential function is not injective, because $e^{z+2\pi \mathrm{i} }= e^z$ for any $z$, since adding $\mathrm{i}\theta$ to $w$ has the effect of rotating $e^z$ counter-clockwise $\theta$ radians. So all the points of the form $z+\mathrm{i}k\theta$  are all mapped to the same number by the exponential function. So the exponential function does not have an inverse function in the standard sense.
	
	There are two solutions to this problem.
	
	\begin{enumerate}
		\item One is to restrict the domain of the exponential function to a region that does not contain any two numbers differing by an integer multiple of $2\pi \mathrm{i}$: this leads naturally to the definition of "\NewTerm{branches}\index{branches}" of $\text{Log}(w)$, which are certain functions that single out one logarithm of each number in their domains.
		
		\item Another way to resolve the indeterminacy is to view the logarithm as a function whose domain is not a region in the complex plane, but a "Riemann surface" that covers the punctured complex plane in an infinite-to-$1$ way.
	\end{enumerate}
	Branches have the advantage that they can be evaluated at complex numbers. On the other hand, the function on the Riemann surface is elegant in that it packages together all branches of $\text{Log}(w)$ and does not require an arbitrary choice as part of its definition.
	
	We can see this with Maple 4.00b easily:
	
	\texttt{>plot3d([r*cos(f),r*sin(f),f],r=0..1,f=-2*Pi..2*Pi,axes=boxed,style=patch,\\
	shading=ZHUE);}
	
	which gives:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/complex_logarithm.jpg}
		\end{center}	
		\caption{Complex Logarithm plot with Maple 4.00b}
	\end{figure}
	
	For this reason, one cannot always apply $\text{Log}$ to both sides of an identity $e^{z_1}=e^{z_2}$ to deduce $z_1=z_2$ . Also, the identity $\text{Log} (z_1z_2)= \text{Log}(z_1) + \text{Log}(z_2)$ can fail: the two sides can differ by an integer multiple of $2\pi \mathrm{i}$.
	
	For each non-zero complex number $w = x + i\mathrm{y}$, the principal value $\text{Log}(w)$ is the logarithm whose imaginary part lies in the interval $[-\pi,+\pi]$. The expression $\text{Log}(0)$ is left undefined since there is no complex number $z$ satisfying $e^z = 0$.
	
	Then the principal value of the complex logarithm can be defined by (\SeeChapter{see section Trigonometry page \pageref{trigonometry}}):
	
	We see also obviously that the function $\text{Log}(w)$ is discontinuous at each negative real number (we can see it on the figure above), but continuous everywhere else in $\mathbb{C}^*$.
	
	Riemann surfaces\index{riemann surfaces} can be thought of as deformed versions of the complex plane: locally near every point they look like patches of the complex plane, but the global topology can be quite different. For example, they can look like a sphere or a torus or a couple of sheets glued together.
	
	The main point of Riemann surfaces is that holomorphic functions may be defined between them. Riemann surfaces are nowadays considered the natural setting for studying the global behaviour of these functions, especially multi-valued functions such as the square root and other algebraic functions, or the logarithm.
	
	Basically, a Riemann surface is simply just a surface, as far as the shape is concerned. Any normal surface you can think of (for example, plane, sphere, torus, ...) are all Riemann surfaces. We say it's Riemann surface, is due to the context, is that we define the surface using complex functions, and for use in studying complex functions.
	
	\subsection{Complex Integral Calculus}
	We have seen just above how to check if a complex function $f (z)$ was differentiable (it must at least respect the Cauchy-Riemann equations) at any point.
	
	Now let us see the opposite case... the integration that is absolutely fascinating in complex plane!
	
	We have obviously taking again the notations of the section of Differential and Integral Calculus:
	
	either in explicit form:
	
	Well once this expression established, let us give a little explanation about how to read it:
	\begin{enumerate}
		\item We know that $u$ and $v$ are dependent both in the general case of $x$ and $y$.
		
		\item We know that $ u $ and $ v $ represent (see examples at the beginning of this section) closed or open curves and also straight lines when $ x $ (or respectively $y$) is fixed and that the other associated variable varies!
	\end{enumerate}
	So each term have an integral in the above expression is in fact a line integral on a family of open or closed curves (including a specific case that is straight lines...)!
	
	This integral can be evaluated using the Green's theorem in the plane (\SeeChapter{see section Vector Calculus page \pageref{green theorem}}) if we consider the particular case of a closed curvilinear path such as:
	
	Let us first study the real part:
	
	Indeed we proved (it is strongly advised to read again this Green's theorem) in the section of Vector Calculus that:
	
	What will be written in our situation:
	
	However, if the function is holomorphic and thus satisfies the Cauchy-Riemann equations we get immediately:
	
	Thus our integral is reduced in the particular case of a closed path:
	
	and... reusing Green's theorem for this imaginary part:
	
	However, if the function is holomorphic (for reminder that is to say differentiable at every point of the complex plane or an open subset of it) and thus satisfies the Cauchy-Riemann equations we get immediately:
	
	and we thus obtain the "\NewTerm{Cauchy theorem}\index{Cauchy theorem}", or "\NewTerm{Cauchy-Goursat theorem}\index{Cauchy-Goursat theorem}" for its generalized version for non continuous functions, which says that if a function is holomorphic (thus satisfying the Cauchy-Riemann equations) and integrated on a closed contour then:
	
	As a corollary (without proof), any function that satisfies the above relation is holomorphic (in the whole complex plane or an open subset of it).
	
	This result gives the possibility in certain fields like quantum physics fields (we think of the Yukawa potential that is not yet treated in this book in detailed) to calculate complicated real definite integrals using the above property. The idea is when choosing the closed contour of the path integral to play to make the real definite integral only as a part only of the path (by generalizing to the complex case) and by equality with zero we deduce its value thanks to the other parts of the integrals of the chosen path (parts that are obviously simple to calculate).
	
	In other words, the idea is to calculate by difference! The difficulty residing in practice in finding the function $f (z)$ and the closed contour that permits to make appear the function $f (x) $ of the researched definite integral...
	
	Using this result, let us make a very important academic example which will be useful later (but who has no connection with the case of calculating a real definite integral).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us calculate:
	
	For this purpose, we will use the simplification that consist to remember (\SeeChapter{see section Numbers page \pageref{euler formula}}) that:
	
	Therefore:
	
	We can then write the path integral as:
	
	Or as on a closed path differentiable at any point (without nodes) the angle to make a full turn will necessarily be between $0$ and $+\pi$. It comes then:
	
	\end{tcolorbox}
	Before we continue by noticing a very interesting and important fact that we will detail later formally: An integral (we do not speak of primitive but of integral!) of the type $1/x$ in $\mathbb{R}$ would not be calculable. But now if we generalize the concept of $\mathbb{C}$, we see that we get go around the singularity via a path integral that enclose the singularity. And ... and ... in our previous calculation $z$ might have only the real value and not the imaginary one (so $z$ reduce to $x$). So the integral of $1/x$ becomes calculable and has a result in the set of complex numbers which is remarkable!
	
	Some mathematicians interpret this by figuring that $1/x$ is a flat projection of a three-dimensional space in which the imaginary axis is perpendicular to the plane $\mathbb{R}^2$ (see figures below). Hence the fact that $1/x$ can integrated in the set $\mathbb{C}$.
	
	Finally, let us indicate that $1/z$ is holomorphic on the whole complex plane except on $0$ (the derivative being the same as $1/x$). Then the function $1/z$ is thus not $\mathbb{C}$-differentiable!
	
	This being done, let us do an important and similar case with the following path integral:
	
	where $z_0$ is a constant complex number. Let us write:
	
	We can then write if we make one turn counter clockwise:
	
	which is valid only if our integration path avoids $z_0$ what otherwise there is a singularity. This latter integral is a little simplistic generalization of the previous one.
	
	Now let us show the important theorem that interests us since the beginning of this section using many proven results so far!
	
	We know that if a function $f (z)$ satisfies the Cauchy-Riemann equations, the if we carefully avoid the value $z_0$ (as in the above calculations), the expression:
	
	is differentiable at all points except in on $z_0$ (where the expression is no longer holomorph) is name a "\NewTerm{singularity}\index{singularity}".

	Indeed, take a holomorphic function $f (z)$ satisfying Cauchy-Riemann equation and subtract a constant ($f(z_0)$) does not change the fact that the expression (in this case the numerator in previous relationship) remain holomorphic. Finally, multiply it by a fraction (the denominator of the above equation) which is also holomorph gives a holomorphic function. But singularities can then appear, we then speak of "\NewTerm{meromorphic functions}\index{meromorphic functions}" (this is the ratio of two holomorphic functions).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A meromorphic function is a function holomorphic in the whole complex plane, except possibly on a set of isolated points each of which is a pole (singularity) for the function (see further below for the concept of pole/singularity). The gamma function (see the plot in the Differential and Integral calculus section) is a famous example of meromorphic function!
	\end{tcolorbox}	
	So if we take the path integral on a closed path avoiding $z_0$, the Cauchy theorem gives us immediately (remember the proof above):
	
	However, this can also be written after rearrangement of terms:
	
	Therefore:
	
	But we have proved above that:
	
	Then we get the result named "\NewTerm{Cauchy's integral theorem}\index{Cauchy's integral theorem}", or more rarely "\NewTerm{Cauchy formula}\index{Cauchy formula}" (of which there is a generalized result we will prove later below):
	
	In fact, in practice all the subtlety is to be able to take back a given holomorphic function $g(z)$ (which therefore satisfies the Cauchy-Riemann equations) by manipulating it in a form of the type:
	
	when its possible... then the calculation of its path integral (closed path) becomes extremely simple since it will be equal to:
	
	by the Cauchy's integral theorem!
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	So we know how to calculate the value of a path integral of an expression that is not holomorph but for which the numerator is holomorph! 
	\end{tcolorbox}
	
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Caution! The sign of the value of a path integral will depend on the direction in which its integration path will be done. If the direction is straightforward (that is to say "counter-clockwise") its sign will be positive; if on the contrary the direction is clockwise his sign will be negative. You probably think that this information is irrelevant since this value is usually zero. Yes... it is, but we will see later the importance of this information when referring to the calculation of what we name the "residuals".
	\end{tcolorbox}
		
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	An important application example is named the "\NewTerm{Gauss' mean value theorem}\index{Gauss' mean value theorem}" that states given $f(z)$ an analytic function on and inside a disk $C_r:|z-z_0|=r$, then:
	
	Indeed, from the Cauchy's integral relation we have (we simply make the change of variable $z=z_0+re^{\mathrm{i}\theta}$):
	
	\end{tcolorbox}
	 There is a similar relation for the derivative $f'(z_0)$ to that given by the Cauchy's integral theorem. Let us see this:
	
	Therefore:
	
	thereby continuing, we have:
	
	In short, we therefore note that:
	
	which is the "\NewTerm{Generalized Cauchy's integral theorem}\index{Generalized Cauchy's integral theorem}\index{Cauchy's integral}".
	
	This result is very powerful because it shows that holomorphic functions are infinitely differentiable (because of the denominator), that is to say analytical, and it is much more difficult to find an equivalent theorem with such simple conditions for real functions.
	
	If we now return to our Taylor expansion of a complex function:
	
	um ... and what do we see here? Well this !:
	
	It follows the following relation named "\NewTerm{Laurent series in positive powers}\index{Laurent series in positive powers}" (there is a more generalized version will be prove later below):
	
	that gives us the formal expression of a complex function in the form of infinite series of integer powers near a point $z_0$ of the complex plane with therefore:
	
	Remembering that $d^{n}f(z_0)/\mathrm{d}z^n$ can be written equivalently $f^n(z_0)$, we see that all the two previous relations gives us the Taylor series expansion that we had obtained in real analysis (\SeeChapter{see section Sequences and Series page \pageref{taylor series}}) and that was:
	
	Thus, the Taylor series in $\mathbb{R}$ are a special case of Laurent series that are in $\mathbb{C}$!!!
	
	This result is quite remarkable because it also shows that we can use the path integral in the complex plane for calculating the coefficients $c_n$ of the Laurent series instead of calculating the derivatives of order $n$ of the function $f$ if these latter are too complicated to determine. Or vice versa... calculate a simple derivation instead of calculating a headache type path integral (typically the case in physics) using the fact that:
	
	The only unfortunate point being that the latter relation is calculable only if we can put the function in path line integral in the form:
	
	where $n$ is a positive or null integer. This is honestly far from to be easy in most cases! The idea would be to find a general path for line integral, valid for any function $f (z)$ such that the denominator (which additionally contains a singularity on $z_0$) disappears. That would be ideal ... but we need a track ... and it will come from the study of the convergence of series of complex powers. Let's see what it is with a qualitative approach!
	
	\pagebreak
	\subsubsection{Convergence of a complex series}
	We saw in the section of Sequences and Series that many real functions could be expressed in Maclaurin series (special case of the Taylor series on $x_0=0$) in the form:
	
	We also showed, by example the only, that this series expansion of infinite powers was valid for some real functions only in a certain domain of definition named "radius of convergence".
	
	Even if this radius of convergence can be determined more or less easily in each case, there are some baffling examples that could not in the early 19th century be understood without complex analysis.
	
	Let's see a simple example to understand what kind of problem it is. Consider for this the two functions:
	
	and before continuing our example, recall that we have proved in the section of Sequences and Series the relation:
	
	relative to a geometric series, that is to say a series whose terms are of the type:
	
	Therefore it comes immediately if $n \rightarrow +\infty$ and $q \in ]-1,+1[$:
	
	if $u_0=1$, we get:
	
	So if we change the notation, we have\label{sum of powers}
	
	Then it comes immediately:
	
	Therefore the two previous functions $g(x)$ and $h(x)$ are defined for a infinite series expansion in powers only in radius of convergence $x \in ]-1,+1[$.
	
	We would get the same result by making a Maclaurin series expansion!
	
	We see that there is trivially for $g(x)$ two singularities that are $x=\left\lbrace -1,+1\right\rbrace$ by cons, basically we do not see trivial singularities for $h (x)$ if we reason only in $\mathbb{F}$ so it can be hard for the latter function to understand the origin of the radius of convergence!
	
	Indeed, if we plot these two functions in $\mathbb{R}$ with Maple 4.00b we get respectively:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/g_h_example_functions.jpg}
		\end{center}
	\end{figure}
	hence the problem of why there is still implicitly a radius of convergence $x \in ]-1,+1[$ for $h (x)$???
	
	An even more blatantly way to highlight the problem, is to show the approach of these two functions by a Maclaurin series expansion with ten terms.
	
	For $g(x)$ we get for example:
	
	\texttt{>with(plots):\\
	>xplot:= plot(1/(1-x\string^2),x=-5..5,thickness=2,color=red):\\
	>tays:= plots[display](xplot):\\
	>for i from 1 by 2 to 10 do\\
		tpl:= convert(taylor(1/(1-x\string^2), x=0,i),polynom):\\
		tays:= tays,plots[display]([xplot,plot(tpl,x=-5..5,y=-2..2,\\
		color=black,title=convert(tpl,string))])\\
		od:\\
	>plots[display]([tays],view=[-5..5,-2..2]);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/g_function_expansion_inspection.jpg}
		\end{center}	
		\caption[]{Plane representation of the function $g$ to visualize the problem}
	\end{figure}
	where we see well that the Maclaurin series (or expression in power series) does not converge outside $x \in ]-1,+1[$  which can be intuitive because of both singularities.
	
	For $h(x)$ we have by cons:
	
	\texttt{
	> with(plots):\\
	> xplot:= plot(1/(1+x\string^2),x=-5..5,thickness=2,color=red):\\
	> tays:= plots[display](xplot):\\
	> for i from 1 by 2 to 10 do\\
		tpl:= convert(taylor(1/(1+x\string^2), x=0,i),polynom):\\
		tays:= tays,plots[display]([xplot,plot(tpl,x=-5..5,y=-2..2,\\
		color=black,title=convert(tpl,string))])\\
	od:\\
	> plots[display]([tays],view=[-5..5,-2..2]);\\
	}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_function_expansion_inspection.jpg}
		\end{center}	
		\caption[Divergent Maclaurin series]{Surprisingly, here the Maclaurin series (in black) does not converge}
	\end{figure}
	where we see well that the Maclaurin series (or the expression in power series) does not converge either outside $x \in ]-1,+1[$ which was unsettling and against-intuitive at the beginning of the history of real analysis.
	
	Today even a high school student knows that he can also think in $\mathbb{C}$ and that $\mathbb{R} \subset \mathbb{C}$. So the real analysis is just a special case and restricted of the field of complex analysis. The fact to extend the domain of a given analytic function is named "\NewTerm{analytic continuation}\index{analytic continuation}". As we will see just now analytic continuation often succeeds in defining further values of a function, for example in a new region where an infinite series representation in terms of which it is initially defined becomes divergent!
	
	The singularity for $h (x)$ in $\mathbb{C}$ comes that latter is then  written:
	
	and there are therefore two singularities $z=\left\lbrace{-\mathrm{i},+\mathrm{i} }\right\rbrace$ that we see well if we represent:
	
	with Maple 4.00b (fortunately we now have the equivalent of a microscope in mathematics with Maple...):
	
	\texttt{>plot3d(abs(1/(1+(re+I*im)\string^2)),re=-3..3,im=-3..3,view=[-2..2,-2..2,-2..2]\\
	,orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/g_inspection_in_C.jpg}
		\end{center}	
		\caption{Complex representation of the function $h$ to highlight the reason for the divergence}
	\end{figure}
	where we can see the two singularities on the imaginary axis and the function $h (x)$ on the real axis (between the two peaks). So when we develop a function in power series, we conclude that the radius of convergence is defined by the whole complex plane and not by the traditional axis of the real analysis.
	
	This makes it more natural to understand why we were talking in section of Sequences and Series of "radius" as seen from above, we have in the complex plane:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_various_radius_convergence.jpg}
		\end{center}	
		\caption{Representation of the various convergence of radius of $h(z)$}
	\end{figure}
	hence the fact that we are talking sometimes about (open) convergence disk and sometimes of (open) convergence radius. Moreover, we notice on the chart that the domain of convergence is convex (any couple of points of the domain can be connected by a straight line that is in the area of convergence).
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Let us Recall that a subset, interval or "open" disc means that we do not take its border as we have seen in the section Topology.
	\end{tcolorbox}
	Then we understand better why the Taylor series does not converge trivially for $h(x)$: it must converge on the whole disc of the complex plane and not just converge on the real axis!
	
	From all this we deduce that our Laurent series in positive powers proved above:
	
	not necessarily converge, unsurprisingly... on the whole complex plane (just like the Taylor series on the real line as this is the equivalent!) but sometimes only in a opened subdomain (convex?) of this plane around $z_0$ (which in the particular example taken above was obviously: $0$).
	
	With our function $h(x)$ expressed using a development of Maclaurin with 5 terms, we see immediately with Maple 4.00b that on the borders of the square inscribed in the disc of convergence, the series does not converge and we're guessing the start of the two singularities:
	
	\texttt{>plot3d(abs(1-(re+I*im)\string^2+(re+I*im)\string^4-(re+I*im)\string^6+(re+I*im)\string^8),\\
	re=-0.7..0.7,im=-0.7..0.7,view=[-1.5..1.5,-1.5..1.5,0..1.5]\\
	,orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_zoom_on_complex_representation.jpg}
		\end{center}	
		\caption{Focus on the complex representation to understand the reason for the divergence}
	\end{figure}
	a little outside the disc of convergence, we obviously have a little bit nonsense:
	
	\texttt{>plot3d(abs(1-(re+I*im)\string^2+(re+I*im)\string^4-(re+I*im)\string^6+(re+I*im)\string^8),re=-3..3,\\
im=-3..3,view=[-1.5..1.5,-1.5..1.5,0..1.5],orientation=[-130,70]\\
	,contours=50,style=PATCHCONTOUR,axes=frame,grid=[100,100],numpoints=10000);}
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_divergence.jpg}
		\end{center}	
		\caption[]{This diverges ... (stalactites ???)}
	\end{figure}
	There is still something interesting to try ... since we are now on a plane, not a straight line right (axis), it is possible for us to make the Taylor expansion around a singularity $z_0$ by deforming the disk in a convex crown/ring simply connected as shown below (the crown/ring being the simplest simply convex geometry arising from the deformation of a disk):
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/h_representation_transformation_disc_in_crow.jpg}
		\end{center}	
		\caption{Representation of the deformation of a disc in a crown/ring}
	\end{figure}
	The advantage of this is to deform the area of convergence on the whole complex plane by avoiding (bypassing) all the singularities. Thus, unlike the Taylor series that are only valid on an interval of the $x$-axis, we would have a new type of series describing a function absolutely everywhere, that is to say before AND after (so around...) singularities!
	
	So obviously we will require that in the deformed crown above the function is always holomorph and analytical (as in the initial convex disc). Before determining what we are going down (generalized Laurent series!), we must first do a study of the decomposition of path integral:
	
	\pagebreak
	\subsection{Path Decomposition}
	The path integrals as given previously can also be written in another form almost classical and used many times in the literature.
	
	Let us see this. First, remember that we have just proved in the special case of a holomorphic function that:
	
	But a closed path can be seen as a path having a round trip:
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/closed_path.jpg}
		\end{center}	
		\caption{Representation of a closed path with round trip}
	\end{figure}
	Therefore we can write:
	
	And now comes what interest us... for this purpose let us focus one  of the path integral of the type:
	
	We already well known (1st form of notation) that any complex number $z$ of the type:
	
	can be (2nd form of notation) written as (Euler form):
	
	and to integrate on a path, nothing prevent us to choose a path where $r$ (the module) would be fixed and $\theta$ variable (we could not have the possibility to do this with the 1st form because by modifying the imaginary or real part, we can't get be guarantee to get a nice smooth curve but this is possible with the Euler form of a complex number)!
	
	Therefore we have:
	
	We write then naturally:
	
	and as:
	
	Therefore:
	
	That we often find in the following form in the literature:
	
	
	
	\subsubsection{Inverse Path}
	If $C$ is a curve going from a point $P$ to a point $Q$, then we denote by $C^-$  the same curve but travelled from $Q$ to $P$.
	
	Let us parametrized $C^-$:
	
	If $C(t)$ it is the curve defined on $[a, b]$, then we define the curve $C^-(t)$ on $[a, b]$ by:
	
	Indeed we have with this parametrization:
	
	and when $t$ increases from $a$ to $b$, $a + b - t$ decreases from $b$ to $a$. $C^-$ is therefore only $C$ but travelled in the opposite direction.
	
	We then have using the last proof:
	
	Let us put:
	
	Therefore:
	
	Then we have:
	
	Therefore if $C^-$ and $C$ are the paths of the same function but travel in the opposite direction, we have by taking our conventional notation (caution! In the second term it is implicit that the parametrization is different from the first one!):
	
	Therefore:
	
	this is why we often say that the sign of the value of a line integral will depend on the direction in which its integration path is travel. If the direction is straightforward (that is to say "counter-clockwise") its sign will be positive; if on the contrary the direction is clockwise its sign will be negative (\SeeChapter{see section Differential and Integral Calculus page \pageref{closed path orientation}}).
	
	\pagebreak
	\subsection{Laurent Series}
	This last relation obtained, we can return to the deformation of our disc of convergence in a crown. We recall that initially the idea is to have the analytical expression of a function as an infinite series of powers in a limited area around a singularity point and all this... in the purpose to be able to calculate for physicists complex path integrals through a method using the properties of complex series!
	
	Let's start with the point (2) that is to say have an infinite power series for a path integral, which will take us more easily to point (1) that is to say get the an analytical expression of a function around a singularity point, by zooming on our crown:
	
	\begin{figure}[H]
		\begin{center}
			\includegraphics{img/analysis/crown.jpg}
		\end{center}	
		\caption[]{Zoom on our crown from our starting example}
	\end{figure}
	We therefore have if the function $f$ is analytic and holomorphic in the crown of outer radius $R$ and inside radius $r$, the following path curvilinear integral in the crown as we proved above (we change notation: $z=z'$ and $z_0=z$):
	
	
	therefore we denote now by $z$ the point where we want to know the function and $z'$ variable of which $f$ depends. This notation change will be justified later for a purely practical reason.
	
	The crown can be broken down into four paths:
	
	If both segments $C_c$ and $-C_c$ are infinitely close, they then correspond to the same path travelled once in a positive direction and once in the negative direction. As we have proved just above that:
	
	It therefore follows that:
	
	Which brings us to write:
	
	where we have put a "+" between the last two terms, because as we shall see immediately, the convergence criterion associated with the traditional notation in this field of study, makes automatically emerge the sign "-".
	
	For the two integral $f_1,f_2$, we know that the fraction can be written as a geometric series as already seen above. Effectively, starting from (now you will understand why we changed the notation):
	
	by assimilating:
	
	where as we have seen, the convergence requires that:
	
	so that $x$ is lower in absolute value to $1$.
	
	We then see the infinite geometric series appearing:
	
	Therefore:
	
	To come back to:
	
	we have in any point $z$ inside the circle of radius $R$ whose border is described by the variable $z'$ and of center $z_0$ the convergence that is assured because:
	
	Then we can write:
	
	Integrating term by term integration, we highlight the development (already known):
	
	with the definition of coefficients $c_n$, where $n$ is a positive or null integer:
	
	This development may do think to the development of Taylor in the sense that only positive (or zero) powers of $(z'-z_0)$ appear, but this is not Taylor development in the case of the crown! Indeed, $c_n$ can not be written this time as:
	
	since, by assumption, $f(z)$ is assumed analytic in the crown only and may therefore very well not be inside the small circle of radius $r$, in particular on $z_0$, in which case $f^{n}(z_0)$ may simply not exist (let us repeat that $z$ is strictly constrained to be in the crown, therefore $r<\vert z \vert <R$). We will see later what happens when $f (z)$ is holomorphic in this disk and that, in particular, $z_0$ is not a singular point.
	
	We still need to treat $f_2$. We then do the same type of development as for $f_1$, with the difference that now:
	
	when $z'$ browse the small circle of radius $r$. To make a geometric series appear, we must write this time:
	
	Therefore:
	
	So we have:
	
	Integrating term by term, we highlight the (new) development:
	
	with:
	
	By changing $n$ in $-n$ in the summation for $f_2$, we have for the sum $f_1(z)+f_2(z)$:
	
	with at this time two distinct $c_n$:
	
	We will now see that these two relations can be combined into one!
	
	For this purpose if we observe well the last two relations, we find that they do not depend at all of $z$ (!) and this is normal since the $c_n$ are the coefficients of the series expansion of $f(z)$ and these are the same at any point of the domain of definition of the function where it is analytic!
	
	So the two contours (circles) can be merged into only one circle since it is located in the crown and has for center $z_0$:
	
	Furthermore, the attentive reader will have noticed that this contour does not even need to be a circle finally! It may be any geometry as long as it is closed and is located in an analytical area!
	
	Thus, we get the two relations:
	
	The two previous relation define the "\NewTerm{general Laurent series}\index{general Laurent series}". It is remarkable and differs from a Taylor series in the sense that it contains all the positive and negative integer powers and the coefficients $c_n$ can a priori not be expressed with the derivatives of $f$.
	
	The power series of $n\geq 0$ is named "\NewTerm{regular part}\index{regular part of a power series}", the negative powers is commonly named "\NewTerm{main part}\index{main part of a power series}".
	
	The series of negative powers converges uniformly everywhere outside $\gamma_r$, that of positive powers within $\gamma_R$. In total the development of Laurent converges uniformly in the common area, which is the crown and therefore also on the unique path $\gamma$.
	
	Let us now show a point that we have mentioned above. If the circle contains no singularity, then all the coefficients:
	
	are zero. First note that $-n-1$ is a positive or zero integer, which we will denote by $p$ such as:
	
	We then have the following integrand along a closed path:
	
	But, if we remove the singularity that requires $f(z')$ is holomorphic (and in anyway this is required by all initial developments of the Laurent series).
	
	As $(z'-z_0)^p$ is polynomial with positive and not null integer powers and that as we know any polynomial satisfying these conditions is differentiable at least once without showing singularity. Thus this term is also holomorphic.
	
	Assuming that the product of two holomorphic functions is holomorphic and that contour $\gamma$ is closed, then we have using the following result proved above (for a holomorphic function):
	
	the following immediate consequence:
	
	if there is no singularity in the small circle of the crown. We then fall back again on a development with only positive powers, the $c_{n\geq 0}$ this time being equal to:
	
	according to the generalized Cauchy's integral theorem proved earlier above. Conversely, we see well that this is the main part (when it exists!) which contains the information on the fact that $f$ is not a priori holomorphic in the small disk. The existence of negative powers shows that $f$ is clearly not bounded on $z_0$.
	
	The classification of singularities of a function will be precisely based on the consideration of the characteristics of the main part of the Laurent  development centered on a singular point of this function.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us see to what looks like the Laurent series of our famous example function:
	
	on a simply convex domain that would be the crown rounding the singularity  $\mathrm{i}$ for example (we could have taken the second singularity $-\mathrm{i}$ but we had to choose one to not repeat twice the explanations below...). This is equivalent therefore to search the power series development of $z-\mathrm{i}$. \\
	
	We will proceed as following:
	
	For what will follow we will use:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	The second fraction can be expressed as a geometric series if as we have already seen:
	
	Therefore it comes:
	
	Let us multiply both sides of this equality by $-i / $2 and then divide them by $z - i$ (the second term in the denominator of the original fraction) for obtain the left term:
	
	and for the right term:
	
	Finally we have the following geometric series:
	
	We see then on this Laurent series around $\mathrm{i}$ of the holomorphic function $f(z)$ that the following coefficient appears:
	
	and then we have with Maple 4.00b:\\
	
	\texttt{>plot3d(abs(-I/2*1/((re+I*im)-I)-(I/2)\string^2-(I/2)\string^3*(re-I*im)-\\
	(I/2)\string^4*(re-I*im)\string^2-(I/2)\string^5*(re-I*im)\string^3),\\
	re=-1.5..1.5,im=-1.5..1.5,view=[-2..2,-2..2,-1..2],\\
	orientation=[-130,70],contours=50,style=PATCHCONTOUR,axes=frame,\\
	grid=[100,100],numpoints=10000);}
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	This gives the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/laurent_series_representation.jpg}
		\caption{Laurent series representation of $f(z)$ with Maple 4.00b}
	\end{figure}
	where we see that the Laurent series allows us to express $f (z)$ in a neighbourhood close to the singularity $\mathrm{i}$ by taking five terms.\\
	
	Ditto if we make the sum of the two Laurent series for the two singularities with seven terms:\\
	
	\texttt{>plot3d(abs(-I/2*1/((re+I*im)-I)-(I/2)\string^2-(I/2)\string^3*(re-I*im)-\\
	(I/2)\string^4*(re-I*im)\string^2-(I/2)\string^5*(re-I*im)\string^3 -(I/2)\string^6*(re-I*im)\string^4\\
	-(I/2)\string^7*(re-I*im)\string^5+I/2*1/((re+I*im)+I)+(I/2)\string^2+
(I/2)\string^3*(re+I*im)+(I/2)\string^4*(re+I*im)\string^2+(I/2)\string^5*(re+I*im)\string^3\\
	+(I/2)\string^6*(re+I*im)\string^4
+(I/2)\string^7*(re+I*im)\string^5),re=-1.5..1.5, im=-1.5..1.5, view=[-2..2,-2..2,-1..2],orientation=[130,70], contours=50, style=PATCHCONTOUR, axes=frame,grid=[100,100],\\
numpoints=10000);}\\

	This gives the image visible on the next page:
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/sum_of_two_laurent_series.jpg}
		\caption{Sum of the two Laurent series of $f(z)$ for both singularities with Maple 4.00b}
	\end{figure}
	\end{tcolorbox}
	
	\subsection{Singularities}
	We have seen just before that it was possible to calculate the path integral of a function, on condition of analyticity, on the outline of a singularity. Our goal will now be to enhance this approach.
	
	We have already mentioned and highlighted in our previous proofs that the integrant in the "Cauchy's integral theorem" was of the form:
	
	where $f(z)$ is well defined in $z_0$.
	
	The point $z-z_0$ is of course a singularity of $f(z)$ and it is not defined there.
	
	As we saw during our proof of Laurent series, $f(z)$ can be expressed as a Laurent series in the form a positive power Laurent series in a convergence disk (or what remains the same: as a series of Laurent in a crown not centered on a singularity...) in the form:
	
	Before continuing, it is customary in mathematics to define a small conventional vocabulary regarding this time the possible singularities of $f(z)$!
	
	Let us first recall that we know, and that we have proved, that all information on the singularities of $f (z)$ are contained in the main part of the Laurent series (negative powers) defined on the crown surrounding $z_0$:
	
	The following classification focus on "\NewTerm{isolated singularities}\index{isolated singularities}", that is to say, a singular point where $f(z)$ is analytic everywhere in the neighbourhood excepted on $z_0$. This classification, as we will see permits us to distinguish three types of singular points, will be useful when developing the theory of residues further.
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] When the limit of the function $\vert f(z) \vert$ exists on $z_0$, we say that the singularity is a "\NewTerm{removable singular point}\index{removable singular point}" or "\NewTerm{apparent singularity}\index{apparent singularity}".
		
		For example:
		
		does not seem to be defined on $z=z_0=0$ but we have a numerator having a Laurent series without negative powers (therefore a simple Taylor series). It then comes into by doing the Maclaurin series (that is to say the Taylor series on $z=z_0=0$...):
		
		We then see that $f (z)$ finally has no term with negative power and therefore we have eliminated the singularity (or that it contains simply no singularities... which can easily be check with Maple 4.00b).
		
		\item[D2.] When on $z_0$ the limit $\vert f(z) \vert $ does not exist we speak about "\NewTerm{essential singularity}".
		
		For example, $z_0=0$ is an essential singularity for the function:
		
		Indeed, if $z$ approaches zero coming from the positive real axis $\mathbb{R}_+$, the function diverges, more precisely, it tends to $+\infty$. If $z$ comes from $\mathbb{R}_-$, the function tends to zero as illustrated by the following Maple 4.00b plot:
		
		
		\texttt{>plot3d(abs(exp(1/(re+I*im))),re=-5..5,im=-5..5,\\
		view=[-3..3,-3..3,-0.5..3],orientation=[-130,70],contours=50,\\
		style=PATCHCONTOUR,axes=frame,grid=[100,100],numpoints=10000)\\
		>plot3d(abs(exp(1/(re+I*im))),re=-5..5,im=-5..5,\\
		view=[-3..3,-3..3,-0.5..3],orientation=[-130,70],contours=50,\\
		style=PATCHCONTOUR,axes=frame,grid=[100,100],numpoints=10000)}
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/plot_essential_singularity.jpg}
			\caption{Essential singularity example with $e^{1/z}$ in Maple 4.00b}
		\end{figure}
		Indeed:
		
		So an equivalent way of defining an essential singularity, is to say that there are an infinite number of terms with negative powers in the main part of the Laurent series.
		
		\item[D3.] When on $z_0$ the limit of $\vert f(z) \vert$ is $+\infty$, we speak about a "\NewTerm{pole}\label{pole}".
		
		This is the last category (as far as we know...) in which we can store a function that is not classifiable neither in the first nor in the second definition above.
		
		So another equivalent way of defining a "pole", is to say that there is a finite number of terms with negative powers in the main part of the Laurent series. If the number of terms is $k$, then we speak of "\NewTerm{pole of order $k$}".
		
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We sometimes say that an essential singularity is a " \NewTerm{pole of order $+\infty$}".\\
	
	\textbf{R2.} A pole of order 1 is named a "\NewTerm{simple pole}". One of order 2 is named a "\NewTerm{double pole}" and so on...
	\end{tcolorbox}
	\end{enumerate}
	If we come back on our example:
	
	We have proved previously that the Laurent series of the function was:
	
	This function has therefore a trivial pole of order $1$ on $z_0=\mathrm{i}$ and also on $z_0=-1$ because in this latter case this infinite series diverge to $+\infty$ and we can easily check this with the following Maple 17.00 command:
	
	\texttt{>sum(-(I*(1/2))\string^n*(-2*I)\string^(n-2), n = 0 .. infinity)}
	
	\subsection{Residue Theorem}\label{residue theorem}
	Consider a function $f(z)$ whose pole is of order less or equal to $k$.
	
	Let us make it  analytic:
	
	that is to say that we have take a function $f(z)$ that we have made analytic after elimination of the poles supposed in finite number - order - less than or equal to $k$ on $z_0$. 
	
	This function $\phi(z)$ has therefore a Laurent series development  in a disc center on $z_0$.
	
	As we have prove it previously, we can therefore by using the following relation:
	
	write:
	
	Using $f(z)$ under the integral it comes:
	
	You must deeply analyse this relation and understand that it link together the integral of a function having singularities with the value on one point of an analytical function having no more singularities!!!
	
	This latter relation can be rewritten by rearranging terms:
	
	And by expressing $\phi^{(k)}(z_0)$ by using (this is authorized because this latter function is analytical) the fact that by definition:
	
	We get obviously:
	
	Therefore by making $\phi(z)$ explicit again:
	
	This latter relation is valid only for ONE isolated singularity (in case you forget!) and where $k$ is equal at least to $1$!

	Mathematicians therefore define:
	
	as being the residue of the function $f(z)$ at the point being an isolated singularity of order $k$. Or respectively:
	
	where the path integral is centered on $z_0$.
	
	Now notice that the term on the right of the equality in the previous relation correspond to the coefficient $c_{-1}$ of the Laurent series. Indeed:
	
	Therefore:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Therefore it comes that on an isolated singularity that can be eliminated, the residue is null because as we saw it before, the path integral rounding a domain without singularity is equal to zero!
	\end{tcolorbox}
	To resume, the relation:
	
	is very interesting for the physicist... because this is a very elegant way for him to calculate the path integral of a non analytic function $f(z)$ having a unique isolated singularity and this just by knowing the order of its poles!
	
	For example if a function $f(z)$ has only a pole of order $1$, we have therefore:
	
	and we replace therefore $z_0$ by the desired value in the parenthesis $(z-z_0)$ and after we calculate the limit between brackets!
	
	Now to go more fare, let us remind that outline of the path integral:
	
	and the curvilinear path of the integral:
	
	are in fact combined (identical) and the coefficients $c_n$ do not depend on $z$! The only constraint on the path is that is closed and in an analytical domain centered on one point.
	
	So if we have several isolated singularities, surrounded by connected curvilinear paths as shown below on the complex plane of a function having a pole of order 3 (i.e. three non-removable singularities $z_0,z_1,z_2$):
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/multiple_surrounded_singularities.jpg}
		\caption{Multiple isolated singularities surrounded by curvilinear paths}
	\end{figure}
	then we have still only one closed curvilinear path but whose different isolated singularities are connected by cross which as we know: the paths that are opposed  cancel themselves! And let us remind that the coefficients are the same throughout on all the path since it is on an analytical domain.
	
	We then have the generalized version of the residue theorem for a function $f$ with $n$ isolated singularities:
	
	with a rigorous approach that is specific to engineers ... who sometimes write this latter relation as following:
	
	where $r$ is therefore a residue. This is an important result in the field of solving differential equations associated with some inverse Laplace transforms (\SeeChapter{see section Functional Analysis page \pageref{Laplace transform}}). This intermediate result will give us the possibility to get an another one a little further of major importance for the section of Corpuscular Quantum Physics.
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take again our famous function:
	
	We know it has a pole of order $1$ on $z_0=\mathrm{i}$ and a pole of order $1$ on $z_0=-\mathrm{i}$. So if we take this time Laurent series with a path that surrounds the two singularities (and not only one) then we have a function with a pole of order $2$.
	
	It comes then for this particular case:
	
	with $n$ being equal to $2$.
	
	Then we have:
	
	and:
	
	\end{tcolorbox}
	
	\pagebreak
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	We can easily check this with Maple 4.00b:\\
	
	\texttt{>readlib(singular):\\
	>singular(1/(1+z\string^2),z);\\
	>readlib(residue):\\
	>residue(1/(1+z\string^2),z=-I);\\
	>residue(1/(1+z\string^2),z=I);\\}

	and therefore:
	
	In fact in this case, the residue theorem gives zero because the function has no poles to infinity which is true since in our example:
	
	Physicists meanwhile say that... the force does make any work on this path ...!
	\end{tcolorbox}
	
	\subsubsection{Pole at infinity}
	We have say before that any function that did not have poles at infinity had therefore the sum of the residues of all the poles that are equal to zeros. This result is very important in physics and merits to be study!
	
	It is almost trivial to recognize the number of poles... but to recognize the poles that are at infinity there are many times traps we can easily fall in.
	
	Let us consider the expression $f(z)\mathrm{d}z$. If $z$ is at the neighbourhood of the infinity then $1/z$ is near $0$. Let us write:
	
	Then we have:
	
	Then the residue at infinity is such that:
	
	with:
	
	Therefore with:
	
	The latter relation we will be indispensable to us in the section of Corpuscular Quantum Physics Corpuscular to build the relativistic Sommerfeld model of hydrogenous atom because we will need to calculate a path integral with a pole.
	
	Let's see an example with the function that accompanies us since the beginning of this section. That is to say:
	
	Therefore it comes:
	
	And we recognize immediately the initial function, in absolute value, and that has therefore no pole on $0$. Therefore $f(z)$ has no pole at infinity.
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{100} & \pbox{20cm}{\score{3}{5} \\ {\tiny 14 votes,  61.43\%}} 
	\end{tabular} 
	\end{flushright}
	
	%to force start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Topology}\label{topology}
	\lettrine[lines=4]{\color{BrickRed}T}opology is an extremely broad field of mathematics for which it is difficult to define precisely the object so the areas where it applies are varied (real line topology, graphs topology, differential topology, complex topology, symplectic topology, etc.). 

We mainly make a distinction with:
	\begin{itemize}
		\item "\NewTerm{General topology}" that establishes the foundational aspects of topology and investigates properties of topological spaces and investigates concepts inherent to topological spaces. It includes point-set topology, which is the foundational topology used in all other branches (including topics like compactness and connectedness).
		
		\item "\NewTerm{Algebraic topology"} tries to measure degrees of connectivity using algebraic constructs such as homology and homotopy groups.
		\item "\NewTerm{Differential topology"} is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry (see section of the same name in the chapter about Geometry page \pageref{differential geometry}) and together they make up the geometric theory of differentiable manifolds.
		\item "\NewTerm{Geometric topology}" primarily studies manifolds and their embeddings (placements) in other manifolds. A particularly active area is low dimensional topology, which studies manifolds of four or fewer dimensions. This includes knot theory, the study of mathematical knots.	
	\end{itemize}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	A "\NewTerm{manifold}\index{manifold}" is a higher dimensional analogue of a curve or a surface. A manifold of dimension $n$ is a space, you can think of a collection of points, that locally looks like $\mathbb{R}^{n}$ for some integer $n$. All curves in the two dimensional plane that do not intersect locally look like part of a line. All smooth surfaces in $\mathbb{R}^3$, that is surfaces that do not have sharp kinks, edges (boundaries), or points all locally look like the two dimensional plane. Thus such surfaces are two dimensional manifolds. One should imagine being able to tear any small piece off the smooth surface, then being able to stretch it, push down any "hills" and push up any "valleys" to end up with a flat piece of the $2$-plane. Therefore a manifold is a space that is locally Euclidean\\
	
	The picture to have in your mind when thinking about manifolds is the relation between a globe and a map. Small pieces of the globe can always be described by maps (pieces of the 2-plane). Moreover, the entire globe can be covered by a collection of maps: an atlas.
	\end{tcolorbox}
	What we can say at first is that in its foundations Topology is very closely related to the set theory, the study of convergence of sequences and series, functional analysis, analysis complex, the differential and integral calculus, vector calculus and the geometry to mention only the most important cases that the reader can already found in this book.

	The origin of Topology comes from the problems that laid to the progress of functional analysis in the rigorous study of continuous functions, their differentiability, their limits at a point (finite or note), the existence of extremums, etc. in higher-dimensional spaces (in fact, implicitly, the goal for topology is to create tools that easily allow to study the properties of functions in all dimensions). All these concepts, needed a rigorous mathematical definition of the intuitive idea of proximity, especially when doing operations on such functions.

	We will try in this section to identify the basis of the structures that allow us to speak about limits and continuity and this only for curiosity as consultants in R\&D and financial engineering we never saw a business application where the subjects below are absolutely necessary to develop a new business or solve a problem. 
	
	The majority of examples that we will take in this section will be in $\mathbb{R}$ (the $\mathbb{R}$ straight line to be more exact...) because it is the most used one by the engineers (and most of times the only one!) and the one we will have to use for the sections on Graph Theory, Statistics, Differential and Integral Calculus and also on Fractals. When we restrict our study of Topology on $\mathbb{R}$ we then speak of "\NewTerm{Real Analysis}".

	\subsection{General Topology}

General topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology.

The fundamental concepts in point-set topology are "\NewTerm{continuity}", "\NewTerm{compactness}", and "\NewTerm{connectedness}".  
	\begin{itemize}
		\item Continuous functions take \underline{nearby} points to nearby points.
		
		\item Compact\label{compact} sets are those that can be covered by finitely many sets of \underline{arbitrarily small} size. 
		
		\item Connected sets are sets that cannot be divided into two pieces that are \underline{far apart}. 
	\end{itemize}		
	The words \underline{nearby}, \underline{arbitrarily small}, and \underline{far apart} can all be made precise by using open sets. If we change the definition of open set, we change what continuous functions, compact sets, and connected sets are. Each choice of definition for open set is named a "\NewTerm{topology}". A set with a topology is named a "\NewTerm{topological space}\index{topological space}\label{topological space}".

	"\NewTerm{Metric spaces}\index{metric space}\label{metric space}" are an important class of topological spaces where distances can be assigned a number named a "\NewTerm{metric}\label{metric}". Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.
	
	An "\NewTerm{inner product}" (\SeeChapter{see section Vector Calculus page \pageref{inner product}}) induces a "\NewTerm{norm}"  (\SeeChapter{see section Vector Calculus page \pageref{vector norm}}) and the norm induces a metric space. 
	
	Therefore we understand better what we will study in this section that can be summarized by the following figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/topological.jpg}
	\end{figure}

	\subsubsection{Topological Spaces}
	Topological spaces form the conceptual foundation on which the concepts of limit, continuity or equivalence are defined.
	
	The framework is general enough to be applied in many different situations: finite sets, discrete sets, geometry spaces, $n$ dimensional numerical spaces and most complex functional areas. These concepts appear in almost all branches of mathematics, they are therefore central to the modern view of mathematics.
	
	\textbf{Definition (\#\mydef):} Consider a non-empty set $X$ (the length of a plastic ruler for example). A "\NewTerm{topology $\mathcal{T}$}" or "\NewTerm{topological space $(x,\mathcal{T})$}" on $X$ is a family $\mathcal{T}$ of parts of $X$ (of length of our rule...) named "\NewTerm{open $V$}"  (as the open intervals seen in the section  of Functional Analysis) such that the following axioms are true:
	\begin{enumerate}
		\item[A1.] The empty set $\varnothing$ and $X$ are considered as open $V$ and must belong to the family of the topology $\mathcal{T}$ (these only both open sets define what we name the "\NewTerm{trivial topology}" that is the most minimal one satisfying all the axioms):
		
		In other words, if we imagine our plastic ruler, the measure zero (strictly speaking: the empty set) must belong to the topology defined on the ruler and the ruler itself (seen as a subset).
		
		\item[A2.] Any finite intersection of open of $\mathcal{T}$ will be an open of $\mathcal{T}$:
		
		
		\item[A3.] Any union of open of $\mathcal{T}$ will be an open of $\mathcal{T}$:
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Mathematicians frequently note by $O$ the family of open sets and by $F$ the family of closed sets. Convention we will not follow in this book.\\

		\textbf{R2.} The close sets of a topology are complementary of open sets. Therefore, the family of  close sets contains among other $X$ and the empty set $\varnothing$...\\
	
		\textbf{R3.} There is no difference between part and subset of a set.
		\end{tcolorbox}
		
		\item[A4.] The couple $(X,\mathcal{T})$ is a "\NewTerm{Hausdorff space}" or "\NewTerm{separate space}" if moreover the property named "\NewTerm{Hausdorff axiom}" is verified:
				
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} A well known example of topological space is $\mathbb{R}$ provided with the set $F$ generated by the open intervals (by the union law), that is to say the intervals of the type $] a, b [$.\\
		
		\textbf{R2.} We will see a very concrete and beautiful + nice application of Hausdorff spaces in our study of fractals in the chapter Theoretical Computing.
	\end{tcolorbox}
	
	\textbf{Definition (\#\mydef):} 
	\begin{enumerate}
		\item[D1.] If we denote by $(X, V)$ a topological space, $V$ designating the open sets of $X$, a "\NewTerm{base}", in the topological sense, of $(X, V)$ is a part $B$ of $V$ such that any open set of $V$ is a an union of open sets of $B$ (this is the same idea as vector spaces but in fact applied to sets ... nothing bad and difficult! If you want an example see the section of Measure Theory).
		
		\item[D2.] In topology a subset $A$ of a topological space $X$ is named "\NewTerm{dense}\index{dense set}\label{dense set}" (in $X$) if for every point $x$ in $X$ either belongs to $A$ or is a limit point of $A$. Informally, for every point in $X$, the point is either in $A$ or arbitrarily "close" to a member of $A$. For instance, every real number is either a rational number or has one arbitrarily close to it. Therefore $\mathbb{Q}$ is dense in $\mathbb{R}$.
	\end{enumerate}
	
	\pagebreak
	\subsection{Metric Space and Distance}\label{distance}
	\textbf{Definition (\#\mydef):} A "\NewTerm{metric space}" denoted by $(X, d)$ or sometimes $X_d$ (or even sometimes just $X$ if the type of distance $d$ cannot not be confused) is by definition a set $X$ with provided with an application:
	
	named "\NewTerm{distance}\index{distance}" or "\NewTerm{metric}", which satisfies the following axioms:
	\begin{itemize}
		\item[A1.] Positivity:
		
		
		\item[A2.] Separation:
		
		\item[A3.] Triangular inequality:
		
		
		\item[A4.] Symmetry: 
		
	\end{itemize}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Some readers will probably see immediately that some of these properties have already been seen in other sections of this book during our study of distances between functional points and during our study of norms (triangle inequality proved in the section of Vector Calculus - the symmetry, positivity, the separation have already been study in the section of Functional Analysis).\\
		
		\textbf{R2.} Some authors omit the axiom A1 which is strictly correct as it trivially follows from A3.\\
	\end{tcolorbox}
	The "distance function" of $\forall x,y \in X$ is thus usually denoted in the more possible sense in mathematics (at least as far as we know):
	
	we will see three examples much more further below with a schema.
	
	\textbf{Definition (\#\mydef):} If we do not impose the axiom A2, we say that $d$ is a "\NewTerm{semi-distance}" on $X$ and if we allow a semi-distance $d$ to take the value $+\infty$, we prefer to say that $d$is a "gap".
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} If a distance $d$ satisfies the property:
		
		property more restrictive that the triangle inequality in some spaces, we say that $d$ is "\NewTerm{ultrametric}".\\
		
		An example of ultrametric distance is the family tree (...):
		
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/family_tree.jpg}
			\caption{Example of ultrametric distance with an orgchart}
		\end{figure}
		We have the following distances:
		
		We note that the distances above do not add up, but we have by cons:
		
		Therefore:
		
		
		\textbf{R2.} Let $(X, d)$ be a metric space and consider $F=\varnothing$ a part of the set $E$. The metric space $(F,\delta)$ where $\delta$ denotes the restriction $d_{F \times F}$ of $d$ is named "\NewTerm{metric subspace}" of $(X, d)$ (we should check that the distance $d$ is equivalent to the distance $\delta$). In this case, we also say that $F$ is provided with the distance induced by this of $X$. We therefore simply note $d$ the induced distance.
	\end{tcolorbox}

	\pagebreak
	Let us give now some examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. If we take for $X$ the plane, or the three-dimensional space of Euclidean geometry and a unit of length, the "distance" in the usual sense is a distance within the meaning of the 4 axioms mentioned above. In these spaces, the three points $A, B, C$ satisfy as we have proved it in the section Vector Calculus:
	
	with other inequality obtained by circular permutation of $A, B, C$. These inequalities are well known, for example between the side lengths of a triangle.\\
	
	E2. If we take $X=\mathbb{R}^n$, $n \in \mathbb{N} \geq 1$ and that we equip $\mathbb{R}^n$ of an Euclidean vector space structure (and not non-Euclidean!) and we take two points:
	
	in $\mathbb{R}^n$, the distance is then given by (we have already proved this in the sections of Functional Analysis and Vector Calculus):
	
	\end{tcolorbox}
	\label{euclidean topology}This latter distance satisfies the five axioms of distance and we name it the "\NewTerm{Euclidean distance}" and is often denoted $L_2$. We can take (it is an interesting property for the general culture), any relation of the form:
	
	is also a distance in $\mathbb{R}^n$ (without proof) named " \NewTerm{$p$-norm}". In the particular case with $n=1$, we have of course:
	
	This is the usual distance on $\mathbb{R}$ and is often denoted $L_2$.
	
	Mathematicians are even stronger by generalizing ever more (the proof has little interest for now in this book) the prior-previous relation (taking into account the definition of the distance) in the form:
	
	which is named "\NewTerm{Hölder distance}". Also sometimes denoted for $L_p:\mathbb{R}^n\mapsto\mathbb{R}$:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Following the intervention of a reader we would like to point out that strictly speaking the above inclusion should be noted $[1,+\infty[ \subset \mathbb{\overline{R}}$ where $\mathbb{\overline{R}}$ is the achieved line (also valid for precision for the Minkowski inequality below).
	\end{tcolorbox}	
	As for the triangle inequality, then given by (\SeeChapter{see section Vector Calculus page \pageref{triangle inequality}}):
	
	The generalization, by the verification of the existence of the Hölder distance, gives us true "\NewTerm{Minkowski inequality}":
		

	Let us continue with our examples:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E3. If we take $X=\mathbb{C}$ we will consider the distance:
	
	Therefore if $z=a+\mathrm{i}b=(a,b)$ and $z'=a'+\mathrm{i}b'=(a',b')$ we have the module that the same manner as norm in $\mathbb{R}^2$, forms a distance:
	\\
	
	E4. Let us consider $E=\varnothing$ an arbitrary set. Let us write:
	
	It is quite check that this distance satisfies the five axioms and that is furthermore an ultrametric distance. This distance is named "\NewTerm{discreet distance}" and the reader will notice that, by analogy, we choosed to express this distance by the Dirac symbol $\delta$ (this is not innocent !!) rather than the traditional $d$.
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Equivalent Distances}
	Sometimes two different distances $d$ and $\delta$ on the same set $E$ are quite similar so that the related metric spaces $(E,d),(E,\delta)$ have the same properties for certain mathematical objects defined by $d$ on one hand, and by $\delta$ on the hand. There are several concepts of equivalences for example first (before the others that require mathematical tools that we have not yet defined):

	\textbf{Definition (\#\mydef):} Let $d$ and $\delta$ be two distances on the same set $E$, $d$ and $\delta$ are named "\NewTerm{equivalent distances}" if there are two real constants $c>0,C>0$ such that:
	
	Therefore:
	
	with $c\leq C$. We note this equivalence by:
	
	The advantage of this definition is the following: if we have convergence for one of the metric, then we have convergence for the other too. More clearly:
	
	verbatim:
	
	
	\subsubsection{Lipschitz Functions}\label{lipschitz functions}
	With respect to the above definitions, we can now assign some additional properties to functions such as we had define in the section of Set Theory or Functional Analysis and analysed (in part...) in the section of Differential and Integral Calculus. The idea is also mainly to build a set of tools enabling the study of differential properties of non differentiable functions.
	
	Let $(E, d)$ and $(F,\delta)$ be metric spaces, and $f:E \rightarrow E$ a function. We define the following properties:
	\begin{enumerate}
		\item[P1.] We say that $f$ is an "\NewTerm{isometry}" if (it is rather intuitive ...!):
		
		
		\item[P2.] If we take the usual distance, the $L$-Lipschitz or "\NewTerm{Lipschits function of order $L$}" is then defined by on a given interval by:
		
		that we can also write:
		
		or what remains the same: all line drawn between two arbitrary points of the graph must have a bounded and finite slope coefficient (derivative) between $L$ and $-L$.
		
		Any such $L$ is referred to as a "\NewTerm{Lipschitz constant}" for the function $f$. As $L$ can be define on intervals (not necessarily the whole domain of definition) smallest constant is sometimes named the "\NewTerm{best Lipschitz constant}".
		
		Otherwise, one can equivalently define a function to be Lipschitz continuous if and only if there exists a constant $L$ such that, for all $x\neq y$:
		
		For real-valued functions of several real variables, this holds if and only if the absolute value of the slopes of all secant lines are bounded by $k$. 
		
		Therefore all Lipschitz must be continuous and any function $f$ that has a bounded $L$ value is more restrictive than just simply being continuous! 
		
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Examples:}\\\\
		E1. The function $f(x)=\sin (x)$ is $1$-Lipschitz as the derivative of the cosine is between $-1$ and $1$.\\
		
		E2. The function $f(x)=x^2$ is locally Lipschitz as for any interval closed and finite interval we can found a bound $L$ but is not globally Lipschitz as when $x\rightarrow \pm\infty$ then the derivative has also no bounds.\\
		
		E3. The function $f(x)=|x|$ has no derivatives on $x=0$ in the ordinary sense. But its derivative in the Lipschitz sense on $x=0$ is given by the a closed interval denoted $\partial_L f(0)=[-1,1]$ given by the bound of $L$ as the ordinary derivatives is less than or equal to $L$ in absolute value!\\
		
		This last example show us that the notion of a local minimum of a function $f(x)$ in the ordinary sense is generalized with Lipschitz condition. As we can now simply define the condition of local minimum of a non-smooth function $f(x)$ as:
		
		rather than in the ordinary sense (more restrictive):
		
	\end{tcolorbox}
		
		
		Schematically for a Lipschitz continuous function, there is a double cone (shown in white) whose vertex can be translated along the graph, so that the graph always remains entirely outside the cone.
		\begin{figure}[H]
			\centering
			\includegraphics{img/analysis/lipschitz.jpg}
			\caption[Example of Lipschitz function]{Example of Lipschitz function (source: Wikipedia)}
		\end{figure}
		
		Intuitively, a Lipschitz continuous function is therefore limited in how fast it can change: there exists a definite real number such that, for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than this real number; this bound is named a "Lipschitz constant" of the function (or "modulus of uniform continuity"). For instance, every function that has bounded first derivatives is Lipschitz.
		
		\item[P3.] If $L=1$, we say that the function $f(x)$ is a "\NewTerm{short map}". If $-1<L<1$, we say that $f(x)$ is "\NewTerm{strictly contracting}\label{strictly contracting}".
		
		\item[P4.] We say that two metric spaces are "\NewTerm{isometric spaces}" if there is a surjective isometry of one over the other (which is quite natural in geometry ...).
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} An isometry is always injective as:
	
	but in general it is not surjective.\\
	
	\textbf{R2.} If $(E,d)$ and $(F,\delta)$ are isometric, of the point of view of the theory of metric spaces they are not discernible, as all their properties are the same, but their elements can be  of very different nature (sequences in one and functions in the other).\\
	\end{tcolorbox}
	
	\pagebreak
	\subsubsection{Continuity and Uniform Continuity}\label{continuity and uniform continuity}
	As we already see it in the section of Functional Analysis, a continuous function is, roughly speaking, a function for which small changes in the input result in small changes in the output and that permits the analysis of limits. Otherwise, a function is said to be a discontinuous function. Formally it was defined by:
	
	In other words remember that this mean that a function is continuous if for every point $x_0$ in the domain $E$, we can make the images of that point ($f(x_0)$) and another point ($f(x)$) arbitrarily close (of a distance $\varepsilon$) if we move the other point ($x$) close enough (distance $\delta$) to our given point.
	
	Hence it is not continuous if:
	
	
	The previous definition is not so good as it make usage of a special case of distance (the absolute value). It is therefore more common to generalize by writing:
	
	
	Now let us state a more restrictive definition!
	
	\textbf{Definition (\#\mydef):} A function $f(x)$ is "\NewTerm{uniformly continuous}" if it satisfies:
	
	with $\lambda=\varepsilon/L$ and $L\neq 0$. In other words, if we can bring two points as close as we want in a space, so can we in the other way (which ensures somehow the derivation.
	
	Hence it is not uniformly continuous if:
	
	
	If we compare the two relations:
	
	the only difference is the order of the quantifiers. Indeed, for something to be continuous, you can check "one $x$ at a time", so for each $x$, you pick a $\varepsilon$ and then find some $\varepsilon>0$ that depends on both $x$ and $\varepsilon$ so that $|f(x)-f(x_0)|<\varepsilon$ if $|x-x_0|<\delta$. If we want uniform continuity, we need to pick first a $\varepsilon$, then find a $\delta$ which is good for ALL the $x$ values we might have.
	
	As the previous definition are not quite easy for everybody let us see the engineer version of these two definitions:
	
	\pagebreak
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] A function $f(x):E \rightarrow \mathbb{R}$ is "\NewTerm{continuous}" at a point $x_0\in A$ if, for all $\varepsilon>0$, there exists a $\delta>0$ such that whenever $|x-x_0|<\delta$ (and $x\in E$) it follows that $|f(x)-f(x_0)|<\varepsilon$.
		
		\item[D2.] A function $f(x):E \rightarrow \mathbb{R}$ is "\NewTerm{uniformly continuous}" on $A$ if, for all $\varepsilon>0$, there exists a $\delta>0$ such that whenever $|x-x'|<\delta$ (and $(x,x')\in E$) it follows that $|f(x)-f(x')|<\varepsilon$.
	\end{enumerate}
	Therefore we see better the difference: continuity is defined at a point $x_0$, whereas uniform continuity is defined on a set $E$. Roughly speaking, uniform continuity requires the existence of a single $\delta>0$ that works for the whole set $E$, and not near the single point $x_0$.
	
	From this definition wee that any uniformly continuous function is continuous but the reciprocity is not true (any uniformly continuous function is not necessarily continuous):
	
	If the chosen distance is known (for example the absolute value for scalar functions such that $d=|\cdot|$ and $\delta=|\cdot|$) the previous definition notation change obviously a little bit:
	
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The function $f(x) = x^2$ is continuous but not uniformly continuous
on the interval $E = [0,+\infty[$.\\

	We prove first that our function $f(x)$ is continuous on $E$. Remember first that:
	
	In our case we can therefore write and check that:
	
	
	Let us choose $x_0=a-1$ with $a>1$ and $\delta=\min(1,\varepsilon/2a)$ (note that $\delta$ depends on $x_0$ since $a$ does). Choose $x \in S$. Assume $|x-x_0|<\delta$. Then $|x-x_0|<1$ so $x<x_0+1$ so $x,x_0<a$ so:
	
	We prove now that $f(x)$ is not uniformly continuous on $E$, i.e.:
	
	Let $\varepsilon=1$. Choose $\delta>0$. Let $x_0=1/\delta$ and $x=x_0+\delta/2$. Then $|x-x_0|=\delta/2<\delta$ but:
	
	as required.
	\end{tcolorbox}
	
	\subsection{Opened and Closed Set}
	\textbf{Definition (\#\mydef):} Consider a set $E$ with a distance $d$. A subset $U$ of $E$ is named "\NewTerm{open subset}" if, for each element of $U$, there is a non-null distance $r$ for which all the elements of $E$ whose distance from this element is less than or equal to $r$, belong to $U$, which gives in mathematical language:
	
	In topology, an open subset is then only an abstract concept generalizing the idea of an open interval in the real line.
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For recall, the symbol "|" means in this context: satisfies the property...
	\end{tcolorbox}	
	In practice, however, open sets are usually chosen to be similar to the open intervals of the real line. The notion of an open set provides a fundamental way to speak of nearness of points in a topological space, without explicitly having a concept of distance defined. 
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The points $(x, y)$ satisfying $x^2 + y^2 = r^2$ are colored blue. The points $(x, y)$ satisfying $x^2 + y^2 < r^2$ are colored red. The red points form an open subset of the plane $\mathbb{R}^2$. The blue points form a boundary set. The union of the red and blue points is a closed set.
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/opened_set.jpg}
	\end{figure}
	\end{tcolorbox}
	This definition may perhaps seem complicated but in fact, its real meaning is simpler than it seems. In fact, according to this definition, an open set in a topological space is nothing more than a set of contiguous points and without borders.
	
	The lack of border comes from the condition $r\neq 0$. Indeed, by reductio ad absurdum, if an open set $U$ had an edge, then for each point on it (the edge) it would still be possible to find a point not belonging to $U$ as close as we want from it. It follows that the distance $r$ becomes necessary therefore zero.

	\textbf{Definitions (\#\mydef):} 
	\begin{enumerate}
		\item[D1.] A "\NewTerm{closed subset}" is an "\NewTerm{open with edge}"

		\item[D2.] A "\NewTerm{neighbourhood}" of a point $E$ is a subset of $E$ containing an open subset containing this point.
	\end{enumerate}
	The definition of an open set can be simplified by introducing an additional concept, that of "open ball":
	
	\subsubsection{Balls}
	Given $x$ an element of $E$:
	
	\textbf{Definition (\#\mydef):} An "\NewTerm{open ball of center $x$ and radius $r>0$}" or "\NewTerm{metric ball of radius $r$ centered at $x$ without border}" is the subset of all the points of $E$ whose the distance $x$ is less than $r$, that we write in general:
	
	An open set can also be defined as a set for which it is possible to define an open ball at each point.
	
	Typically in the real plane where $d$ is the euclidean distance:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/open_set.jpg}
		\caption{An open ball of radius $r$, centered at the point $x$}
	\end{figure}
	An open set can also be defined as a set for which it is possible to define an open ball at each point.
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The open such defined, form what we name an "\NewTerm{induced topology}" by the distance $d$ or also a "\NewTerm{metric topology}".\\
	
	\textbf{R2.} We name an "\NewTerm{open cover}" $U$ of $E$, a set of open of $E$ whose union is equal to $E$. In other words: A collection of open sets that collectively cover another set.\\
	
	Formally, if:
	
	is an indexed family of open sets $U_\alpha$, then $C$ is a cover of $X$ if:
	
	Visually in a naive way this gives:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/open_cover.jpg}
	\end{figure}
	\end{tcolorbox}	
	\textbf{Definition (\#\mydef):} A "\NewTerm{closed ball}" is similar to an open ball but differs in the sense that we include the elements located at a distance $r$ from the center:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For $0<r<r'$ the inclusions $_oB_x^r \subset B_x^r \subset B(x,r')$ are direct consequences of the definition of the open and closed ball.
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The usual distance in $\mathbb{R}$ is given by $d(x,y)=|x-y|$. The balls are there simple intervals. For $x \in \mathbb{R}$ and $r\in \mathbb{R}_{+}^{*}$, we have:
	
	\end{tcolorbox}
	\textbf{Definition (\#\mydef):} A "\NewTerm{sphere}" is given by:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Since by definition $r>0$, open and closed balls are not empty because they contain at least their center. By cons, a sphere may be empty!
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	With $\mathbb{R}^n,\mathbb{C}^n$ we have seen in the previous examples we could set different distances. To distinguish them, we denote then by:
	
	So in $\mathbb{R}^2$ the closed balls with center O and of radius unit equivalent to the previous three formulations, have the following shapes (remember that $0<r\leq 1$ in this example):
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/shape_some_distances.jpg}
		\caption{Examples of closed balls of unit radius with different distances}
	\end{figure}
	\end{tcolorbox}
	For example in statistics (see the section of the same name) we also use (among a lot of others) the Chi-2 distance given by:
	
	Or always in (multivariate) statistics (have a look to the Statistics section but also to the Industrial Engineering one) the "\NewTerm{Mahalanobis distance}"\index{Mahalanobis distance}\label{Mahalanobis distance}:
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	The proof that the Mahalonobis distance is indeed a distance is straightforward. Indeed, without loss of generality, let us rewrite $x-\mu=\vec{x}$. Then:
	
	But as we have proved it in the section Statistics at page \pageref{positive semi-definitiveness of covariance matrix}, $\Sigma$ is positive semi-definite, hence $\Sigma^{-1}$ is most of time invertible (\SeeChapter{see section Statistics page \pageref{positive semi-definite matrix not always invertible}}). When it's invertible, all its eigenvalues are positive, then $\Sigma^{-1}$ is also positive semi-definite (\SeeChapter{see section Linear Algebra page \pageref{inverse definite positive matrix is also definite positive}}). This implies that:
	
	Therefore the Mahalonobis distance in indeed a distance. Another way to see it as engineer is just to consider that as $\Sigma^{-1}\vec{x}$ simply gives just another vector $\vec{y}$ such that:
	
	This it's just the euclidean distance between two different vectors!
	\end{tcolorbox}	
	For any other semi-positive definite matrix $A$ other than the variance-covariance matrix we also define the "\NewTerm{elliptic metric}"\index{elliptic metric}\label{elliptic metric}:
	
	
	Or in the Error Correcting Codes section we use the Hamming distance given by:
	
	and so on...
	
	\subsubsection{Partitions}
	Now that we have defined the concepts of balls, we can finally (almost) rigorously define the concepts of open and closed intervals (which in a space of more than one dimension are named "partitions") that we have so often used in the section of Functional Analysis and Differential and Integral Calculus.
	
	\textbf{Definition (\#\mydef):} Let $(X,d)$ of a metric space. We say that a subset $A$ of $X$ is "bounded" if there is a closed ball $_fB_{r_0}^r(X)$ such that $A \subseteq _fB_{r_0}^r(X)$:
	
	Given the previous note on balls inclusions, it is clear that we can replace the word "closed" by"open". Moreover the triangle inequality implies that the bounded character of $A$ does not depend on the choice of $x_0$ (with a $x_0^{\prime}$ we simply need to replace $r$ by $r'=r+d\left(x_0,x_0^{\prime}\right)$).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	The odd–even topology is the topology where $X = \mathbb{N}$ and:
	
	the unbounded partitions of radius $r<1$. \\
	
	 Therefore we see that unless $P$ is trivial, at least one set in $P$ contains more than one point, and the elements of this set are topologically indistinguishable: the topology does not separate points!
	\end{tcolorbox}
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Let $X$ be a set and $(Y,d)$ a metric space. If $X$ is a set, we say that a function $f: X\mapsto Y$ is "bounded" if its image $f (X)$ is bounded (the case of the sine or cosine function, for example).
		
		\item[D2.] Given $(E, d)$ a metric space, and given $A$ a non-empty subset of $E$. For any $u\in E$ we note $d(u, A)$ and name "\NewTerm{distance $u$ to $A$}", the positive real number:
		
		We extend the concept by writing:
		
		If $A$ and $B$ are two parts (subsets) of $E$ we have respectively (perhaps this is more understandable in this way for some readers...)
		
		The reader must take care here to interpret $d(A,B)$ as the infinimum of the distance between the sets $A$ and $B$, because the distance between the parties does not always define a distance in the usual way on the part for example of $\mathcal{P}(\mathbb{R})$.
		
		Indeed, if we take again our famous example:
			
	 	we have $d(A,B)=0$ when $n \rightarrow 0$ while $A\neq B$.
	 	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} If the reader has well understood the definition of "parts" (and especially the previous example) he has probably noticed that it does not necessarily always exist a $a\in A$ such that $d(u,A)=d(u,a)$. Accordingly, we write:
		
		Moreover, if such an $\alpha$ exists, it is obviously not necessarily unique.\\
		
		\textbf{R2.} It should be remembered that this distance also meets the $5$ axioms of distances (we can give the proof on request)!
		\end{tcolorbox}	
	
		\item[D3.] Given $(E, d)$ a metric space, and let $A$ be a part (subset) of $E$. We name "\NewTerm{adhesion}" of $A$ and denote by $\text{adh} (A)$ the subset of $E$ defined by:
		
		For example, the adhesion of the part (subset) of rational numbers $\mathbb{Q}$ (part $A$) of $\mathbb{R}$  (the metric space $E$) is a subset of $\mathbb{R}$ itself since any real number is the limit of a rational.
		
		Especially, since $\forall u\in A:\quad =+\infty$, we have $\text{adh}(\varnothing)=\varnothing$, and since $\forall u\in E:\quad d(u,E)=0$, we have $\text{adh}(E)=E$.
		
		\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
		\textbf{R1.} Any element of the set $\text{adh}(A)$ is named "adherent point" of $A$.\\
		
		\textbf{R2.} We say that a part $A$ of $E$ is and "\NewTerm{closed part}" if it is equal to its adherence.\\
		
		\textbf{R3.} We say that a part $A$ of $E$ is an "\NewTerm{open part}" if its complementary relatively to $E$:
		
		is closed.
		\end{tcolorbox}	
	\end{enumerate}
	It follows from the definitions that (without proof):
	
	and:
	
	with some properties (supposed as very obvious but we can give the proof on request):
	\begin{enumerate}
		\item[P1.] If $A\subset E$ and $B\subset E$ satisfies $A\subset B$, then we have:
		
		
		\item[P2.] For all $A\subset E$, any $u\in E$ we have:
		
		The latter property has for corollary (obvious and therefore without proof excepted on request):
		If for any $u\in E$, we have $d(u,A)=d(u,B)$ and $A,B\neq \varnothing$, we then have:
		
	\end{enumerate}
	
	\subsubsection{Formal Ball}
	The concept of distance from a point to a set gives the possibility to extend the notions of ball and sphere see previously. We will see now the basis concepts of a "\NewTerm{formal ball}" also named "\NewTerm{generalized ball}".
	
	\begin{enumerate}
		\item[D1.] Given $A\neq \varnothing$ and given a $r>0$. We name "\NewTerm{generalized open ball}" of center $A$ of radius $r$, the following set:
		
		and respectively "\NewTerm{generalized closed ball}":
		
		and respectively  a "\NewTerm{generalized sphere}":
		
		
		\item[D2.] Given $(E, d)$ a metric space and let $A, B$ be two non-empty parts (subsets) of $E$. We denote by $g (A, B)$ and name "\NewTerm{gap}" of $A$ to $B$, the real number greater than or equal to zero such that:
		
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		The triangle inequality $g(A,B)\leq g(A,C)+g(C,B)$ is not valid in the context of gaps. To prove it, a single example that contradicts this inequality is sufficient.
		\end{tcolorbox}
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		In $\mathbb{R}$ let us take $A=\{0,1\},B=\{2,3\},C=\{1,3\}$ then we have:
		
		\end{tcolorbox}	
	\end{enumerate}
	
	\subsubsection{Diameter}
	\textbf{Definition (\#\mydef)}: Given $(E, d)$ a metric space and $A$ a non-empty part (subset) of $E$. We denote $\text{diam}(A)$ and name "\NewTerm{diameter}" of $A$, the positive non-zero real number:
	
	Every non-empty part (subset) $A$ of a metric space satisfying $\text{diam}<+\infty$ will also be say "bounded".
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We consider the empty set $\varnothing$ as bound set of diameter $A$.
	\end{tcolorbox}	
	If the whole metric space $(E,d)$ is bounded, we say that the distance $d$ is bounded. For example, the discrete distance is limited, the usual distance on $\mathbb{R}$ is not.
	
	We also have the following properties (the first two are usually trivial, the third one comes from the definition of the diameter itself):
	\begin{enumerate}
		\item[P1.] $\text{diam}(A)=0 \Leftrightarrow A=\{a\}$ or $A=\varnothing$
		
		\item[P2.] $A\subset B \Rightarrow \text{diam}(A)\leq \text{diam}(B)$
		
		\item[P3.] $\text{diam}(_fB_x^r)\leq 2r,\text{diam}(_oB_x^r)\leq 2r,\text{diam}(S_x^r)\leq 2r$
		
		\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
		\bcbombe Caution!!! Concerning the latter property, the reader must take the habit of thinking with the Euclidean distance. The first common pitfall is to think that the second diameter (that of the open ball) should be strictly less but that would be forgetting that the board has no thickness strictly speaking! 
		\end{tcolorbox}
		
		There is also often a understanding problem with $\text{diam}(S_x^r)\leq 2r$. To be convinced just take the discrete distance (that for two points that are not confused is equal to $1$, otherwise $0$). Thus, in a metric space where we take $S_x^r$ with $r=1$, we have indeed $\text{diam}(S_x^1)\leq 1$ (that is an interesting case because almost completely counter-intuitive).
		
		\item[P4.] $\text{diam}(A\cup B)\leq \text{diam}(A)+g(A,B)+\text{diam}(B)$
		
		To be convinced, in $\mathbb{R}$ take $A=B$, then we have (trivial strict inferiority):
		
		
		\item[P5.] $A$ is bounded if and only if: $\exists r>0,\exists x\in E:\quad A\subset _oB_x^r$
	\end{enumerate}

	\textbf{Definition (\#\mydef):} We name "\NewTerm{Hausdorff excess}" or "\NewTerm{Hausdorff distance}" from $X$ to $Y$:
	
	that we found often in the literature with the more condensed notation:
	
	or much more explicitly:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/hausdorff_distance.jpg}
		\caption[]{Components of the calculation of $d_H$ between $X$ and $Y$ (source: Wikipedia)}
	\end{figure}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Let us take $X\subset \mathbb{R}^2$ as the unit radius circle centered at the origin and $Y$ to the square circumscribing it. Elementary geometry concepts obviously leads to finding that the Hausdorff distance between the circle and the square is therefore:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/hausdorff_distance_highschool_example.jpg}
		\caption{High-school example of a Hausdorff distance in the plane}
	\end{figure}
	technically:
	
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	We have generally $e(X,Y)\neq e(Y,X)$ and these quantities may not be finite.
	\end{tcolorbox}
	
	\subsection{Varieties}\label{varieties}
	We now introduce the "varieties". These are topological spaces that are "locally as $\mathbb{R}^2$" (our space for example ...), that is locally euclidean.
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] A "\NewTerm{topological variety of dimension $n$}" is a Hausdorff space $M$ such that for every $p\in M$ there exists an open neighbourhood $U\subset M$ with $p\in U$, an open neighbourhood $U' \subset \mathbb{R}^n$ and a homeomorphism such that:
		
		\item[D2.] A "\NewTerm{homeomorphism}" between two spaces is a continuous bijection whose inverse is also continuous.

		\item[D3.] The pairs $(U,\varphi)$ are named "\NewTerm{maps}", $U$ being the "\NewTerm{domain of the map}" and $\varphi$ the "\NewTerm{coordinate application}". Instead of "map" sometimes we say also "coordinate system".

		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		We will denote by $\dim(M)$ the dimension of a topological variety. Therefore:
		
		\end{tcolorbox}
	
		\item[D4.] Given $M$ be a topological variety of dimension $n$. A family $A$ of maps of $M$ family is named a  "\NewTerm{atlas}" if for each $x\in M$, there is exists a map $(U, \varphi)\in A$ such as $x\in U$.
	\end{enumerate}
	If $(U_1,\varphi_1),(U_2,\varphi_2)$ are two maps of $M$ such as $U_1\cap U_2\neq \varnothing$, then the application of map changes:
	
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/homeomorphism_of_maps.jpg}
		\caption{Maps homeomorphism}
	\end{figure}
	is obviously also a homeomorphism. More "geometrically" it looks like this:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.6]{img/analysis/maps_homeomorphism.jpg}
		\caption[Intuitive maps homeomorphism]{Intuitive maps homeomorphism (source: ?)}
	\end{figure}
	
	\pagebreak
	\subsubsection{Subvariety}
	\textbf{Definition (\#\mydef):}A subset of a variety is itself a variety named a "\NewTerm{subvariety}\index{subvariety}\label{subvariety}". 

	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	A sphere of the three-dimensional Euclidean space $\mathbb{R}^3$ is an (algebraic) variety since it is defined by a polynomial equation. For example and is obviously smooth and locally euclidean:
	
	defines the sphere of radius $1$ centered at the origin. Its intersection with the $xy$-plane is a circle given by the system of polynomial equations:
	
	Hence the circle is itself an algebraic variety, and a subvariety of the sphere, and of the plane as well.
	\end{tcolorbox}
	

	\pagebreak
	\subsubsection{Surfaces Homeomorphism}
	\textbf{Definition (\#\mydef):} In the mathematical field of topology, a "\NewTerm{homeomorphism} or "\NewTerm{topological isomorphism}" is a continuous function between topological spaces that has a continuous inverse function. Roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. 
	
	More formally, remember that an application $\varphi: X \mapsto Y$ between two topological spaces is named a homeomorphism if it has the following properties:
	\begin{enumerate}
		\item $\varphi$ is a bijection (one-to-one and onto)
		
		\item $\varphi$ is continuous
		
		\item The reciprocal function $\varphi^{-1}$ is continuous 
	\end{enumerate}
	
	Can we say that a square (being a special map in $\mathbb{R}^2$) is homeomorph to circle (being another special map in $\mathbb{R}^2$), or a torus to a cup of tee... If this is possible we must be able to find a closed form bijective expression between the two surfaces.
	
	As the pure theoretical concepts are very not friendly in our point of view let us begin with a two dimension special case. Let us show (prove) first that we can transform all interior points of  square of side $1$ into all interior points circle of radius $1$. This is represented by the  know figure:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/isomorphism_circle_square.jpg}
	\end{figure}
	Such mappings have particular interest in industrial design or just simply for communication purposes (Photoshop effects or Statistics charts deformation as we do many times in the \texttt{R} Software):
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{img/analysis/chessboard_isomorphic_circle_square.jpg}
	\end{figure}
	or for defishing fish eyes captors, picture or security mirrors:
	\begin{figure}[H]
		\centering
		\includegraphics[scale=1.25]{img/analysis/defishing.jpg}
	\end{figure}
	Recall that we defined unit disc as the set:
	
	If we think of the unit disc as a continuum of concentric circles with radii growing from zero to one, we can parametrize the unit disc as the set:
	
	In doing so, we introduced a parameter $t$ this is the distance of point $(u,v)$ to the origin.
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/continnum_disc.jpg}
	\end{figure}
	In analogy to the circular continuum of the unit disc, one can write the square region $[-1,1] \times [-1,1]$ as the set:
	
	In other words, the square can be considered as a continuum of concentric shrunken FG-squircles (\SeeChapter{see section Analytical Geometry page \pageref{fg squircle}}).
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/continnum_square.jpg}
	\end{figure}

	Topologists denote the proof that the interior points of two geometries are homeomorph in this special case using the following notation
	
	We will now show that $\mathring{\mathcal{D}}\mapsto \mathring{\mathcal{D}}$ as it is the most common case in practice in our point of view. That is to say:
	\begin{figure}[H]
		\centering
		\includegraphics{img/analysis/mapping_circle_to_square.jpg}
	\end{figure}
	We can establish a correspondence between the unit disc and the square region by mapping every circular contour in the interior of the disc to a squircular contour in the interior of the square. In other words, we map contour curves in the circular continuum of the disc to those in the squircular continuum of the square. This can be done by equating the parameter $t$ of both sets to get the equation:
	
	We name this equation the "\NewTerm{squircularity condition}" for mapping a circular disc to a square region.
	
	It is easy to derive the FG-Squircular mapping by combining the squircularity condition:
	
	That we can also write:
	
	Using radial to cartesian coordinates (\SeeChapter{see section Vector Calculus page \pageref{polar coordinates}}):
	
	Therefore by equivalence we get:
	
	After substitution of parameter $t$, we get:
	

	In other words, this is a radial mapping that converts circular contours on the disc to squircular contours on the square.
	
	We shall now derive the inverse equations for the FG-Squircular mapping. But as it is boring to write in \LaTeX{} and it is not used to much in practice we will omit the latter for the moment.
	
	\subsubsection{Differential Varieties}
	\textbf{Definitions (\#\mydef)}:
	\begin{enumerate}
		\item[D1.] A "\NewTerm{differentiable variety}" is a topological space $M$ where the applications $\varphi$ are of class $\mathcal{C}^{+\infty}$.

		\item[D2.] A "\NewTerm{diffeomorphism}" is an application where $\varphi: U\mapsto U' $ where $U,U'$ are open domains of $\mathbb{R}^n$ and if $\varphi$ is a homeomorphism and furthermore $U,U'$ are differentiable!
		\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
		"Differentiable" in this context will always mean of class $\mathcal{C}^{+\infty}$.
		\end{tcolorbox}

		\item[D3.] Given a topological variety $M:=M^n$ (to simplify the notations), two maps $(U_1,\varphi_1),(U_2,\varphi_2)$ of $M$ are named \NewTerm{compatible maps} (more precisely: compatible of class $\mathcal{C}^{+\infty}$ if one of these two properties is satisfied:
		\begin{enumerate}
			\item[P1.] $U_1\cap U_2\neq \varnothing$ and the application $\varphi_2\circ \varphi_1^{-1}$ of map changes is a diffeomorphism.

			\item[P2.] $U_1\cap U_2 =\varnothing$
		\end{enumerate}
		An atlas $A$ of $M$ is differentiable if all maps of $A$ are compatible between them.
	\end{enumerate}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Given a differentiable atlas, it is sometimes necessary to complete it: we say that a map of $M$ is compatible with a differentiable atlas if it is compatible with every map of $A$. An atlas of $A$ is a "\NewTerm{maximal atlas}" if every compatible map of $A$ belongs already to $A$. A maximal atlas is named a "\NewTerm{differentiable structure}".
	\end{tcolorbox}

	\begin{flushright}
	\begin{tabular}{l c}
	\circled{70} & \pbox{20cm}{\score{4}{5} \\ {\tiny 12 votes,  71.67\%}} 
	\end{tabular} 
	\end{flushright}

	%to make section start on odd page
	\newpage
	\thispagestyle{empty}
	\mbox{}
	\section{Measure Theory}\label{measure theory}
	\begin{tcolorbox}[colback=red!5,borderline={1mm}{2mm}{red!5},arc=0mm,boxrule=0pt]
	\bcbombe Caution! The level of abstraction and of motivation required for reading and understanding this section is quite high for engineers (target audience of this book for recall). The reader should be comfortable with the concepts seen in the section Set Theory as well as the one one Topology. We also apologize for the actual lack of figures. 
	\end{tcolorbox}
	\lettrine[lines=4]{\color{BrickRed}T}he measure, in the topological sense, will allow us to generalize the elementary notion of measure of a segment or area (in the Riemann sense, for example) and is inseparable from the new theory of integration that will build Lebesgue from the years to 1901-1902 and we will address here to build mathematical tools much more powerful than the simple Riemann integral (\SeeChapter{see section Differential and Integral Calculus page \pageref{riemann integral}}) with practical and numerical example in MATLAB\textsuperscript{TM}.
	
	The philosophers of science who developed measurement theory were largely concerned with epistemic questions like: we can't observe correlations between physical objects and real numbers, so how can the use of real numbers be justified in terms of things we can observe? Indeed, privileges a single unit of mass, involves real numbers in the facts of
mass. Why is the latter bad?: 
	\begin{enumerate}
		\item Real numbers are abstract and therefore causally inert
		
		\item Real numbers don't fundamentally exist
		
		\item Real numbers are constructed entities, and constructed entities can't be involved  in fundamental facts
	\end{enumerate}

	The Measure Theory will also allow us to rigorously define the concept of measurement (no matter what is the measure) and so return to the important results of the study of probabilities (\SeeChapter{see section Probabilities page \pageref{probabilities}}). Indeed, we will see (we will define the vocabulary that follows just now further below) why $(U, A, P)$ is a "\NewTerm{probability space}" where $A$ is in fact a "\NewTerm{tribe}" on $U$ and $P$ a measure on the measurable space $(U , A)$.
	
	\subsection{Measurable Spaces}
	When in mathematics we calculate derivatives, primitives or simply count stuff, we carry implicitly a measure of an object or set of objects. Rigorously, mathematicians want to define how the measured thing can be structured, how to make a measurement of it and the properties resulting!
	
	\textbf{Definitions (\#\mydef):}
	\begin{enumerate}
		\item[D1.] Let $E$ be a set, a "\NewTerm{tribe}" on $E$ is a family $\mathcal{A}$ (this notation comes from the fact that many people speak of "\textbf{A}lgebra sets" instead of "tribe") of subsets of $E$ satisfying the following axioms:
		\begin{enumerate}
			\item[A1.] $E\in \mathcal{A}$ (see examples below - $E$ being one of the possible elements of $\mathcal{A}$).
			
			\item[A2.]  If $A$ is a member of a tribe then:
			
			This means that $\mathcal{A} $ is "\NewTerm{stable by transition to complementary}". This axiom implies that the empty set is always an element of a tribe!
			
			\item[A3.] For any sequence $(A_n)$ of elements of $\mathcal{A}$ we have:
			
			 We then say that $\mathcal{A}$ is then "\NewTerm{stable by countable union}".
		\end{enumerate}
		For example, the graduating from a simple ruler of measurement ... satisfies these three axioms!
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} We write $E\in \mathcal{A}$ because we consider with this notation $E$ not anymore as a subset of $\mathcal{A}$  but as an element of $\mathcal{A}$!\\
	
	\textbf{R2.} The uncountable cases are typical of topology, statistics or integral calculus!
	\end{tcolorbox}	
	
	\item[D2.] The pair $(E,\mathcal{A})$ is named "\NewTerm{measurable space}" and we say that the elements of $\mathcal{A}$ are "\NewTerm{measurable sets}".
	
	\item[D3.] If in the third axiom we require that $\mathcal{A}$ is stable under finite (uncountable) union then we impose the more general notion of "\NewTerm{$\sigma$-algebra}\label{sigma algebra}". Thus, a tribe is necessarily contained in an $\sigma$-algebra (but the opposite is not true just because the axiom is stronger) such that we can write:
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	In the field of probabilities, $E$ is assimilated to the Universe of events and $\mathcal{A}$ to a family of events and we speak then of "\NewTerm{probabilistic space}" or simply of... "\NewTerm{measurable space}".
	\end{tcolorbox}	
	\end{enumerate}
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	E1. Given $E=\{1,2\}$ a set of cardinal 2... The only two tribe $\mathcal{A}$ that satisfy the three axioms are:
	
	There are no other tribes for the set $E$ as these two (the normal one, and the maximum one), because we must not forget that the union of each elements of the tribe must also be in the tribe (axiom A3), and also the complement of a member (axiom A2).\\
	
	We also see from this example that if $E$ is set then $\{E,\varnothing\}$ is indeed a tribe!\\
	
	E2. The set of parts of $E$, denoted $\mathcal{P}(E)$ is also a tribe (dixit previous example).
	\end{tcolorbox}
	A tribe $\mathcal{A}$ is also "\NewTerm{stable by the union of the finite complementaries}". Indeed, if $(A_n)$ is a sequence of elements of $\mathcal{A}$ we have (trivial when taking for example the previous first example):
	
	A tribe is also "\NewTerm{stable by finite intersection}", that is to say (trivial also by taking the previous first example):
	
	which brings to the property that a tribe is stable by finite unions and intersections. Especially, if we take two elements of a tribe $A,B\in \mathcal{A}$, then $A\setminus B\in \mathcal{A}$ with for recall (\SeeChapter{see section Set Theory page \pageref{symmetric difference}}):
	
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	Most readers should probably easily see with the previous first example that if $(\mathcal{A}_i)_I$ is a family of tribes on $E$ the $\bigcap_I \mathcal{A}_i$ is also a tribe (the verification is almost immediate).
	\end{tcolorbox}	
	Well it is nice to play with potatoes and sub-potatoes... and their complementary but let us continue...
	
	\textbf{Definition (\#\mydef):} Given $E$ a set and $\mathcal{B}$ a family of subsets of $\mathcal{P}(E)$ such that $\mathcal{B}\subset \mathcal{P}(E)$. We denote by definition:
	
	the "\NewTerm{generated tribe}" by $\mathcal{B}$. Therefore $\sigma(\mathcal{B})$ is by definition the smallest tribe containing $\mathcal{B}$ (and by extension the smallest tribe of $E$).

	Below are three small examples that gives the opportunity to check if what precedes has been well understood and that also gives the possibility to highlight important results for what will follow:
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Examples:}\\\\
	Given a set $E$ and $A\subset E,A \neq E$ and also $\mathcal{B}=\{A\}$ then (when $A$ is seen as a subset of $E$ as given by the statement of a family of subsets!):
	
	
	E2. If $\mathcal{A}$ is a tribe on $E$ then:
	

	E3. Given $E=\{1,2,3,4\}$ and $A=\{\{1,2\},{3}\}$ we then have (take care because now $A$ is a family of parts (subsets) and not only a unique subset!) the following generated tribe:
	
	Rather than determining this tribe by seeking the smallest tribe $\mathcal{P}(E)$ containing $A$ (which would be laborious) we play with the axioms defining a tribe to easily find it.
	
	So therefore we find well in $\sigma(A)$ at least the obligatory empty set $\{\varnothing\}$ and also:
	
	following the axiom A1 and:
	
	itself by the definition of $\sigma(A)$ and the complementaries of:
	
	following the axiom A2 and also the unions:
	
	following the axiom A3.
	\end{tcolorbox}
	\textbf{Definition (\#\mydef)}: Let $E$ be a topological space (\SeeChapter{see section Topology page \pageref{topological space}}). We denote by $\mathcal{B}(E)$ the tribe generated by the open sets of $E$. $\mathcal{B}(E)$ is named the "\NewTerm{borelian tribe}" of $E$. The elements of $\mathcal{B}(E)$ are named the "\NewTerm{borelians}" of $E$. 
	
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} The notion of borelian tribe is especially interesting because it is necessary for the definition of "Lebesgue tribe" and afterwards to the "Lebesgue measure" that will lead us to define the  famous "Lebesgue integral"!\\
	
	\textbf{R2.} The tribe $\mathcal{B}(E)$ being stable by going to the complementary, it also contains all closed subsets.\\
	
	\textbf{R3.} If $E$ is a topological space with a finite basis, $\mathcal{B}(A)$ in generated by the opens of the basis.
	\end{tcolorbox}	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	If $\mathbb{R}$ designates the space provided of real numbers with the Euclidean topology (\SeeChapter{see section Topology page \pageref{euclidean topology}}), the family of open intervals with rational bounds is a "\NewTerm{countable base}" (given the bounds...) of $\mathbb{R}$ and therefore generates $\mathcal{B}(\mathbb{R})$ . Same thing for $\mathbb{R}^d$ with for countable basis the family of open spaces with rational bounds.
	\end{tcolorbox}
	\begin{theorem}
	Let us now consider a dense set (\SeeChapter{see section Topology page \pageref{dense set}}) in $\mathbb{R}$. The following families generate $\mathcal{B}(\mathbb{R})$:
	
	\end{theorem}
	\begin{dem}
	Given (the family of open subsets):
	
	We have obviously:
	
	Furthermore:
	
	Therefore the intervals of the type $[a,b[$ with $a$ and $b$ in $\mathcal{S}$ also belongs to $\sigma(\mathcal{F})$. Therefore, if we generalize, with $x<y$, it exists a sequence $(a_n)$ of elements of $\mathcal{S}$ decreasing to $x$ and a sequence $(b_n)$ of elements of $\mathcal{S}$ increasing to $y$ such that:
	
	which bring in the same way as $E\in \mathcal{A}$ that $\mathcal{B}(\mathbb{R})\subseteq\sigma(\mathcal{F})$. Other cases can be treated analogously.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	\begin{theorem}
	Given $(E, \mathcal{A})$ a measurable space and $A\subseteq E$ (and $A\in \mathcal{A}$) (where $A$ is therefore considerate as a subset and non as an element!). The family $\{A\cap B| B\in \mathcal{A}\}$ is a tribe on $A$ named "\NewTerm{trace tribe}" of $\mathcal{A}$ on $A$, that we will denote by $A\cap \mathcal{A}$. Furthermore, if $A\in \mathcal{A}$, the trace tribe is formed by the measurable elements contained in $A$.
	\end{theorem}
	\begin{dem}
	We will do a proof by the example (... yes it is not a real proof...). For this we check the three points that define a tribe:
	\begin{enumerate}
		\item $A=(E\cap A) \Rightarrow A\in (A\cap \mathcal{A})$
		
		\item Given $B\in\mathcal{A},A \setminus (A\cap B)=A\setminus B=A \cap B^c=A \cap B^c$ and therefore $A\setminus (A\cap B)\in (A\cap \mathcal{A})$
		\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
		\textbf{{\Large \ding{45}}Example:}\\\\
		Given $E=\{1,2,3\}$ then (a tribe among others - do not forget the stability by union!):
		
		Let us choose $A=\{1,2\},B=\{2,3\}$ (it is obvious that $\{A\cap B|B\in \mathcal{A}\}$ is a tribe on $A$). Then:
		
		and we have well $\{1\}\in A$ and also $A\in (A\cap \mathcal{A})$.
		\end{tcolorbox}
		
		\item Given $(A\cap B_n)$ a sequence of elements of $A\cap \mathcal{A}\; (B_n\in \mathcal{A})$ then:
		
		The last statement of the proposition will be supposed as obvious (if not, let us know!).
	\end{enumerate}
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	Given now $E$ a set, $\mathcal{C}$ a family of subsets of $E$ and $A\subseteq E$ non empty. We denote by $A\cap \mathcal{C}$, the trace $\mathcal{C}$ on $A$ and $\sigma_A(A\cap \mathcal{C})$ the tribe generated on $A$. Therefore:
	
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given the set $E=\{1,2,3,4\},\mathcal{C}=\{\{1,2\},\{3\}\},A=\{3,4\}$ then:
	
	and let us check that $A\cap \sigma(\mathcal{C})=\sigma_A(A\cap \mathcal{C})$:
	
	So the equality is satisfied!
	\end{tcolorbox}
	A trivial corollary of this equality is that if we consider a topological space $ E$ and $A\subseteq E$ with the induced topology, then:
	
	We will study more in details $\sigma$-algebra in measurement theory but first let us recall that a tribe (sometimes named "algebra of sets") on $E$ must satisfy the following properties:
	\begin{enumerate}
		\item[P1.] Has to contain $E$
		\item[P2.] Must be stable by the complementary
		\item[P3.] Must be stable by countable union or intersection
	\end{enumerate}
	and a $\sigma$-algebra on $E$ is less restrictive than a tribe as it has to satisfy:
	\begin{enumerate}
		\item[P1.] Has to contain $E$
		\item[P2.] Must be stable by the complementary
		\item[P3.] Must be stable by finite (uncountable) union or intersection
	\end{enumerate}
	Let us recall (\SeeChapter{see section Set Theory page \pageref{symmetric difference}}) that if we have $E$ that is a set, then for every $A,B\subseteq E$ we define the symmetric difference $A\Delta B$ between $A$ and $B$ by:
	
	Trivial properties are as follows:
	\begin{enumerate}
		\item[P1.] A $\sigma$-algebra is stable by symmetric difference ($A,B\in \mathcal{A}$ we have $A\Delta B\in \mathcal{A}$)
		
		\item[P2.] $A\Delta B=B\Delta A$
		\item[P3.] $A^c\Delta B^c=A\Delta B$
			
		\item[P4.] $A\Delta B=(A\cup B)\setminus (A\cap B)$
	\end{enumerate}
	\begin{theorem}
	If $\mathcal{B}$ is a $\sigma$-algebra over $E$, then $(\mathcal{B},\Delta,\cap)$ is a "\NewTerm{Boolean ring}" (or "Boolean algebra" but be careful with the term "algebra" here which can cause confusion with the corresponding structure in Set theory) with $\varnothing$ and $E$ as neutral "additive" element ($\Delta$) and respectively" multiplicative" ($\cap$).
	\end{theorem}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	For reminders on the items listed in the preceding paragraph, the reader can refer to the section of Set Theory page \pageref{set theory} and the subsection of Boolean Algebra (\SeeChapter{see section Formal Logic Systems page \pageref{boolean algebra}}).
	\end{tcolorbox}	
	\begin{dem}
	The "addition" $\Delta$ is associative because developing we get (this can verified by an arrow diagram if needed - the "potatoes"):
	
	and the latter expression is stable by permutation (commutation) of $A$ and $C$ (same method of verification). Therefore:
	
	We check that $\varnothing$ is neutral with respect to the symmetric difference (the proof that $E$ is neutral with respect to inclusion is obvious). It is trivial that:
	
	$(\mathcal{B},\Delta,\varnothing)$ is therefore well an Abelian group with respect to the law $\Delta$ (symmetric difference).
	
	Finally $\cap$ is distributive with respect to $\Delta$. Indeed:
	
	What makes $(\mathcal{B},\Delta,\varnothing)$ is indeed a ring (furthermore of a commutative ring!).
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}

	\pagebreak
	\subsubsection{Monotone Classes}
	\textbf{Definition (\#\mydef)}: Let $E$ be a set. A "\NewTerm{monotone class}" on $E$ is a family $\mathcal{C}$ of subsets of $E$ satisfying the following axioms:
	\begin{enumerate}
		\item[A1.] $E\in \mathcal{C}$
		\item[A2.] $A,B\in \mathcal{C}$ and $A\subseteq B \Rightarrow \setminus A\in \mathcal{C}$
		\item[A3.] If $(A_n)$ is an increasing sequence (take care to the word "increasing"!) of elements of $\mathcal{C}$ then $\displaystyle\bigcup_{i=1}^{+\infty} A_i\in \mathcal{C}$ (stable by countable increasing union).
	\end{enumerate}
	\begin{tcolorbox}[title=Remarks,colframe=black,arc=10pt]
	\textbf{R1.} An increasing sequence of sets is: $A_1\subseteq A_2\subseteq A_3 ...$\\
	
	\textbf{R2.} The first two axioms imply that $\mathcal{C}$ is by complementary.\\
	
	\textbf{R3.} The three axioms together leads to that the monotonous class is stable by decreasing intersection. A way to check this to take the complement of each element of the increasing sequence to fall back on the decreasing sequence and vice versa.
	\end{tcolorbox}
	Every $\sigma$-algebra is a monotone class, because $\sigma$-algebras are closed under arbitrary countable unions and intersections.
	
	Therefore:
	
	In the same way as for the tribes, if we consider a family $(\mathcal{C}_i)_I$ of monotone class on $E$. Then $\bigcap_I \mathcal{C}_i$ is a monotone class (the proof is verified immediately by the three previous axioms).
	\begin{tcolorbox}[colframe=black,colback=white,sharp corners]
	\textbf{{\Large \ding{45}}Example:}\\\\
	Given $E$ a set, $\mathcal{P}(E)$ is a monotone class on $E$. More generally, a tribe is a monotone class.

	Equivalently to tribes, let us consider a set $E$ and $\mathcal{C}\subseteq \mathcal{P}(E)$. Given $\mathcal{S}$ the family of all monotone class containing $\mathcal{C}$, $\mathcal{S}$ is not empty because $\mathcal{P}(E)\in \mathcal{S}$. We denote by:
	
	the monotone class generated by $\mathcal{C}$. Therefore $\mathcal{M}(\mathcal{C})$ is the smallest monotone class containing $\mathcal{C}$ (and satisfying obviously the previous axioms).
	\end{tcolorbox}
	\begin{tcolorbox}[title=Remark,colframe=black,arc=10pt]
	If $E$ is a set and $\mathcal{C}\subseteq \mathcal{P}(E)$ then $\mathcal{C}\subseteq \sigma(\mathcal{C})$, as $\sigma(\mathcal{C})$ is a monotone class (and also a tribe) containing $\mathcal{C}$ and therefore contains also $\mathcal{M}(\mathcal{C})$ (see the examples with tribes).
	\end{tcolorbox}	
	 \begin{theorem}
	Given $E$ as set. If $\mathcal{C}$ is a family of parts of $E$ that we impose as stable by finite intersection then $\mathcal{C}=\sigma{\mathcal{C}}$ (we then have to prove that the smallest tribe of $\mathcal{C}$ is equal to the smallest monotone class of $\mathcal{C}$. If we do not impose that $\mathcal{C}$ is stable by finite intersection we would not have necessarily the equality!
	\end{theorem}
	\begin{dem}
	As already said: $\mathcal{M}(\mathcal{C})\subseteq \sigma(\mathcal{C})$ (which is trivial). We will prove first that $\mathcal{M}(\mathcal{C})$ is a tribe on $E$. For this it is sufficient to show that $\mathcal{M}(\mathcal{C})$ is (also) stable by countable union (and not necessarily by an increasing sequence of elements!).
	
	Let us considerate following families for the proof:
	
	By the previous definitions $\mathcal{M}_1\subseteq \mathcal{C}$ but $\mathcal{C}$ being (imposed) stable by finite intersections implies that $\mathcal{C}\subseteq \mathcal{M}_1$ and therefore (it is the same reasoning as for the tribes):
	
	$\mathcal{M}_1$ is a monotone class, indeed $E\in \mathcal{M}_1$, if $A_1,A_2 \in \mathcal{M}_1$ and that $A_1\subseteq A_2$ (second axiom) then:	
	
	and therefore (which supports the fact that the other elements $(A_n)$ satisfy the previous relation):
	
	If $(A_n)$ is an increasing sequence of elements of $\mathcal{M}_1$ then:
	
	as $(A_n\cap B)$ is an increasing sequence.
	
	Therefore $\mathcal{M}_1$ is indeed a monotone class and by $\mathcal{C}\subseteq \mathcal{M}_1 \subseteq \mathcal{M}(\mathcal{C})$, we therefore have:
	
	The latter equality implies $\mathcal{C}\subseteq \mathcal{M}_2$. As for $\mathcal{M}_1$, we show that  $\mathcal{M}_2$ is a monotone class and therefore $\mathcal{C}= \mathcal{M}_2$, which means by extension that $\mathcal{M}(\mathcal{C})$ is therefore stable by finite intersections.
	
	$\mathcal{M}(\mathcal{C})$ being stable by complementary this take us to that $\mathcal{M}(\mathcal{C})$  is, we just proved it, stable by finite unions (but we want to prove that it is stable by countable union!).
	
	Given now a sequence $(A_n)$ of elements of $\mathcal{M}(\mathcal{C})$. We consider the sequence:
	
	$(B_n)$ is an increasing sequence of elements of $\mathcal{M}(\mathcal{C})$, therefore:
	
	but:
	
	Therefore:
	
	Therefore $\mathcal{M}(\mathcal{C})$ is stable by countable union and finally $\mathcal{M}(\mathcal{C})$ is a tribe. But as $\mathcal{C}\subseteq \mathcal{M}(\mathcal{C})$ this brings us to $\mathcal{M}(\mathcal{C})=\sigma(\mathcal{C})$.
	\begin{flushright}
		$\blacksquare$  Q.E.D.
	\end{flushright}
	\end{dem}
	We will later see some important applications of this theorem (but first we want to improve the above text with figure and more simple and practical examples!).
	
	\begin{flushright}
	\begin{tabular}{l c}
	\circled{50} & \pbox{20cm}{\score{4}{5} \\ {\tiny 17 votes,  76.47\%}} 
	\end{tabular} 
	\end{flushright}